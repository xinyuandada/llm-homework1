{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 0.15382248884786956,
  "eval_steps": 500,
  "global_step": 1000,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0015382248884786955,
      "grad_norm": 0.7800440192222595,
      "learning_rate": 9.995385325334565e-05,
      "loss": 3.3764,
      "step": 10
    },
    {
      "epoch": 0.003076449776957391,
      "grad_norm": 0.7170557975769043,
      "learning_rate": 9.990257909039636e-05,
      "loss": 3.2895,
      "step": 20
    },
    {
      "epoch": 0.0046146746654360865,
      "grad_norm": 0.7733101844787598,
      "learning_rate": 9.985130492744707e-05,
      "loss": 3.2437,
      "step": 30
    },
    {
      "epoch": 0.006152899553914782,
      "grad_norm": 1.1588221788406372,
      "learning_rate": 9.980003076449777e-05,
      "loss": 3.0131,
      "step": 40
    },
    {
      "epoch": 0.007691124442393478,
      "grad_norm": 1.1910779476165771,
      "learning_rate": 9.974875660154848e-05,
      "loss": 2.8109,
      "step": 50
    },
    {
      "epoch": 0.009229349330872173,
      "grad_norm": 0.9716565012931824,
      "learning_rate": 9.969748243859919e-05,
      "loss": 2.7559,
      "step": 60
    },
    {
      "epoch": 0.01076757421935087,
      "grad_norm": 1.3204234838485718,
      "learning_rate": 9.96462082756499e-05,
      "loss": 2.9574,
      "step": 70
    },
    {
      "epoch": 0.012305799107829564,
      "grad_norm": 1.1385749578475952,
      "learning_rate": 9.959493411270061e-05,
      "loss": 2.6185,
      "step": 80
    },
    {
      "epoch": 0.01384402399630826,
      "grad_norm": 1.5976508855819702,
      "learning_rate": 9.954365994975132e-05,
      "loss": 2.7861,
      "step": 90
    },
    {
      "epoch": 0.015382248884786957,
      "grad_norm": 1.2703982591629028,
      "learning_rate": 9.949238578680203e-05,
      "loss": 2.7095,
      "step": 100
    },
    {
      "epoch": 0.01692047377326565,
      "grad_norm": 0.9696293473243713,
      "learning_rate": 9.944111162385275e-05,
      "loss": 2.6845,
      "step": 110
    },
    {
      "epoch": 0.018458698661744346,
      "grad_norm": 1.0070174932479858,
      "learning_rate": 9.938983746090346e-05,
      "loss": 2.5992,
      "step": 120
    },
    {
      "epoch": 0.019996923550223044,
      "grad_norm": 0.7789586186408997,
      "learning_rate": 9.933856329795417e-05,
      "loss": 2.5619,
      "step": 130
    },
    {
      "epoch": 0.02153514843870174,
      "grad_norm": 0.9720857739448547,
      "learning_rate": 9.928728913500488e-05,
      "loss": 2.5737,
      "step": 140
    },
    {
      "epoch": 0.023073373327180433,
      "grad_norm": 0.9559718370437622,
      "learning_rate": 9.923601497205559e-05,
      "loss": 2.582,
      "step": 150
    },
    {
      "epoch": 0.024611598215659128,
      "grad_norm": 1.2294028997421265,
      "learning_rate": 9.91847408091063e-05,
      "loss": 2.5172,
      "step": 160
    },
    {
      "epoch": 0.026149823104137826,
      "grad_norm": 0.8206753730773926,
      "learning_rate": 9.9133466646157e-05,
      "loss": 2.6148,
      "step": 170
    },
    {
      "epoch": 0.02768804799261652,
      "grad_norm": 1.6609854698181152,
      "learning_rate": 9.908219248320772e-05,
      "loss": 2.5482,
      "step": 180
    },
    {
      "epoch": 0.029226272881095215,
      "grad_norm": 0.9564527869224548,
      "learning_rate": 9.903091832025842e-05,
      "loss": 2.5977,
      "step": 190
    },
    {
      "epoch": 0.030764497769573913,
      "grad_norm": 1.0379178524017334,
      "learning_rate": 9.897964415730913e-05,
      "loss": 2.6212,
      "step": 200
    },
    {
      "epoch": 0.032302722658052604,
      "grad_norm": 1.038809895515442,
      "learning_rate": 9.892836999435984e-05,
      "loss": 2.5757,
      "step": 210
    },
    {
      "epoch": 0.0338409475465313,
      "grad_norm": 1.1101423501968384,
      "learning_rate": 9.887709583141056e-05,
      "loss": 2.4347,
      "step": 220
    },
    {
      "epoch": 0.03537917243501,
      "grad_norm": 1.4410618543624878,
      "learning_rate": 9.882582166846127e-05,
      "loss": 2.451,
      "step": 230
    },
    {
      "epoch": 0.03691739732348869,
      "grad_norm": 1.5485572814941406,
      "learning_rate": 9.877454750551197e-05,
      "loss": 2.4091,
      "step": 240
    },
    {
      "epoch": 0.03845562221196739,
      "grad_norm": 0.8919523358345032,
      "learning_rate": 9.872327334256268e-05,
      "loss": 2.4517,
      "step": 250
    },
    {
      "epoch": 0.03999384710044609,
      "grad_norm": 1.5257225036621094,
      "learning_rate": 9.86719991796134e-05,
      "loss": 2.5079,
      "step": 260
    },
    {
      "epoch": 0.04153207198892478,
      "grad_norm": 1.1131129264831543,
      "learning_rate": 9.862072501666411e-05,
      "loss": 2.5233,
      "step": 270
    },
    {
      "epoch": 0.04307029687740348,
      "grad_norm": 1.0646756887435913,
      "learning_rate": 9.856945085371482e-05,
      "loss": 2.5011,
      "step": 280
    },
    {
      "epoch": 0.044608521765882175,
      "grad_norm": 0.9087584614753723,
      "learning_rate": 9.851817669076553e-05,
      "loss": 2.4325,
      "step": 290
    },
    {
      "epoch": 0.046146746654360866,
      "grad_norm": 0.9257020950317383,
      "learning_rate": 9.846690252781624e-05,
      "loss": 2.4766,
      "step": 300
    },
    {
      "epoch": 0.047684971542839565,
      "grad_norm": 1.2664592266082764,
      "learning_rate": 9.841562836486695e-05,
      "loss": 2.379,
      "step": 310
    },
    {
      "epoch": 0.049223196431318256,
      "grad_norm": 1.7497199773788452,
      "learning_rate": 9.836435420191767e-05,
      "loss": 2.497,
      "step": 320
    },
    {
      "epoch": 0.050761421319796954,
      "grad_norm": 1.0710874795913696,
      "learning_rate": 9.831308003896836e-05,
      "loss": 2.4277,
      "step": 330
    },
    {
      "epoch": 0.05229964620827565,
      "grad_norm": 1.1468404531478882,
      "learning_rate": 9.826180587601907e-05,
      "loss": 2.4062,
      "step": 340
    },
    {
      "epoch": 0.05383787109675434,
      "grad_norm": 0.9473360776901245,
      "learning_rate": 9.821053171306978e-05,
      "loss": 2.3603,
      "step": 350
    },
    {
      "epoch": 0.05537609598523304,
      "grad_norm": 0.9064487218856812,
      "learning_rate": 9.815925755012049e-05,
      "loss": 2.5071,
      "step": 360
    },
    {
      "epoch": 0.05691432087371174,
      "grad_norm": 1.2210891246795654,
      "learning_rate": 9.810798338717121e-05,
      "loss": 2.3493,
      "step": 370
    },
    {
      "epoch": 0.05845254576219043,
      "grad_norm": 1.030822992324829,
      "learning_rate": 9.805670922422192e-05,
      "loss": 2.5064,
      "step": 380
    },
    {
      "epoch": 0.05999077065066913,
      "grad_norm": 1.2741059064865112,
      "learning_rate": 9.800543506127263e-05,
      "loss": 2.4764,
      "step": 390
    },
    {
      "epoch": 0.06152899553914783,
      "grad_norm": 1.203805685043335,
      "learning_rate": 9.795416089832334e-05,
      "loss": 2.447,
      "step": 400
    },
    {
      "epoch": 0.06306722042762652,
      "grad_norm": 1.0742758512496948,
      "learning_rate": 9.790288673537404e-05,
      "loss": 2.427,
      "step": 410
    },
    {
      "epoch": 0.06460544531610521,
      "grad_norm": 1.02286958694458,
      "learning_rate": 9.785161257242476e-05,
      "loss": 2.4394,
      "step": 420
    },
    {
      "epoch": 0.06614367020458391,
      "grad_norm": 0.9090739488601685,
      "learning_rate": 9.780033840947547e-05,
      "loss": 2.4173,
      "step": 430
    },
    {
      "epoch": 0.0676818950930626,
      "grad_norm": 1.149408221244812,
      "learning_rate": 9.774906424652618e-05,
      "loss": 2.456,
      "step": 440
    },
    {
      "epoch": 0.0692201199815413,
      "grad_norm": 0.963049590587616,
      "learning_rate": 9.769779008357689e-05,
      "loss": 2.3834,
      "step": 450
    },
    {
      "epoch": 0.07075834487002,
      "grad_norm": 0.9821059107780457,
      "learning_rate": 9.76465159206276e-05,
      "loss": 2.4565,
      "step": 460
    },
    {
      "epoch": 0.0722965697584987,
      "grad_norm": 1.0546011924743652,
      "learning_rate": 9.759524175767832e-05,
      "loss": 2.396,
      "step": 470
    },
    {
      "epoch": 0.07383479464697738,
      "grad_norm": 1.0994268655776978,
      "learning_rate": 9.754396759472903e-05,
      "loss": 2.4549,
      "step": 480
    },
    {
      "epoch": 0.07537301953545608,
      "grad_norm": 0.8789143562316895,
      "learning_rate": 9.749269343177974e-05,
      "loss": 2.4605,
      "step": 490
    },
    {
      "epoch": 0.07691124442393478,
      "grad_norm": 0.9876823425292969,
      "learning_rate": 9.744141926883043e-05,
      "loss": 2.4503,
      "step": 500
    },
    {
      "epoch": 0.07844946931241348,
      "grad_norm": 1.1950620412826538,
      "learning_rate": 9.739014510588114e-05,
      "loss": 2.5059,
      "step": 510
    },
    {
      "epoch": 0.07998769420089218,
      "grad_norm": 1.1006996631622314,
      "learning_rate": 9.733887094293186e-05,
      "loss": 2.2882,
      "step": 520
    },
    {
      "epoch": 0.08152591908937086,
      "grad_norm": 0.9053552150726318,
      "learning_rate": 9.728759677998257e-05,
      "loss": 2.3396,
      "step": 530
    },
    {
      "epoch": 0.08306414397784956,
      "grad_norm": 1.1737337112426758,
      "learning_rate": 9.723632261703328e-05,
      "loss": 2.4247,
      "step": 540
    },
    {
      "epoch": 0.08460236886632826,
      "grad_norm": 0.8845342993736267,
      "learning_rate": 9.718504845408399e-05,
      "loss": 2.3835,
      "step": 550
    },
    {
      "epoch": 0.08614059375480695,
      "grad_norm": 1.0540077686309814,
      "learning_rate": 9.71337742911347e-05,
      "loss": 2.369,
      "step": 560
    },
    {
      "epoch": 0.08767881864328565,
      "grad_norm": 0.9919202923774719,
      "learning_rate": 9.708250012818541e-05,
      "loss": 2.4425,
      "step": 570
    },
    {
      "epoch": 0.08921704353176435,
      "grad_norm": 0.9196600914001465,
      "learning_rate": 9.703122596523612e-05,
      "loss": 2.3381,
      "step": 580
    },
    {
      "epoch": 0.09075526842024303,
      "grad_norm": 1.1251168251037598,
      "learning_rate": 9.697995180228683e-05,
      "loss": 2.3186,
      "step": 590
    },
    {
      "epoch": 0.09229349330872173,
      "grad_norm": 1.3643468618392944,
      "learning_rate": 9.692867763933754e-05,
      "loss": 2.4726,
      "step": 600
    },
    {
      "epoch": 0.09383171819720043,
      "grad_norm": 0.8772892951965332,
      "learning_rate": 9.687740347638825e-05,
      "loss": 2.4736,
      "step": 610
    },
    {
      "epoch": 0.09536994308567913,
      "grad_norm": 1.061119556427002,
      "learning_rate": 9.682612931343897e-05,
      "loss": 2.4213,
      "step": 620
    },
    {
      "epoch": 0.09690816797415783,
      "grad_norm": 0.9421314597129822,
      "learning_rate": 9.677485515048968e-05,
      "loss": 2.4277,
      "step": 630
    },
    {
      "epoch": 0.09844639286263651,
      "grad_norm": 1.2146601676940918,
      "learning_rate": 9.672358098754039e-05,
      "loss": 2.2897,
      "step": 640
    },
    {
      "epoch": 0.09998461775111521,
      "grad_norm": 1.2775753736495972,
      "learning_rate": 9.66723068245911e-05,
      "loss": 2.4684,
      "step": 650
    },
    {
      "epoch": 0.10152284263959391,
      "grad_norm": 1.0711489915847778,
      "learning_rate": 9.66210326616418e-05,
      "loss": 2.3954,
      "step": 660
    },
    {
      "epoch": 0.1030610675280726,
      "grad_norm": 0.9492761492729187,
      "learning_rate": 9.656975849869251e-05,
      "loss": 2.3824,
      "step": 670
    },
    {
      "epoch": 0.1045992924165513,
      "grad_norm": 0.8662605285644531,
      "learning_rate": 9.651848433574322e-05,
      "loss": 2.2737,
      "step": 680
    },
    {
      "epoch": 0.10613751730503,
      "grad_norm": 0.9089555740356445,
      "learning_rate": 9.646721017279393e-05,
      "loss": 2.4527,
      "step": 690
    },
    {
      "epoch": 0.10767574219350869,
      "grad_norm": 0.8828738331794739,
      "learning_rate": 9.641593600984464e-05,
      "loss": 2.3143,
      "step": 700
    },
    {
      "epoch": 0.10921396708198738,
      "grad_norm": 0.8621166944503784,
      "learning_rate": 9.636466184689535e-05,
      "loss": 2.2994,
      "step": 710
    },
    {
      "epoch": 0.11075219197046608,
      "grad_norm": 0.9188641309738159,
      "learning_rate": 9.631338768394606e-05,
      "loss": 2.3995,
      "step": 720
    },
    {
      "epoch": 0.11229041685894478,
      "grad_norm": 0.9558343291282654,
      "learning_rate": 9.626211352099678e-05,
      "loss": 2.3769,
      "step": 730
    },
    {
      "epoch": 0.11382864174742348,
      "grad_norm": 0.8890336751937866,
      "learning_rate": 9.621083935804749e-05,
      "loss": 2.345,
      "step": 740
    },
    {
      "epoch": 0.11536686663590216,
      "grad_norm": 1.3892899751663208,
      "learning_rate": 9.61595651950982e-05,
      "loss": 2.3247,
      "step": 750
    },
    {
      "epoch": 0.11690509152438086,
      "grad_norm": 0.9079137444496155,
      "learning_rate": 9.61082910321489e-05,
      "loss": 2.5095,
      "step": 760
    },
    {
      "epoch": 0.11844331641285956,
      "grad_norm": 0.9309729933738708,
      "learning_rate": 9.60570168691996e-05,
      "loss": 2.3357,
      "step": 770
    },
    {
      "epoch": 0.11998154130133826,
      "grad_norm": 1.0791698694229126,
      "learning_rate": 9.600574270625033e-05,
      "loss": 2.3941,
      "step": 780
    },
    {
      "epoch": 0.12151976618981696,
      "grad_norm": 0.9481079578399658,
      "learning_rate": 9.595446854330104e-05,
      "loss": 2.3263,
      "step": 790
    },
    {
      "epoch": 0.12305799107829565,
      "grad_norm": 1.384405255317688,
      "learning_rate": 9.590319438035175e-05,
      "loss": 2.3435,
      "step": 800
    },
    {
      "epoch": 0.12459621596677434,
      "grad_norm": 0.9731020927429199,
      "learning_rate": 9.585192021740246e-05,
      "loss": 2.3627,
      "step": 810
    },
    {
      "epoch": 0.12613444085525305,
      "grad_norm": 0.7296550869941711,
      "learning_rate": 9.580064605445316e-05,
      "loss": 2.2322,
      "step": 820
    },
    {
      "epoch": 0.12767266574373173,
      "grad_norm": 1.0827419757843018,
      "learning_rate": 9.574937189150389e-05,
      "loss": 2.4777,
      "step": 830
    },
    {
      "epoch": 0.12921089063221042,
      "grad_norm": 0.99042809009552,
      "learning_rate": 9.569809772855458e-05,
      "loss": 2.4005,
      "step": 840
    },
    {
      "epoch": 0.13074911552068913,
      "grad_norm": 1.324620246887207,
      "learning_rate": 9.564682356560529e-05,
      "loss": 2.4839,
      "step": 850
    },
    {
      "epoch": 0.13228734040916781,
      "grad_norm": 1.4613444805145264,
      "learning_rate": 9.5595549402656e-05,
      "loss": 2.4002,
      "step": 860
    },
    {
      "epoch": 0.13382556529764653,
      "grad_norm": 0.9288026094436646,
      "learning_rate": 9.554427523970671e-05,
      "loss": 2.3541,
      "step": 870
    },
    {
      "epoch": 0.1353637901861252,
      "grad_norm": 1.129841685295105,
      "learning_rate": 9.549300107675743e-05,
      "loss": 2.3574,
      "step": 880
    },
    {
      "epoch": 0.1369020150746039,
      "grad_norm": 0.8787457346916199,
      "learning_rate": 9.544172691380814e-05,
      "loss": 2.3814,
      "step": 890
    },
    {
      "epoch": 0.1384402399630826,
      "grad_norm": 0.9640584588050842,
      "learning_rate": 9.539045275085885e-05,
      "loss": 2.3224,
      "step": 900
    },
    {
      "epoch": 0.1399784648515613,
      "grad_norm": 1.1772041320800781,
      "learning_rate": 9.533917858790956e-05,
      "loss": 2.3084,
      "step": 910
    },
    {
      "epoch": 0.14151668974004,
      "grad_norm": 0.8222691416740417,
      "learning_rate": 9.528790442496027e-05,
      "loss": 2.4192,
      "step": 920
    },
    {
      "epoch": 0.1430549146285187,
      "grad_norm": 0.8213545680046082,
      "learning_rate": 9.523663026201098e-05,
      "loss": 2.3573,
      "step": 930
    },
    {
      "epoch": 0.1445931395169974,
      "grad_norm": 1.1404590606689453,
      "learning_rate": 9.518535609906169e-05,
      "loss": 2.3049,
      "step": 940
    },
    {
      "epoch": 0.14613136440547608,
      "grad_norm": 1.1463240385055542,
      "learning_rate": 9.51340819361124e-05,
      "loss": 2.4483,
      "step": 950
    },
    {
      "epoch": 0.14766958929395477,
      "grad_norm": 1.2217503786087036,
      "learning_rate": 9.50828077731631e-05,
      "loss": 2.4451,
      "step": 960
    },
    {
      "epoch": 0.14920781418243348,
      "grad_norm": 0.8242997527122498,
      "learning_rate": 9.503153361021381e-05,
      "loss": 2.3317,
      "step": 970
    },
    {
      "epoch": 0.15074603907091216,
      "grad_norm": 1.1569207906723022,
      "learning_rate": 9.498025944726452e-05,
      "loss": 2.2817,
      "step": 980
    },
    {
      "epoch": 0.15228426395939088,
      "grad_norm": 1.1216130256652832,
      "learning_rate": 9.492898528431525e-05,
      "loss": 2.3543,
      "step": 990
    },
    {
      "epoch": 0.15382248884786956,
      "grad_norm": 1.131046175956726,
      "learning_rate": 9.487771112136596e-05,
      "loss": 2.3527,
      "step": 1000
    }
  ],
  "logging_steps": 10,
  "max_steps": 19503,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 2110267588608000.0,
  "train_batch_size": 8,
  "trial_name": null,
  "trial_params": null
}
