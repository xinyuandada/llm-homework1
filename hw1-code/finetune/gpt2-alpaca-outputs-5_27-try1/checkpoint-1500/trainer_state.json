{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 0.23073373327180433,
  "eval_steps": 500,
  "global_step": 1500,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0015382248884786955,
      "grad_norm": 0.7800440192222595,
      "learning_rate": 9.995385325334565e-05,
      "loss": 3.3764,
      "step": 10
    },
    {
      "epoch": 0.003076449776957391,
      "grad_norm": 0.7170557975769043,
      "learning_rate": 9.990257909039636e-05,
      "loss": 3.2895,
      "step": 20
    },
    {
      "epoch": 0.0046146746654360865,
      "grad_norm": 0.7733101844787598,
      "learning_rate": 9.985130492744707e-05,
      "loss": 3.2437,
      "step": 30
    },
    {
      "epoch": 0.006152899553914782,
      "grad_norm": 1.1588221788406372,
      "learning_rate": 9.980003076449777e-05,
      "loss": 3.0131,
      "step": 40
    },
    {
      "epoch": 0.007691124442393478,
      "grad_norm": 1.1910779476165771,
      "learning_rate": 9.974875660154848e-05,
      "loss": 2.8109,
      "step": 50
    },
    {
      "epoch": 0.009229349330872173,
      "grad_norm": 0.9716565012931824,
      "learning_rate": 9.969748243859919e-05,
      "loss": 2.7559,
      "step": 60
    },
    {
      "epoch": 0.01076757421935087,
      "grad_norm": 1.3204234838485718,
      "learning_rate": 9.96462082756499e-05,
      "loss": 2.9574,
      "step": 70
    },
    {
      "epoch": 0.012305799107829564,
      "grad_norm": 1.1385749578475952,
      "learning_rate": 9.959493411270061e-05,
      "loss": 2.6185,
      "step": 80
    },
    {
      "epoch": 0.01384402399630826,
      "grad_norm": 1.5976508855819702,
      "learning_rate": 9.954365994975132e-05,
      "loss": 2.7861,
      "step": 90
    },
    {
      "epoch": 0.015382248884786957,
      "grad_norm": 1.2703982591629028,
      "learning_rate": 9.949238578680203e-05,
      "loss": 2.7095,
      "step": 100
    },
    {
      "epoch": 0.01692047377326565,
      "grad_norm": 0.9696293473243713,
      "learning_rate": 9.944111162385275e-05,
      "loss": 2.6845,
      "step": 110
    },
    {
      "epoch": 0.018458698661744346,
      "grad_norm": 1.0070174932479858,
      "learning_rate": 9.938983746090346e-05,
      "loss": 2.5992,
      "step": 120
    },
    {
      "epoch": 0.019996923550223044,
      "grad_norm": 0.7789586186408997,
      "learning_rate": 9.933856329795417e-05,
      "loss": 2.5619,
      "step": 130
    },
    {
      "epoch": 0.02153514843870174,
      "grad_norm": 0.9720857739448547,
      "learning_rate": 9.928728913500488e-05,
      "loss": 2.5737,
      "step": 140
    },
    {
      "epoch": 0.023073373327180433,
      "grad_norm": 0.9559718370437622,
      "learning_rate": 9.923601497205559e-05,
      "loss": 2.582,
      "step": 150
    },
    {
      "epoch": 0.024611598215659128,
      "grad_norm": 1.2294028997421265,
      "learning_rate": 9.91847408091063e-05,
      "loss": 2.5172,
      "step": 160
    },
    {
      "epoch": 0.026149823104137826,
      "grad_norm": 0.8206753730773926,
      "learning_rate": 9.9133466646157e-05,
      "loss": 2.6148,
      "step": 170
    },
    {
      "epoch": 0.02768804799261652,
      "grad_norm": 1.6609854698181152,
      "learning_rate": 9.908219248320772e-05,
      "loss": 2.5482,
      "step": 180
    },
    {
      "epoch": 0.029226272881095215,
      "grad_norm": 0.9564527869224548,
      "learning_rate": 9.903091832025842e-05,
      "loss": 2.5977,
      "step": 190
    },
    {
      "epoch": 0.030764497769573913,
      "grad_norm": 1.0379178524017334,
      "learning_rate": 9.897964415730913e-05,
      "loss": 2.6212,
      "step": 200
    },
    {
      "epoch": 0.032302722658052604,
      "grad_norm": 1.038809895515442,
      "learning_rate": 9.892836999435984e-05,
      "loss": 2.5757,
      "step": 210
    },
    {
      "epoch": 0.0338409475465313,
      "grad_norm": 1.1101423501968384,
      "learning_rate": 9.887709583141056e-05,
      "loss": 2.4347,
      "step": 220
    },
    {
      "epoch": 0.03537917243501,
      "grad_norm": 1.4410618543624878,
      "learning_rate": 9.882582166846127e-05,
      "loss": 2.451,
      "step": 230
    },
    {
      "epoch": 0.03691739732348869,
      "grad_norm": 1.5485572814941406,
      "learning_rate": 9.877454750551197e-05,
      "loss": 2.4091,
      "step": 240
    },
    {
      "epoch": 0.03845562221196739,
      "grad_norm": 0.8919523358345032,
      "learning_rate": 9.872327334256268e-05,
      "loss": 2.4517,
      "step": 250
    },
    {
      "epoch": 0.03999384710044609,
      "grad_norm": 1.5257225036621094,
      "learning_rate": 9.86719991796134e-05,
      "loss": 2.5079,
      "step": 260
    },
    {
      "epoch": 0.04153207198892478,
      "grad_norm": 1.1131129264831543,
      "learning_rate": 9.862072501666411e-05,
      "loss": 2.5233,
      "step": 270
    },
    {
      "epoch": 0.04307029687740348,
      "grad_norm": 1.0646756887435913,
      "learning_rate": 9.856945085371482e-05,
      "loss": 2.5011,
      "step": 280
    },
    {
      "epoch": 0.044608521765882175,
      "grad_norm": 0.9087584614753723,
      "learning_rate": 9.851817669076553e-05,
      "loss": 2.4325,
      "step": 290
    },
    {
      "epoch": 0.046146746654360866,
      "grad_norm": 0.9257020950317383,
      "learning_rate": 9.846690252781624e-05,
      "loss": 2.4766,
      "step": 300
    },
    {
      "epoch": 0.047684971542839565,
      "grad_norm": 1.2664592266082764,
      "learning_rate": 9.841562836486695e-05,
      "loss": 2.379,
      "step": 310
    },
    {
      "epoch": 0.049223196431318256,
      "grad_norm": 1.7497199773788452,
      "learning_rate": 9.836435420191767e-05,
      "loss": 2.497,
      "step": 320
    },
    {
      "epoch": 0.050761421319796954,
      "grad_norm": 1.0710874795913696,
      "learning_rate": 9.831308003896836e-05,
      "loss": 2.4277,
      "step": 330
    },
    {
      "epoch": 0.05229964620827565,
      "grad_norm": 1.1468404531478882,
      "learning_rate": 9.826180587601907e-05,
      "loss": 2.4062,
      "step": 340
    },
    {
      "epoch": 0.05383787109675434,
      "grad_norm": 0.9473360776901245,
      "learning_rate": 9.821053171306978e-05,
      "loss": 2.3603,
      "step": 350
    },
    {
      "epoch": 0.05537609598523304,
      "grad_norm": 0.9064487218856812,
      "learning_rate": 9.815925755012049e-05,
      "loss": 2.5071,
      "step": 360
    },
    {
      "epoch": 0.05691432087371174,
      "grad_norm": 1.2210891246795654,
      "learning_rate": 9.810798338717121e-05,
      "loss": 2.3493,
      "step": 370
    },
    {
      "epoch": 0.05845254576219043,
      "grad_norm": 1.030822992324829,
      "learning_rate": 9.805670922422192e-05,
      "loss": 2.5064,
      "step": 380
    },
    {
      "epoch": 0.05999077065066913,
      "grad_norm": 1.2741059064865112,
      "learning_rate": 9.800543506127263e-05,
      "loss": 2.4764,
      "step": 390
    },
    {
      "epoch": 0.06152899553914783,
      "grad_norm": 1.203805685043335,
      "learning_rate": 9.795416089832334e-05,
      "loss": 2.447,
      "step": 400
    },
    {
      "epoch": 0.06306722042762652,
      "grad_norm": 1.0742758512496948,
      "learning_rate": 9.790288673537404e-05,
      "loss": 2.427,
      "step": 410
    },
    {
      "epoch": 0.06460544531610521,
      "grad_norm": 1.02286958694458,
      "learning_rate": 9.785161257242476e-05,
      "loss": 2.4394,
      "step": 420
    },
    {
      "epoch": 0.06614367020458391,
      "grad_norm": 0.9090739488601685,
      "learning_rate": 9.780033840947547e-05,
      "loss": 2.4173,
      "step": 430
    },
    {
      "epoch": 0.0676818950930626,
      "grad_norm": 1.149408221244812,
      "learning_rate": 9.774906424652618e-05,
      "loss": 2.456,
      "step": 440
    },
    {
      "epoch": 0.0692201199815413,
      "grad_norm": 0.963049590587616,
      "learning_rate": 9.769779008357689e-05,
      "loss": 2.3834,
      "step": 450
    },
    {
      "epoch": 0.07075834487002,
      "grad_norm": 0.9821059107780457,
      "learning_rate": 9.76465159206276e-05,
      "loss": 2.4565,
      "step": 460
    },
    {
      "epoch": 0.0722965697584987,
      "grad_norm": 1.0546011924743652,
      "learning_rate": 9.759524175767832e-05,
      "loss": 2.396,
      "step": 470
    },
    {
      "epoch": 0.07383479464697738,
      "grad_norm": 1.0994268655776978,
      "learning_rate": 9.754396759472903e-05,
      "loss": 2.4549,
      "step": 480
    },
    {
      "epoch": 0.07537301953545608,
      "grad_norm": 0.8789143562316895,
      "learning_rate": 9.749269343177974e-05,
      "loss": 2.4605,
      "step": 490
    },
    {
      "epoch": 0.07691124442393478,
      "grad_norm": 0.9876823425292969,
      "learning_rate": 9.744141926883043e-05,
      "loss": 2.4503,
      "step": 500
    },
    {
      "epoch": 0.07844946931241348,
      "grad_norm": 1.1950620412826538,
      "learning_rate": 9.739014510588114e-05,
      "loss": 2.5059,
      "step": 510
    },
    {
      "epoch": 0.07998769420089218,
      "grad_norm": 1.1006996631622314,
      "learning_rate": 9.733887094293186e-05,
      "loss": 2.2882,
      "step": 520
    },
    {
      "epoch": 0.08152591908937086,
      "grad_norm": 0.9053552150726318,
      "learning_rate": 9.728759677998257e-05,
      "loss": 2.3396,
      "step": 530
    },
    {
      "epoch": 0.08306414397784956,
      "grad_norm": 1.1737337112426758,
      "learning_rate": 9.723632261703328e-05,
      "loss": 2.4247,
      "step": 540
    },
    {
      "epoch": 0.08460236886632826,
      "grad_norm": 0.8845342993736267,
      "learning_rate": 9.718504845408399e-05,
      "loss": 2.3835,
      "step": 550
    },
    {
      "epoch": 0.08614059375480695,
      "grad_norm": 1.0540077686309814,
      "learning_rate": 9.71337742911347e-05,
      "loss": 2.369,
      "step": 560
    },
    {
      "epoch": 0.08767881864328565,
      "grad_norm": 0.9919202923774719,
      "learning_rate": 9.708250012818541e-05,
      "loss": 2.4425,
      "step": 570
    },
    {
      "epoch": 0.08921704353176435,
      "grad_norm": 0.9196600914001465,
      "learning_rate": 9.703122596523612e-05,
      "loss": 2.3381,
      "step": 580
    },
    {
      "epoch": 0.09075526842024303,
      "grad_norm": 1.1251168251037598,
      "learning_rate": 9.697995180228683e-05,
      "loss": 2.3186,
      "step": 590
    },
    {
      "epoch": 0.09229349330872173,
      "grad_norm": 1.3643468618392944,
      "learning_rate": 9.692867763933754e-05,
      "loss": 2.4726,
      "step": 600
    },
    {
      "epoch": 0.09383171819720043,
      "grad_norm": 0.8772892951965332,
      "learning_rate": 9.687740347638825e-05,
      "loss": 2.4736,
      "step": 610
    },
    {
      "epoch": 0.09536994308567913,
      "grad_norm": 1.061119556427002,
      "learning_rate": 9.682612931343897e-05,
      "loss": 2.4213,
      "step": 620
    },
    {
      "epoch": 0.09690816797415783,
      "grad_norm": 0.9421314597129822,
      "learning_rate": 9.677485515048968e-05,
      "loss": 2.4277,
      "step": 630
    },
    {
      "epoch": 0.09844639286263651,
      "grad_norm": 1.2146601676940918,
      "learning_rate": 9.672358098754039e-05,
      "loss": 2.2897,
      "step": 640
    },
    {
      "epoch": 0.09998461775111521,
      "grad_norm": 1.2775753736495972,
      "learning_rate": 9.66723068245911e-05,
      "loss": 2.4684,
      "step": 650
    },
    {
      "epoch": 0.10152284263959391,
      "grad_norm": 1.0711489915847778,
      "learning_rate": 9.66210326616418e-05,
      "loss": 2.3954,
      "step": 660
    },
    {
      "epoch": 0.1030610675280726,
      "grad_norm": 0.9492761492729187,
      "learning_rate": 9.656975849869251e-05,
      "loss": 2.3824,
      "step": 670
    },
    {
      "epoch": 0.1045992924165513,
      "grad_norm": 0.8662605285644531,
      "learning_rate": 9.651848433574322e-05,
      "loss": 2.2737,
      "step": 680
    },
    {
      "epoch": 0.10613751730503,
      "grad_norm": 0.9089555740356445,
      "learning_rate": 9.646721017279393e-05,
      "loss": 2.4527,
      "step": 690
    },
    {
      "epoch": 0.10767574219350869,
      "grad_norm": 0.8828738331794739,
      "learning_rate": 9.641593600984464e-05,
      "loss": 2.3143,
      "step": 700
    },
    {
      "epoch": 0.10921396708198738,
      "grad_norm": 0.8621166944503784,
      "learning_rate": 9.636466184689535e-05,
      "loss": 2.2994,
      "step": 710
    },
    {
      "epoch": 0.11075219197046608,
      "grad_norm": 0.9188641309738159,
      "learning_rate": 9.631338768394606e-05,
      "loss": 2.3995,
      "step": 720
    },
    {
      "epoch": 0.11229041685894478,
      "grad_norm": 0.9558343291282654,
      "learning_rate": 9.626211352099678e-05,
      "loss": 2.3769,
      "step": 730
    },
    {
      "epoch": 0.11382864174742348,
      "grad_norm": 0.8890336751937866,
      "learning_rate": 9.621083935804749e-05,
      "loss": 2.345,
      "step": 740
    },
    {
      "epoch": 0.11536686663590216,
      "grad_norm": 1.3892899751663208,
      "learning_rate": 9.61595651950982e-05,
      "loss": 2.3247,
      "step": 750
    },
    {
      "epoch": 0.11690509152438086,
      "grad_norm": 0.9079137444496155,
      "learning_rate": 9.61082910321489e-05,
      "loss": 2.5095,
      "step": 760
    },
    {
      "epoch": 0.11844331641285956,
      "grad_norm": 0.9309729933738708,
      "learning_rate": 9.60570168691996e-05,
      "loss": 2.3357,
      "step": 770
    },
    {
      "epoch": 0.11998154130133826,
      "grad_norm": 1.0791698694229126,
      "learning_rate": 9.600574270625033e-05,
      "loss": 2.3941,
      "step": 780
    },
    {
      "epoch": 0.12151976618981696,
      "grad_norm": 0.9481079578399658,
      "learning_rate": 9.595446854330104e-05,
      "loss": 2.3263,
      "step": 790
    },
    {
      "epoch": 0.12305799107829565,
      "grad_norm": 1.384405255317688,
      "learning_rate": 9.590319438035175e-05,
      "loss": 2.3435,
      "step": 800
    },
    {
      "epoch": 0.12459621596677434,
      "grad_norm": 0.9731020927429199,
      "learning_rate": 9.585192021740246e-05,
      "loss": 2.3627,
      "step": 810
    },
    {
      "epoch": 0.12613444085525305,
      "grad_norm": 0.7296550869941711,
      "learning_rate": 9.580064605445316e-05,
      "loss": 2.2322,
      "step": 820
    },
    {
      "epoch": 0.12767266574373173,
      "grad_norm": 1.0827419757843018,
      "learning_rate": 9.574937189150389e-05,
      "loss": 2.4777,
      "step": 830
    },
    {
      "epoch": 0.12921089063221042,
      "grad_norm": 0.99042809009552,
      "learning_rate": 9.569809772855458e-05,
      "loss": 2.4005,
      "step": 840
    },
    {
      "epoch": 0.13074911552068913,
      "grad_norm": 1.324620246887207,
      "learning_rate": 9.564682356560529e-05,
      "loss": 2.4839,
      "step": 850
    },
    {
      "epoch": 0.13228734040916781,
      "grad_norm": 1.4613444805145264,
      "learning_rate": 9.5595549402656e-05,
      "loss": 2.4002,
      "step": 860
    },
    {
      "epoch": 0.13382556529764653,
      "grad_norm": 0.9288026094436646,
      "learning_rate": 9.554427523970671e-05,
      "loss": 2.3541,
      "step": 870
    },
    {
      "epoch": 0.1353637901861252,
      "grad_norm": 1.129841685295105,
      "learning_rate": 9.549300107675743e-05,
      "loss": 2.3574,
      "step": 880
    },
    {
      "epoch": 0.1369020150746039,
      "grad_norm": 0.8787457346916199,
      "learning_rate": 9.544172691380814e-05,
      "loss": 2.3814,
      "step": 890
    },
    {
      "epoch": 0.1384402399630826,
      "grad_norm": 0.9640584588050842,
      "learning_rate": 9.539045275085885e-05,
      "loss": 2.3224,
      "step": 900
    },
    {
      "epoch": 0.1399784648515613,
      "grad_norm": 1.1772041320800781,
      "learning_rate": 9.533917858790956e-05,
      "loss": 2.3084,
      "step": 910
    },
    {
      "epoch": 0.14151668974004,
      "grad_norm": 0.8222691416740417,
      "learning_rate": 9.528790442496027e-05,
      "loss": 2.4192,
      "step": 920
    },
    {
      "epoch": 0.1430549146285187,
      "grad_norm": 0.8213545680046082,
      "learning_rate": 9.523663026201098e-05,
      "loss": 2.3573,
      "step": 930
    },
    {
      "epoch": 0.1445931395169974,
      "grad_norm": 1.1404590606689453,
      "learning_rate": 9.518535609906169e-05,
      "loss": 2.3049,
      "step": 940
    },
    {
      "epoch": 0.14613136440547608,
      "grad_norm": 1.1463240385055542,
      "learning_rate": 9.51340819361124e-05,
      "loss": 2.4483,
      "step": 950
    },
    {
      "epoch": 0.14766958929395477,
      "grad_norm": 1.2217503786087036,
      "learning_rate": 9.50828077731631e-05,
      "loss": 2.4451,
      "step": 960
    },
    {
      "epoch": 0.14920781418243348,
      "grad_norm": 0.8242997527122498,
      "learning_rate": 9.503153361021381e-05,
      "loss": 2.3317,
      "step": 970
    },
    {
      "epoch": 0.15074603907091216,
      "grad_norm": 1.1569207906723022,
      "learning_rate": 9.498025944726452e-05,
      "loss": 2.2817,
      "step": 980
    },
    {
      "epoch": 0.15228426395939088,
      "grad_norm": 1.1216130256652832,
      "learning_rate": 9.492898528431525e-05,
      "loss": 2.3543,
      "step": 990
    },
    {
      "epoch": 0.15382248884786956,
      "grad_norm": 1.131046175956726,
      "learning_rate": 9.487771112136596e-05,
      "loss": 2.3527,
      "step": 1000
    },
    {
      "epoch": 0.15536071373634824,
      "grad_norm": 1.2182101011276245,
      "learning_rate": 9.482643695841665e-05,
      "loss": 2.2678,
      "step": 1010
    },
    {
      "epoch": 0.15689893862482696,
      "grad_norm": 0.9577516317367554,
      "learning_rate": 9.477516279546736e-05,
      "loss": 2.4039,
      "step": 1020
    },
    {
      "epoch": 0.15843716351330564,
      "grad_norm": 1.0458829402923584,
      "learning_rate": 9.472388863251808e-05,
      "loss": 2.413,
      "step": 1030
    },
    {
      "epoch": 0.15997538840178435,
      "grad_norm": 0.9187437891960144,
      "learning_rate": 9.467261446956879e-05,
      "loss": 2.3664,
      "step": 1040
    },
    {
      "epoch": 0.16151361329026304,
      "grad_norm": 0.829723060131073,
      "learning_rate": 9.46213403066195e-05,
      "loss": 2.3837,
      "step": 1050
    },
    {
      "epoch": 0.16305183817874172,
      "grad_norm": 1.2389100790023804,
      "learning_rate": 9.457006614367021e-05,
      "loss": 2.3635,
      "step": 1060
    },
    {
      "epoch": 0.16459006306722043,
      "grad_norm": 1.0236015319824219,
      "learning_rate": 9.451879198072092e-05,
      "loss": 2.2544,
      "step": 1070
    },
    {
      "epoch": 0.16612828795569912,
      "grad_norm": 0.8570135831832886,
      "learning_rate": 9.446751781777163e-05,
      "loss": 2.3395,
      "step": 1080
    },
    {
      "epoch": 0.16766651284417783,
      "grad_norm": 0.9625948071479797,
      "learning_rate": 9.441624365482235e-05,
      "loss": 2.3859,
      "step": 1090
    },
    {
      "epoch": 0.1692047377326565,
      "grad_norm": 0.89539635181427,
      "learning_rate": 9.436496949187305e-05,
      "loss": 2.3562,
      "step": 1100
    },
    {
      "epoch": 0.1707429626211352,
      "grad_norm": 0.9785100221633911,
      "learning_rate": 9.431369532892376e-05,
      "loss": 2.2446,
      "step": 1110
    },
    {
      "epoch": 0.1722811875096139,
      "grad_norm": 1.1737624406814575,
      "learning_rate": 9.426242116597446e-05,
      "loss": 2.3936,
      "step": 1120
    },
    {
      "epoch": 0.1738194123980926,
      "grad_norm": 0.9451086521148682,
      "learning_rate": 9.421114700302517e-05,
      "loss": 2.4288,
      "step": 1130
    },
    {
      "epoch": 0.1753576372865713,
      "grad_norm": 0.8831786513328552,
      "learning_rate": 9.41598728400759e-05,
      "loss": 2.3756,
      "step": 1140
    },
    {
      "epoch": 0.17689586217505,
      "grad_norm": 1.3798778057098389,
      "learning_rate": 9.41085986771266e-05,
      "loss": 2.482,
      "step": 1150
    },
    {
      "epoch": 0.1784340870635287,
      "grad_norm": 0.9072500467300415,
      "learning_rate": 9.405732451417731e-05,
      "loss": 2.2366,
      "step": 1160
    },
    {
      "epoch": 0.17997231195200739,
      "grad_norm": 0.851820707321167,
      "learning_rate": 9.400605035122802e-05,
      "loss": 2.2708,
      "step": 1170
    },
    {
      "epoch": 0.18151053684048607,
      "grad_norm": 0.9339284300804138,
      "learning_rate": 9.395477618827873e-05,
      "loss": 2.268,
      "step": 1180
    },
    {
      "epoch": 0.18304876172896478,
      "grad_norm": 0.8741077184677124,
      "learning_rate": 9.390350202532944e-05,
      "loss": 2.3972,
      "step": 1190
    },
    {
      "epoch": 0.18458698661744347,
      "grad_norm": 1.1137354373931885,
      "learning_rate": 9.385222786238015e-05,
      "loss": 2.2709,
      "step": 1200
    },
    {
      "epoch": 0.18612521150592218,
      "grad_norm": 0.8969918489456177,
      "learning_rate": 9.380095369943086e-05,
      "loss": 2.3409,
      "step": 1210
    },
    {
      "epoch": 0.18766343639440086,
      "grad_norm": 1.399888515472412,
      "learning_rate": 9.374967953648157e-05,
      "loss": 2.356,
      "step": 1220
    },
    {
      "epoch": 0.18920166128287955,
      "grad_norm": 1.1646440029144287,
      "learning_rate": 9.369840537353228e-05,
      "loss": 2.3076,
      "step": 1230
    },
    {
      "epoch": 0.19073988617135826,
      "grad_norm": 1.1363041400909424,
      "learning_rate": 9.3647131210583e-05,
      "loss": 2.3777,
      "step": 1240
    },
    {
      "epoch": 0.19227811105983694,
      "grad_norm": 1.016471266746521,
      "learning_rate": 9.359585704763371e-05,
      "loss": 2.3759,
      "step": 1250
    },
    {
      "epoch": 0.19381633594831565,
      "grad_norm": 0.8463709950447083,
      "learning_rate": 9.354458288468442e-05,
      "loss": 2.2938,
      "step": 1260
    },
    {
      "epoch": 0.19535456083679434,
      "grad_norm": 1.0321208238601685,
      "learning_rate": 9.349330872173511e-05,
      "loss": 2.3482,
      "step": 1270
    },
    {
      "epoch": 0.19689278572527302,
      "grad_norm": 0.801830530166626,
      "learning_rate": 9.344203455878582e-05,
      "loss": 2.2615,
      "step": 1280
    },
    {
      "epoch": 0.19843101061375173,
      "grad_norm": 0.9908499121665955,
      "learning_rate": 9.339076039583655e-05,
      "loss": 2.2918,
      "step": 1290
    },
    {
      "epoch": 0.19996923550223042,
      "grad_norm": 0.9015299081802368,
      "learning_rate": 9.333948623288725e-05,
      "loss": 2.3913,
      "step": 1300
    },
    {
      "epoch": 0.20150746039070913,
      "grad_norm": 0.8517335653305054,
      "learning_rate": 9.328821206993796e-05,
      "loss": 2.3604,
      "step": 1310
    },
    {
      "epoch": 0.20304568527918782,
      "grad_norm": 0.7576839923858643,
      "learning_rate": 9.323693790698867e-05,
      "loss": 2.2648,
      "step": 1320
    },
    {
      "epoch": 0.2045839101676665,
      "grad_norm": 1.0572640895843506,
      "learning_rate": 9.318566374403938e-05,
      "loss": 2.3066,
      "step": 1330
    },
    {
      "epoch": 0.2061221350561452,
      "grad_norm": 1.2618391513824463,
      "learning_rate": 9.313438958109009e-05,
      "loss": 2.4479,
      "step": 1340
    },
    {
      "epoch": 0.2076603599446239,
      "grad_norm": 0.9082226753234863,
      "learning_rate": 9.30831154181408e-05,
      "loss": 2.319,
      "step": 1350
    },
    {
      "epoch": 0.2091985848331026,
      "grad_norm": 1.137063980102539,
      "learning_rate": 9.303184125519151e-05,
      "loss": 2.3538,
      "step": 1360
    },
    {
      "epoch": 0.2107368097215813,
      "grad_norm": 1.2156848907470703,
      "learning_rate": 9.298056709224222e-05,
      "loss": 2.3672,
      "step": 1370
    },
    {
      "epoch": 0.21227503461006,
      "grad_norm": 1.0615837574005127,
      "learning_rate": 9.292929292929293e-05,
      "loss": 2.318,
      "step": 1380
    },
    {
      "epoch": 0.2138132594985387,
      "grad_norm": 0.7031446695327759,
      "learning_rate": 9.287801876634365e-05,
      "loss": 2.3754,
      "step": 1390
    },
    {
      "epoch": 0.21535148438701737,
      "grad_norm": 1.0792100429534912,
      "learning_rate": 9.282674460339436e-05,
      "loss": 2.3928,
      "step": 1400
    },
    {
      "epoch": 0.21688970927549608,
      "grad_norm": 0.9547038078308105,
      "learning_rate": 9.277547044044507e-05,
      "loss": 2.3834,
      "step": 1410
    },
    {
      "epoch": 0.21842793416397477,
      "grad_norm": 0.9081752896308899,
      "learning_rate": 9.272419627749578e-05,
      "loss": 2.366,
      "step": 1420
    },
    {
      "epoch": 0.21996615905245348,
      "grad_norm": 0.8674251437187195,
      "learning_rate": 9.267292211454649e-05,
      "loss": 2.3302,
      "step": 1430
    },
    {
      "epoch": 0.22150438394093216,
      "grad_norm": 0.7524937391281128,
      "learning_rate": 9.26216479515972e-05,
      "loss": 2.3265,
      "step": 1440
    },
    {
      "epoch": 0.22304260882941085,
      "grad_norm": 1.0890637636184692,
      "learning_rate": 9.25703737886479e-05,
      "loss": 2.4002,
      "step": 1450
    },
    {
      "epoch": 0.22458083371788956,
      "grad_norm": 0.9604703783988953,
      "learning_rate": 9.251909962569861e-05,
      "loss": 2.3997,
      "step": 1460
    },
    {
      "epoch": 0.22611905860636825,
      "grad_norm": 0.9587043523788452,
      "learning_rate": 9.246782546274932e-05,
      "loss": 2.1888,
      "step": 1470
    },
    {
      "epoch": 0.22765728349484696,
      "grad_norm": 0.8627868294715881,
      "learning_rate": 9.241655129980003e-05,
      "loss": 2.2271,
      "step": 1480
    },
    {
      "epoch": 0.22919550838332564,
      "grad_norm": 1.6658141613006592,
      "learning_rate": 9.236527713685074e-05,
      "loss": 2.3808,
      "step": 1490
    },
    {
      "epoch": 0.23073373327180433,
      "grad_norm": 0.8292087316513062,
      "learning_rate": 9.231400297390146e-05,
      "loss": 2.3831,
      "step": 1500
    }
  ],
  "logging_steps": 10,
  "max_steps": 19503,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 3165401382912000.0,
  "train_batch_size": 8,
  "trial_name": null,
  "trial_params": null
}
