{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 2.1535148438701737,
  "eval_steps": 500,
  "global_step": 14000,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0015382248884786955,
      "grad_norm": 0.7800440192222595,
      "learning_rate": 9.995385325334565e-05,
      "loss": 3.3764,
      "step": 10
    },
    {
      "epoch": 0.003076449776957391,
      "grad_norm": 0.7170557975769043,
      "learning_rate": 9.990257909039636e-05,
      "loss": 3.2895,
      "step": 20
    },
    {
      "epoch": 0.0046146746654360865,
      "grad_norm": 0.7733101844787598,
      "learning_rate": 9.985130492744707e-05,
      "loss": 3.2437,
      "step": 30
    },
    {
      "epoch": 0.006152899553914782,
      "grad_norm": 1.1588221788406372,
      "learning_rate": 9.980003076449777e-05,
      "loss": 3.0131,
      "step": 40
    },
    {
      "epoch": 0.007691124442393478,
      "grad_norm": 1.1910779476165771,
      "learning_rate": 9.974875660154848e-05,
      "loss": 2.8109,
      "step": 50
    },
    {
      "epoch": 0.009229349330872173,
      "grad_norm": 0.9716565012931824,
      "learning_rate": 9.969748243859919e-05,
      "loss": 2.7559,
      "step": 60
    },
    {
      "epoch": 0.01076757421935087,
      "grad_norm": 1.3204234838485718,
      "learning_rate": 9.96462082756499e-05,
      "loss": 2.9574,
      "step": 70
    },
    {
      "epoch": 0.012305799107829564,
      "grad_norm": 1.1385749578475952,
      "learning_rate": 9.959493411270061e-05,
      "loss": 2.6185,
      "step": 80
    },
    {
      "epoch": 0.01384402399630826,
      "grad_norm": 1.5976508855819702,
      "learning_rate": 9.954365994975132e-05,
      "loss": 2.7861,
      "step": 90
    },
    {
      "epoch": 0.015382248884786957,
      "grad_norm": 1.2703982591629028,
      "learning_rate": 9.949238578680203e-05,
      "loss": 2.7095,
      "step": 100
    },
    {
      "epoch": 0.01692047377326565,
      "grad_norm": 0.9696293473243713,
      "learning_rate": 9.944111162385275e-05,
      "loss": 2.6845,
      "step": 110
    },
    {
      "epoch": 0.018458698661744346,
      "grad_norm": 1.0070174932479858,
      "learning_rate": 9.938983746090346e-05,
      "loss": 2.5992,
      "step": 120
    },
    {
      "epoch": 0.019996923550223044,
      "grad_norm": 0.7789586186408997,
      "learning_rate": 9.933856329795417e-05,
      "loss": 2.5619,
      "step": 130
    },
    {
      "epoch": 0.02153514843870174,
      "grad_norm": 0.9720857739448547,
      "learning_rate": 9.928728913500488e-05,
      "loss": 2.5737,
      "step": 140
    },
    {
      "epoch": 0.023073373327180433,
      "grad_norm": 0.9559718370437622,
      "learning_rate": 9.923601497205559e-05,
      "loss": 2.582,
      "step": 150
    },
    {
      "epoch": 0.024611598215659128,
      "grad_norm": 1.2294028997421265,
      "learning_rate": 9.91847408091063e-05,
      "loss": 2.5172,
      "step": 160
    },
    {
      "epoch": 0.026149823104137826,
      "grad_norm": 0.8206753730773926,
      "learning_rate": 9.9133466646157e-05,
      "loss": 2.6148,
      "step": 170
    },
    {
      "epoch": 0.02768804799261652,
      "grad_norm": 1.6609854698181152,
      "learning_rate": 9.908219248320772e-05,
      "loss": 2.5482,
      "step": 180
    },
    {
      "epoch": 0.029226272881095215,
      "grad_norm": 0.9564527869224548,
      "learning_rate": 9.903091832025842e-05,
      "loss": 2.5977,
      "step": 190
    },
    {
      "epoch": 0.030764497769573913,
      "grad_norm": 1.0379178524017334,
      "learning_rate": 9.897964415730913e-05,
      "loss": 2.6212,
      "step": 200
    },
    {
      "epoch": 0.032302722658052604,
      "grad_norm": 1.038809895515442,
      "learning_rate": 9.892836999435984e-05,
      "loss": 2.5757,
      "step": 210
    },
    {
      "epoch": 0.0338409475465313,
      "grad_norm": 1.1101423501968384,
      "learning_rate": 9.887709583141056e-05,
      "loss": 2.4347,
      "step": 220
    },
    {
      "epoch": 0.03537917243501,
      "grad_norm": 1.4410618543624878,
      "learning_rate": 9.882582166846127e-05,
      "loss": 2.451,
      "step": 230
    },
    {
      "epoch": 0.03691739732348869,
      "grad_norm": 1.5485572814941406,
      "learning_rate": 9.877454750551197e-05,
      "loss": 2.4091,
      "step": 240
    },
    {
      "epoch": 0.03845562221196739,
      "grad_norm": 0.8919523358345032,
      "learning_rate": 9.872327334256268e-05,
      "loss": 2.4517,
      "step": 250
    },
    {
      "epoch": 0.03999384710044609,
      "grad_norm": 1.5257225036621094,
      "learning_rate": 9.86719991796134e-05,
      "loss": 2.5079,
      "step": 260
    },
    {
      "epoch": 0.04153207198892478,
      "grad_norm": 1.1131129264831543,
      "learning_rate": 9.862072501666411e-05,
      "loss": 2.5233,
      "step": 270
    },
    {
      "epoch": 0.04307029687740348,
      "grad_norm": 1.0646756887435913,
      "learning_rate": 9.856945085371482e-05,
      "loss": 2.5011,
      "step": 280
    },
    {
      "epoch": 0.044608521765882175,
      "grad_norm": 0.9087584614753723,
      "learning_rate": 9.851817669076553e-05,
      "loss": 2.4325,
      "step": 290
    },
    {
      "epoch": 0.046146746654360866,
      "grad_norm": 0.9257020950317383,
      "learning_rate": 9.846690252781624e-05,
      "loss": 2.4766,
      "step": 300
    },
    {
      "epoch": 0.047684971542839565,
      "grad_norm": 1.2664592266082764,
      "learning_rate": 9.841562836486695e-05,
      "loss": 2.379,
      "step": 310
    },
    {
      "epoch": 0.049223196431318256,
      "grad_norm": 1.7497199773788452,
      "learning_rate": 9.836435420191767e-05,
      "loss": 2.497,
      "step": 320
    },
    {
      "epoch": 0.050761421319796954,
      "grad_norm": 1.0710874795913696,
      "learning_rate": 9.831308003896836e-05,
      "loss": 2.4277,
      "step": 330
    },
    {
      "epoch": 0.05229964620827565,
      "grad_norm": 1.1468404531478882,
      "learning_rate": 9.826180587601907e-05,
      "loss": 2.4062,
      "step": 340
    },
    {
      "epoch": 0.05383787109675434,
      "grad_norm": 0.9473360776901245,
      "learning_rate": 9.821053171306978e-05,
      "loss": 2.3603,
      "step": 350
    },
    {
      "epoch": 0.05537609598523304,
      "grad_norm": 0.9064487218856812,
      "learning_rate": 9.815925755012049e-05,
      "loss": 2.5071,
      "step": 360
    },
    {
      "epoch": 0.05691432087371174,
      "grad_norm": 1.2210891246795654,
      "learning_rate": 9.810798338717121e-05,
      "loss": 2.3493,
      "step": 370
    },
    {
      "epoch": 0.05845254576219043,
      "grad_norm": 1.030822992324829,
      "learning_rate": 9.805670922422192e-05,
      "loss": 2.5064,
      "step": 380
    },
    {
      "epoch": 0.05999077065066913,
      "grad_norm": 1.2741059064865112,
      "learning_rate": 9.800543506127263e-05,
      "loss": 2.4764,
      "step": 390
    },
    {
      "epoch": 0.06152899553914783,
      "grad_norm": 1.203805685043335,
      "learning_rate": 9.795416089832334e-05,
      "loss": 2.447,
      "step": 400
    },
    {
      "epoch": 0.06306722042762652,
      "grad_norm": 1.0742758512496948,
      "learning_rate": 9.790288673537404e-05,
      "loss": 2.427,
      "step": 410
    },
    {
      "epoch": 0.06460544531610521,
      "grad_norm": 1.02286958694458,
      "learning_rate": 9.785161257242476e-05,
      "loss": 2.4394,
      "step": 420
    },
    {
      "epoch": 0.06614367020458391,
      "grad_norm": 0.9090739488601685,
      "learning_rate": 9.780033840947547e-05,
      "loss": 2.4173,
      "step": 430
    },
    {
      "epoch": 0.0676818950930626,
      "grad_norm": 1.149408221244812,
      "learning_rate": 9.774906424652618e-05,
      "loss": 2.456,
      "step": 440
    },
    {
      "epoch": 0.0692201199815413,
      "grad_norm": 0.963049590587616,
      "learning_rate": 9.769779008357689e-05,
      "loss": 2.3834,
      "step": 450
    },
    {
      "epoch": 0.07075834487002,
      "grad_norm": 0.9821059107780457,
      "learning_rate": 9.76465159206276e-05,
      "loss": 2.4565,
      "step": 460
    },
    {
      "epoch": 0.0722965697584987,
      "grad_norm": 1.0546011924743652,
      "learning_rate": 9.759524175767832e-05,
      "loss": 2.396,
      "step": 470
    },
    {
      "epoch": 0.07383479464697738,
      "grad_norm": 1.0994268655776978,
      "learning_rate": 9.754396759472903e-05,
      "loss": 2.4549,
      "step": 480
    },
    {
      "epoch": 0.07537301953545608,
      "grad_norm": 0.8789143562316895,
      "learning_rate": 9.749269343177974e-05,
      "loss": 2.4605,
      "step": 490
    },
    {
      "epoch": 0.07691124442393478,
      "grad_norm": 0.9876823425292969,
      "learning_rate": 9.744141926883043e-05,
      "loss": 2.4503,
      "step": 500
    },
    {
      "epoch": 0.07844946931241348,
      "grad_norm": 1.1950620412826538,
      "learning_rate": 9.739014510588114e-05,
      "loss": 2.5059,
      "step": 510
    },
    {
      "epoch": 0.07998769420089218,
      "grad_norm": 1.1006996631622314,
      "learning_rate": 9.733887094293186e-05,
      "loss": 2.2882,
      "step": 520
    },
    {
      "epoch": 0.08152591908937086,
      "grad_norm": 0.9053552150726318,
      "learning_rate": 9.728759677998257e-05,
      "loss": 2.3396,
      "step": 530
    },
    {
      "epoch": 0.08306414397784956,
      "grad_norm": 1.1737337112426758,
      "learning_rate": 9.723632261703328e-05,
      "loss": 2.4247,
      "step": 540
    },
    {
      "epoch": 0.08460236886632826,
      "grad_norm": 0.8845342993736267,
      "learning_rate": 9.718504845408399e-05,
      "loss": 2.3835,
      "step": 550
    },
    {
      "epoch": 0.08614059375480695,
      "grad_norm": 1.0540077686309814,
      "learning_rate": 9.71337742911347e-05,
      "loss": 2.369,
      "step": 560
    },
    {
      "epoch": 0.08767881864328565,
      "grad_norm": 0.9919202923774719,
      "learning_rate": 9.708250012818541e-05,
      "loss": 2.4425,
      "step": 570
    },
    {
      "epoch": 0.08921704353176435,
      "grad_norm": 0.9196600914001465,
      "learning_rate": 9.703122596523612e-05,
      "loss": 2.3381,
      "step": 580
    },
    {
      "epoch": 0.09075526842024303,
      "grad_norm": 1.1251168251037598,
      "learning_rate": 9.697995180228683e-05,
      "loss": 2.3186,
      "step": 590
    },
    {
      "epoch": 0.09229349330872173,
      "grad_norm": 1.3643468618392944,
      "learning_rate": 9.692867763933754e-05,
      "loss": 2.4726,
      "step": 600
    },
    {
      "epoch": 0.09383171819720043,
      "grad_norm": 0.8772892951965332,
      "learning_rate": 9.687740347638825e-05,
      "loss": 2.4736,
      "step": 610
    },
    {
      "epoch": 0.09536994308567913,
      "grad_norm": 1.061119556427002,
      "learning_rate": 9.682612931343897e-05,
      "loss": 2.4213,
      "step": 620
    },
    {
      "epoch": 0.09690816797415783,
      "grad_norm": 0.9421314597129822,
      "learning_rate": 9.677485515048968e-05,
      "loss": 2.4277,
      "step": 630
    },
    {
      "epoch": 0.09844639286263651,
      "grad_norm": 1.2146601676940918,
      "learning_rate": 9.672358098754039e-05,
      "loss": 2.2897,
      "step": 640
    },
    {
      "epoch": 0.09998461775111521,
      "grad_norm": 1.2775753736495972,
      "learning_rate": 9.66723068245911e-05,
      "loss": 2.4684,
      "step": 650
    },
    {
      "epoch": 0.10152284263959391,
      "grad_norm": 1.0711489915847778,
      "learning_rate": 9.66210326616418e-05,
      "loss": 2.3954,
      "step": 660
    },
    {
      "epoch": 0.1030610675280726,
      "grad_norm": 0.9492761492729187,
      "learning_rate": 9.656975849869251e-05,
      "loss": 2.3824,
      "step": 670
    },
    {
      "epoch": 0.1045992924165513,
      "grad_norm": 0.8662605285644531,
      "learning_rate": 9.651848433574322e-05,
      "loss": 2.2737,
      "step": 680
    },
    {
      "epoch": 0.10613751730503,
      "grad_norm": 0.9089555740356445,
      "learning_rate": 9.646721017279393e-05,
      "loss": 2.4527,
      "step": 690
    },
    {
      "epoch": 0.10767574219350869,
      "grad_norm": 0.8828738331794739,
      "learning_rate": 9.641593600984464e-05,
      "loss": 2.3143,
      "step": 700
    },
    {
      "epoch": 0.10921396708198738,
      "grad_norm": 0.8621166944503784,
      "learning_rate": 9.636466184689535e-05,
      "loss": 2.2994,
      "step": 710
    },
    {
      "epoch": 0.11075219197046608,
      "grad_norm": 0.9188641309738159,
      "learning_rate": 9.631338768394606e-05,
      "loss": 2.3995,
      "step": 720
    },
    {
      "epoch": 0.11229041685894478,
      "grad_norm": 0.9558343291282654,
      "learning_rate": 9.626211352099678e-05,
      "loss": 2.3769,
      "step": 730
    },
    {
      "epoch": 0.11382864174742348,
      "grad_norm": 0.8890336751937866,
      "learning_rate": 9.621083935804749e-05,
      "loss": 2.345,
      "step": 740
    },
    {
      "epoch": 0.11536686663590216,
      "grad_norm": 1.3892899751663208,
      "learning_rate": 9.61595651950982e-05,
      "loss": 2.3247,
      "step": 750
    },
    {
      "epoch": 0.11690509152438086,
      "grad_norm": 0.9079137444496155,
      "learning_rate": 9.61082910321489e-05,
      "loss": 2.5095,
      "step": 760
    },
    {
      "epoch": 0.11844331641285956,
      "grad_norm": 0.9309729933738708,
      "learning_rate": 9.60570168691996e-05,
      "loss": 2.3357,
      "step": 770
    },
    {
      "epoch": 0.11998154130133826,
      "grad_norm": 1.0791698694229126,
      "learning_rate": 9.600574270625033e-05,
      "loss": 2.3941,
      "step": 780
    },
    {
      "epoch": 0.12151976618981696,
      "grad_norm": 0.9481079578399658,
      "learning_rate": 9.595446854330104e-05,
      "loss": 2.3263,
      "step": 790
    },
    {
      "epoch": 0.12305799107829565,
      "grad_norm": 1.384405255317688,
      "learning_rate": 9.590319438035175e-05,
      "loss": 2.3435,
      "step": 800
    },
    {
      "epoch": 0.12459621596677434,
      "grad_norm": 0.9731020927429199,
      "learning_rate": 9.585192021740246e-05,
      "loss": 2.3627,
      "step": 810
    },
    {
      "epoch": 0.12613444085525305,
      "grad_norm": 0.7296550869941711,
      "learning_rate": 9.580064605445316e-05,
      "loss": 2.2322,
      "step": 820
    },
    {
      "epoch": 0.12767266574373173,
      "grad_norm": 1.0827419757843018,
      "learning_rate": 9.574937189150389e-05,
      "loss": 2.4777,
      "step": 830
    },
    {
      "epoch": 0.12921089063221042,
      "grad_norm": 0.99042809009552,
      "learning_rate": 9.569809772855458e-05,
      "loss": 2.4005,
      "step": 840
    },
    {
      "epoch": 0.13074911552068913,
      "grad_norm": 1.324620246887207,
      "learning_rate": 9.564682356560529e-05,
      "loss": 2.4839,
      "step": 850
    },
    {
      "epoch": 0.13228734040916781,
      "grad_norm": 1.4613444805145264,
      "learning_rate": 9.5595549402656e-05,
      "loss": 2.4002,
      "step": 860
    },
    {
      "epoch": 0.13382556529764653,
      "grad_norm": 0.9288026094436646,
      "learning_rate": 9.554427523970671e-05,
      "loss": 2.3541,
      "step": 870
    },
    {
      "epoch": 0.1353637901861252,
      "grad_norm": 1.129841685295105,
      "learning_rate": 9.549300107675743e-05,
      "loss": 2.3574,
      "step": 880
    },
    {
      "epoch": 0.1369020150746039,
      "grad_norm": 0.8787457346916199,
      "learning_rate": 9.544172691380814e-05,
      "loss": 2.3814,
      "step": 890
    },
    {
      "epoch": 0.1384402399630826,
      "grad_norm": 0.9640584588050842,
      "learning_rate": 9.539045275085885e-05,
      "loss": 2.3224,
      "step": 900
    },
    {
      "epoch": 0.1399784648515613,
      "grad_norm": 1.1772041320800781,
      "learning_rate": 9.533917858790956e-05,
      "loss": 2.3084,
      "step": 910
    },
    {
      "epoch": 0.14151668974004,
      "grad_norm": 0.8222691416740417,
      "learning_rate": 9.528790442496027e-05,
      "loss": 2.4192,
      "step": 920
    },
    {
      "epoch": 0.1430549146285187,
      "grad_norm": 0.8213545680046082,
      "learning_rate": 9.523663026201098e-05,
      "loss": 2.3573,
      "step": 930
    },
    {
      "epoch": 0.1445931395169974,
      "grad_norm": 1.1404590606689453,
      "learning_rate": 9.518535609906169e-05,
      "loss": 2.3049,
      "step": 940
    },
    {
      "epoch": 0.14613136440547608,
      "grad_norm": 1.1463240385055542,
      "learning_rate": 9.51340819361124e-05,
      "loss": 2.4483,
      "step": 950
    },
    {
      "epoch": 0.14766958929395477,
      "grad_norm": 1.2217503786087036,
      "learning_rate": 9.50828077731631e-05,
      "loss": 2.4451,
      "step": 960
    },
    {
      "epoch": 0.14920781418243348,
      "grad_norm": 0.8242997527122498,
      "learning_rate": 9.503153361021381e-05,
      "loss": 2.3317,
      "step": 970
    },
    {
      "epoch": 0.15074603907091216,
      "grad_norm": 1.1569207906723022,
      "learning_rate": 9.498025944726452e-05,
      "loss": 2.2817,
      "step": 980
    },
    {
      "epoch": 0.15228426395939088,
      "grad_norm": 1.1216130256652832,
      "learning_rate": 9.492898528431525e-05,
      "loss": 2.3543,
      "step": 990
    },
    {
      "epoch": 0.15382248884786956,
      "grad_norm": 1.131046175956726,
      "learning_rate": 9.487771112136596e-05,
      "loss": 2.3527,
      "step": 1000
    },
    {
      "epoch": 0.15536071373634824,
      "grad_norm": 1.2182101011276245,
      "learning_rate": 9.482643695841665e-05,
      "loss": 2.2678,
      "step": 1010
    },
    {
      "epoch": 0.15689893862482696,
      "grad_norm": 0.9577516317367554,
      "learning_rate": 9.477516279546736e-05,
      "loss": 2.4039,
      "step": 1020
    },
    {
      "epoch": 0.15843716351330564,
      "grad_norm": 1.0458829402923584,
      "learning_rate": 9.472388863251808e-05,
      "loss": 2.413,
      "step": 1030
    },
    {
      "epoch": 0.15997538840178435,
      "grad_norm": 0.9187437891960144,
      "learning_rate": 9.467261446956879e-05,
      "loss": 2.3664,
      "step": 1040
    },
    {
      "epoch": 0.16151361329026304,
      "grad_norm": 0.829723060131073,
      "learning_rate": 9.46213403066195e-05,
      "loss": 2.3837,
      "step": 1050
    },
    {
      "epoch": 0.16305183817874172,
      "grad_norm": 1.2389100790023804,
      "learning_rate": 9.457006614367021e-05,
      "loss": 2.3635,
      "step": 1060
    },
    {
      "epoch": 0.16459006306722043,
      "grad_norm": 1.0236015319824219,
      "learning_rate": 9.451879198072092e-05,
      "loss": 2.2544,
      "step": 1070
    },
    {
      "epoch": 0.16612828795569912,
      "grad_norm": 0.8570135831832886,
      "learning_rate": 9.446751781777163e-05,
      "loss": 2.3395,
      "step": 1080
    },
    {
      "epoch": 0.16766651284417783,
      "grad_norm": 0.9625948071479797,
      "learning_rate": 9.441624365482235e-05,
      "loss": 2.3859,
      "step": 1090
    },
    {
      "epoch": 0.1692047377326565,
      "grad_norm": 0.89539635181427,
      "learning_rate": 9.436496949187305e-05,
      "loss": 2.3562,
      "step": 1100
    },
    {
      "epoch": 0.1707429626211352,
      "grad_norm": 0.9785100221633911,
      "learning_rate": 9.431369532892376e-05,
      "loss": 2.2446,
      "step": 1110
    },
    {
      "epoch": 0.1722811875096139,
      "grad_norm": 1.1737624406814575,
      "learning_rate": 9.426242116597446e-05,
      "loss": 2.3936,
      "step": 1120
    },
    {
      "epoch": 0.1738194123980926,
      "grad_norm": 0.9451086521148682,
      "learning_rate": 9.421114700302517e-05,
      "loss": 2.4288,
      "step": 1130
    },
    {
      "epoch": 0.1753576372865713,
      "grad_norm": 0.8831786513328552,
      "learning_rate": 9.41598728400759e-05,
      "loss": 2.3756,
      "step": 1140
    },
    {
      "epoch": 0.17689586217505,
      "grad_norm": 1.3798778057098389,
      "learning_rate": 9.41085986771266e-05,
      "loss": 2.482,
      "step": 1150
    },
    {
      "epoch": 0.1784340870635287,
      "grad_norm": 0.9072500467300415,
      "learning_rate": 9.405732451417731e-05,
      "loss": 2.2366,
      "step": 1160
    },
    {
      "epoch": 0.17997231195200739,
      "grad_norm": 0.851820707321167,
      "learning_rate": 9.400605035122802e-05,
      "loss": 2.2708,
      "step": 1170
    },
    {
      "epoch": 0.18151053684048607,
      "grad_norm": 0.9339284300804138,
      "learning_rate": 9.395477618827873e-05,
      "loss": 2.268,
      "step": 1180
    },
    {
      "epoch": 0.18304876172896478,
      "grad_norm": 0.8741077184677124,
      "learning_rate": 9.390350202532944e-05,
      "loss": 2.3972,
      "step": 1190
    },
    {
      "epoch": 0.18458698661744347,
      "grad_norm": 1.1137354373931885,
      "learning_rate": 9.385222786238015e-05,
      "loss": 2.2709,
      "step": 1200
    },
    {
      "epoch": 0.18612521150592218,
      "grad_norm": 0.8969918489456177,
      "learning_rate": 9.380095369943086e-05,
      "loss": 2.3409,
      "step": 1210
    },
    {
      "epoch": 0.18766343639440086,
      "grad_norm": 1.399888515472412,
      "learning_rate": 9.374967953648157e-05,
      "loss": 2.356,
      "step": 1220
    },
    {
      "epoch": 0.18920166128287955,
      "grad_norm": 1.1646440029144287,
      "learning_rate": 9.369840537353228e-05,
      "loss": 2.3076,
      "step": 1230
    },
    {
      "epoch": 0.19073988617135826,
      "grad_norm": 1.1363041400909424,
      "learning_rate": 9.3647131210583e-05,
      "loss": 2.3777,
      "step": 1240
    },
    {
      "epoch": 0.19227811105983694,
      "grad_norm": 1.016471266746521,
      "learning_rate": 9.359585704763371e-05,
      "loss": 2.3759,
      "step": 1250
    },
    {
      "epoch": 0.19381633594831565,
      "grad_norm": 0.8463709950447083,
      "learning_rate": 9.354458288468442e-05,
      "loss": 2.2938,
      "step": 1260
    },
    {
      "epoch": 0.19535456083679434,
      "grad_norm": 1.0321208238601685,
      "learning_rate": 9.349330872173511e-05,
      "loss": 2.3482,
      "step": 1270
    },
    {
      "epoch": 0.19689278572527302,
      "grad_norm": 0.801830530166626,
      "learning_rate": 9.344203455878582e-05,
      "loss": 2.2615,
      "step": 1280
    },
    {
      "epoch": 0.19843101061375173,
      "grad_norm": 0.9908499121665955,
      "learning_rate": 9.339076039583655e-05,
      "loss": 2.2918,
      "step": 1290
    },
    {
      "epoch": 0.19996923550223042,
      "grad_norm": 0.9015299081802368,
      "learning_rate": 9.333948623288725e-05,
      "loss": 2.3913,
      "step": 1300
    },
    {
      "epoch": 0.20150746039070913,
      "grad_norm": 0.8517335653305054,
      "learning_rate": 9.328821206993796e-05,
      "loss": 2.3604,
      "step": 1310
    },
    {
      "epoch": 0.20304568527918782,
      "grad_norm": 0.7576839923858643,
      "learning_rate": 9.323693790698867e-05,
      "loss": 2.2648,
      "step": 1320
    },
    {
      "epoch": 0.2045839101676665,
      "grad_norm": 1.0572640895843506,
      "learning_rate": 9.318566374403938e-05,
      "loss": 2.3066,
      "step": 1330
    },
    {
      "epoch": 0.2061221350561452,
      "grad_norm": 1.2618391513824463,
      "learning_rate": 9.313438958109009e-05,
      "loss": 2.4479,
      "step": 1340
    },
    {
      "epoch": 0.2076603599446239,
      "grad_norm": 0.9082226753234863,
      "learning_rate": 9.30831154181408e-05,
      "loss": 2.319,
      "step": 1350
    },
    {
      "epoch": 0.2091985848331026,
      "grad_norm": 1.137063980102539,
      "learning_rate": 9.303184125519151e-05,
      "loss": 2.3538,
      "step": 1360
    },
    {
      "epoch": 0.2107368097215813,
      "grad_norm": 1.2156848907470703,
      "learning_rate": 9.298056709224222e-05,
      "loss": 2.3672,
      "step": 1370
    },
    {
      "epoch": 0.21227503461006,
      "grad_norm": 1.0615837574005127,
      "learning_rate": 9.292929292929293e-05,
      "loss": 2.318,
      "step": 1380
    },
    {
      "epoch": 0.2138132594985387,
      "grad_norm": 0.7031446695327759,
      "learning_rate": 9.287801876634365e-05,
      "loss": 2.3754,
      "step": 1390
    },
    {
      "epoch": 0.21535148438701737,
      "grad_norm": 1.0792100429534912,
      "learning_rate": 9.282674460339436e-05,
      "loss": 2.3928,
      "step": 1400
    },
    {
      "epoch": 0.21688970927549608,
      "grad_norm": 0.9547038078308105,
      "learning_rate": 9.277547044044507e-05,
      "loss": 2.3834,
      "step": 1410
    },
    {
      "epoch": 0.21842793416397477,
      "grad_norm": 0.9081752896308899,
      "learning_rate": 9.272419627749578e-05,
      "loss": 2.366,
      "step": 1420
    },
    {
      "epoch": 0.21996615905245348,
      "grad_norm": 0.8674251437187195,
      "learning_rate": 9.267292211454649e-05,
      "loss": 2.3302,
      "step": 1430
    },
    {
      "epoch": 0.22150438394093216,
      "grad_norm": 0.7524937391281128,
      "learning_rate": 9.26216479515972e-05,
      "loss": 2.3265,
      "step": 1440
    },
    {
      "epoch": 0.22304260882941085,
      "grad_norm": 1.0890637636184692,
      "learning_rate": 9.25703737886479e-05,
      "loss": 2.4002,
      "step": 1450
    },
    {
      "epoch": 0.22458083371788956,
      "grad_norm": 0.9604703783988953,
      "learning_rate": 9.251909962569861e-05,
      "loss": 2.3997,
      "step": 1460
    },
    {
      "epoch": 0.22611905860636825,
      "grad_norm": 0.9587043523788452,
      "learning_rate": 9.246782546274932e-05,
      "loss": 2.1888,
      "step": 1470
    },
    {
      "epoch": 0.22765728349484696,
      "grad_norm": 0.8627868294715881,
      "learning_rate": 9.241655129980003e-05,
      "loss": 2.2271,
      "step": 1480
    },
    {
      "epoch": 0.22919550838332564,
      "grad_norm": 1.6658141613006592,
      "learning_rate": 9.236527713685074e-05,
      "loss": 2.3808,
      "step": 1490
    },
    {
      "epoch": 0.23073373327180433,
      "grad_norm": 0.8292087316513062,
      "learning_rate": 9.231400297390146e-05,
      "loss": 2.3831,
      "step": 1500
    },
    {
      "epoch": 0.23227195816028304,
      "grad_norm": 1.0639952421188354,
      "learning_rate": 9.226272881095217e-05,
      "loss": 2.4253,
      "step": 1510
    },
    {
      "epoch": 0.23381018304876172,
      "grad_norm": 0.911286473274231,
      "learning_rate": 9.221145464800287e-05,
      "loss": 2.3882,
      "step": 1520
    },
    {
      "epoch": 0.23534840793724043,
      "grad_norm": 1.0653918981552124,
      "learning_rate": 9.216018048505358e-05,
      "loss": 2.262,
      "step": 1530
    },
    {
      "epoch": 0.23688663282571912,
      "grad_norm": 0.775364339351654,
      "learning_rate": 9.21089063221043e-05,
      "loss": 2.3303,
      "step": 1540
    },
    {
      "epoch": 0.2384248577141978,
      "grad_norm": 0.9708721041679382,
      "learning_rate": 9.205763215915501e-05,
      "loss": 2.2572,
      "step": 1550
    },
    {
      "epoch": 0.23996308260267651,
      "grad_norm": 1.3037928342819214,
      "learning_rate": 9.200635799620572e-05,
      "loss": 2.3202,
      "step": 1560
    },
    {
      "epoch": 0.2415013074911552,
      "grad_norm": 0.8447539210319519,
      "learning_rate": 9.195508383325643e-05,
      "loss": 2.3549,
      "step": 1570
    },
    {
      "epoch": 0.2430395323796339,
      "grad_norm": 0.9687184691429138,
      "learning_rate": 9.190380967030714e-05,
      "loss": 2.3997,
      "step": 1580
    },
    {
      "epoch": 0.2445777572681126,
      "grad_norm": 1.085591197013855,
      "learning_rate": 9.185253550735785e-05,
      "loss": 2.3954,
      "step": 1590
    },
    {
      "epoch": 0.2461159821565913,
      "grad_norm": 0.7975506782531738,
      "learning_rate": 9.180126134440857e-05,
      "loss": 2.4023,
      "step": 1600
    },
    {
      "epoch": 0.24765420704507,
      "grad_norm": 0.8976469039916992,
      "learning_rate": 9.174998718145926e-05,
      "loss": 2.2802,
      "step": 1610
    },
    {
      "epoch": 0.24919243193354867,
      "grad_norm": 1.018182396888733,
      "learning_rate": 9.169871301850997e-05,
      "loss": 2.3703,
      "step": 1620
    },
    {
      "epoch": 0.2507306568220274,
      "grad_norm": 0.7566492557525635,
      "learning_rate": 9.164743885556068e-05,
      "loss": 2.2385,
      "step": 1630
    },
    {
      "epoch": 0.2522688817105061,
      "grad_norm": 1.6852009296417236,
      "learning_rate": 9.159616469261139e-05,
      "loss": 2.367,
      "step": 1640
    },
    {
      "epoch": 0.25380710659898476,
      "grad_norm": 1.081018090248108,
      "learning_rate": 9.154489052966211e-05,
      "loss": 2.2997,
      "step": 1650
    },
    {
      "epoch": 0.25534533148746347,
      "grad_norm": 1.0540295839309692,
      "learning_rate": 9.149361636671282e-05,
      "loss": 2.2617,
      "step": 1660
    },
    {
      "epoch": 0.2568835563759422,
      "grad_norm": 1.0538369417190552,
      "learning_rate": 9.144234220376353e-05,
      "loss": 2.3,
      "step": 1670
    },
    {
      "epoch": 0.25842178126442084,
      "grad_norm": 0.8911623358726501,
      "learning_rate": 9.139106804081424e-05,
      "loss": 2.3207,
      "step": 1680
    },
    {
      "epoch": 0.25996000615289955,
      "grad_norm": 1.278957724571228,
      "learning_rate": 9.133979387786494e-05,
      "loss": 2.3398,
      "step": 1690
    },
    {
      "epoch": 0.26149823104137826,
      "grad_norm": 1.0844378471374512,
      "learning_rate": 9.128851971491566e-05,
      "loss": 2.3718,
      "step": 1700
    },
    {
      "epoch": 0.26303645592985697,
      "grad_norm": 0.988380491733551,
      "learning_rate": 9.123724555196637e-05,
      "loss": 2.3238,
      "step": 1710
    },
    {
      "epoch": 0.26457468081833563,
      "grad_norm": 0.7687071561813354,
      "learning_rate": 9.118597138901708e-05,
      "loss": 2.3395,
      "step": 1720
    },
    {
      "epoch": 0.26611290570681434,
      "grad_norm": 1.2303543090820312,
      "learning_rate": 9.113469722606779e-05,
      "loss": 2.3137,
      "step": 1730
    },
    {
      "epoch": 0.26765113059529305,
      "grad_norm": 1.3194670677185059,
      "learning_rate": 9.10834230631185e-05,
      "loss": 2.2938,
      "step": 1740
    },
    {
      "epoch": 0.2691893554837717,
      "grad_norm": 0.8565366268157959,
      "learning_rate": 9.103214890016922e-05,
      "loss": 2.1883,
      "step": 1750
    },
    {
      "epoch": 0.2707275803722504,
      "grad_norm": 0.9481441974639893,
      "learning_rate": 9.098087473721993e-05,
      "loss": 2.2142,
      "step": 1760
    },
    {
      "epoch": 0.27226580526072913,
      "grad_norm": 1.1457215547561646,
      "learning_rate": 9.092960057427064e-05,
      "loss": 2.3523,
      "step": 1770
    },
    {
      "epoch": 0.2738040301492078,
      "grad_norm": 0.8666920065879822,
      "learning_rate": 9.087832641132133e-05,
      "loss": 2.333,
      "step": 1780
    },
    {
      "epoch": 0.2753422550376865,
      "grad_norm": 0.9516326189041138,
      "learning_rate": 9.082705224837204e-05,
      "loss": 2.3227,
      "step": 1790
    },
    {
      "epoch": 0.2768804799261652,
      "grad_norm": 1.125641942024231,
      "learning_rate": 9.077577808542276e-05,
      "loss": 2.2811,
      "step": 1800
    },
    {
      "epoch": 0.2784187048146439,
      "grad_norm": 0.7248914837837219,
      "learning_rate": 9.072450392247347e-05,
      "loss": 2.229,
      "step": 1810
    },
    {
      "epoch": 0.2799569297031226,
      "grad_norm": 1.1648125648498535,
      "learning_rate": 9.067322975952418e-05,
      "loss": 2.3461,
      "step": 1820
    },
    {
      "epoch": 0.2814951545916013,
      "grad_norm": 0.9306198954582214,
      "learning_rate": 9.062195559657489e-05,
      "loss": 2.4411,
      "step": 1830
    },
    {
      "epoch": 0.28303337948008,
      "grad_norm": 0.8632981777191162,
      "learning_rate": 9.05706814336256e-05,
      "loss": 2.3147,
      "step": 1840
    },
    {
      "epoch": 0.28457160436855866,
      "grad_norm": 1.0031105279922485,
      "learning_rate": 9.051940727067631e-05,
      "loss": 2.2313,
      "step": 1850
    },
    {
      "epoch": 0.2861098292570374,
      "grad_norm": 1.087646484375,
      "learning_rate": 9.046813310772702e-05,
      "loss": 2.3478,
      "step": 1860
    },
    {
      "epoch": 0.2876480541455161,
      "grad_norm": 1.4194533824920654,
      "learning_rate": 9.041685894477773e-05,
      "loss": 2.2992,
      "step": 1870
    },
    {
      "epoch": 0.2891862790339948,
      "grad_norm": 0.7619257569313049,
      "learning_rate": 9.036558478182844e-05,
      "loss": 2.2513,
      "step": 1880
    },
    {
      "epoch": 0.29072450392247345,
      "grad_norm": 0.8279052376747131,
      "learning_rate": 9.031431061887915e-05,
      "loss": 2.2539,
      "step": 1890
    },
    {
      "epoch": 0.29226272881095217,
      "grad_norm": 1.0974736213684082,
      "learning_rate": 9.026303645592987e-05,
      "loss": 2.2179,
      "step": 1900
    },
    {
      "epoch": 0.2938009536994309,
      "grad_norm": 0.9489269256591797,
      "learning_rate": 9.021176229298058e-05,
      "loss": 2.3707,
      "step": 1910
    },
    {
      "epoch": 0.29533917858790953,
      "grad_norm": 1.0058236122131348,
      "learning_rate": 9.016048813003129e-05,
      "loss": 2.2795,
      "step": 1920
    },
    {
      "epoch": 0.29687740347638825,
      "grad_norm": 0.9993000626564026,
      "learning_rate": 9.0109213967082e-05,
      "loss": 2.3638,
      "step": 1930
    },
    {
      "epoch": 0.29841562836486696,
      "grad_norm": 1.0579222440719604,
      "learning_rate": 9.00579398041327e-05,
      "loss": 2.2531,
      "step": 1940
    },
    {
      "epoch": 0.2999538532533456,
      "grad_norm": 0.7501667141914368,
      "learning_rate": 9.000666564118341e-05,
      "loss": 2.2578,
      "step": 1950
    },
    {
      "epoch": 0.3014920781418243,
      "grad_norm": 1.1398831605911255,
      "learning_rate": 8.995539147823412e-05,
      "loss": 2.2054,
      "step": 1960
    },
    {
      "epoch": 0.30303030303030304,
      "grad_norm": 0.9692484140396118,
      "learning_rate": 8.990411731528483e-05,
      "loss": 2.2561,
      "step": 1970
    },
    {
      "epoch": 0.30456852791878175,
      "grad_norm": 1.1439406871795654,
      "learning_rate": 8.985284315233554e-05,
      "loss": 2.317,
      "step": 1980
    },
    {
      "epoch": 0.3061067528072604,
      "grad_norm": 0.9182436466217041,
      "learning_rate": 8.980156898938625e-05,
      "loss": 2.3221,
      "step": 1990
    },
    {
      "epoch": 0.3076449776957391,
      "grad_norm": 0.8540138602256775,
      "learning_rate": 8.975029482643696e-05,
      "loss": 2.258,
      "step": 2000
    },
    {
      "epoch": 0.30918320258421783,
      "grad_norm": 0.9826604127883911,
      "learning_rate": 8.969902066348768e-05,
      "loss": 2.3034,
      "step": 2010
    },
    {
      "epoch": 0.3107214274726965,
      "grad_norm": 0.9897715449333191,
      "learning_rate": 8.964774650053839e-05,
      "loss": 2.2375,
      "step": 2020
    },
    {
      "epoch": 0.3122596523611752,
      "grad_norm": 0.9915783405303955,
      "learning_rate": 8.959647233758909e-05,
      "loss": 2.282,
      "step": 2030
    },
    {
      "epoch": 0.3137978772496539,
      "grad_norm": 0.7774962782859802,
      "learning_rate": 8.95451981746398e-05,
      "loss": 2.2192,
      "step": 2040
    },
    {
      "epoch": 0.31533610213813257,
      "grad_norm": 1.0849714279174805,
      "learning_rate": 8.94939240116905e-05,
      "loss": 2.1735,
      "step": 2050
    },
    {
      "epoch": 0.3168743270266113,
      "grad_norm": 1.3296504020690918,
      "learning_rate": 8.944264984874123e-05,
      "loss": 2.3533,
      "step": 2060
    },
    {
      "epoch": 0.31841255191509,
      "grad_norm": 0.8607456684112549,
      "learning_rate": 8.939137568579194e-05,
      "loss": 2.2663,
      "step": 2070
    },
    {
      "epoch": 0.3199507768035687,
      "grad_norm": 0.9673560857772827,
      "learning_rate": 8.934010152284265e-05,
      "loss": 2.2401,
      "step": 2080
    },
    {
      "epoch": 0.32148900169204736,
      "grad_norm": 0.9406920075416565,
      "learning_rate": 8.928882735989335e-05,
      "loss": 2.3318,
      "step": 2090
    },
    {
      "epoch": 0.3230272265805261,
      "grad_norm": 0.9379817247390747,
      "learning_rate": 8.923755319694406e-05,
      "loss": 2.2699,
      "step": 2100
    },
    {
      "epoch": 0.3245654514690048,
      "grad_norm": 0.9792314767837524,
      "learning_rate": 8.918627903399479e-05,
      "loss": 2.312,
      "step": 2110
    },
    {
      "epoch": 0.32610367635748344,
      "grad_norm": 1.2682822942733765,
      "learning_rate": 8.913500487104548e-05,
      "loss": 2.1895,
      "step": 2120
    },
    {
      "epoch": 0.32764190124596215,
      "grad_norm": 1.0079998970031738,
      "learning_rate": 8.908373070809619e-05,
      "loss": 2.2983,
      "step": 2130
    },
    {
      "epoch": 0.32918012613444086,
      "grad_norm": 0.9478371739387512,
      "learning_rate": 8.90324565451469e-05,
      "loss": 2.3464,
      "step": 2140
    },
    {
      "epoch": 0.3307183510229196,
      "grad_norm": 1.0139496326446533,
      "learning_rate": 8.898118238219761e-05,
      "loss": 2.3395,
      "step": 2150
    },
    {
      "epoch": 0.33225657591139823,
      "grad_norm": 0.8629897832870483,
      "learning_rate": 8.892990821924833e-05,
      "loss": 2.2077,
      "step": 2160
    },
    {
      "epoch": 0.33379480079987695,
      "grad_norm": 1.4620726108551025,
      "learning_rate": 8.887863405629904e-05,
      "loss": 2.3096,
      "step": 2170
    },
    {
      "epoch": 0.33533302568835566,
      "grad_norm": 0.9702004790306091,
      "learning_rate": 8.882735989334975e-05,
      "loss": 2.3519,
      "step": 2180
    },
    {
      "epoch": 0.3368712505768343,
      "grad_norm": 0.9932650327682495,
      "learning_rate": 8.877608573040046e-05,
      "loss": 2.3453,
      "step": 2190
    },
    {
      "epoch": 0.338409475465313,
      "grad_norm": 0.9327139258384705,
      "learning_rate": 8.872481156745115e-05,
      "loss": 2.2213,
      "step": 2200
    },
    {
      "epoch": 0.33994770035379174,
      "grad_norm": 0.8932101130485535,
      "learning_rate": 8.867353740450188e-05,
      "loss": 2.3821,
      "step": 2210
    },
    {
      "epoch": 0.3414859252422704,
      "grad_norm": 0.8775930404663086,
      "learning_rate": 8.862226324155259e-05,
      "loss": 2.3518,
      "step": 2220
    },
    {
      "epoch": 0.3430241501307491,
      "grad_norm": 1.0364453792572021,
      "learning_rate": 8.85709890786033e-05,
      "loss": 2.2383,
      "step": 2230
    },
    {
      "epoch": 0.3445623750192278,
      "grad_norm": 0.8616227507591248,
      "learning_rate": 8.8519714915654e-05,
      "loss": 2.2831,
      "step": 2240
    },
    {
      "epoch": 0.34610059990770653,
      "grad_norm": 1.1039588451385498,
      "learning_rate": 8.846844075270471e-05,
      "loss": 2.1571,
      "step": 2250
    },
    {
      "epoch": 0.3476388247961852,
      "grad_norm": 0.9709256887435913,
      "learning_rate": 8.841716658975544e-05,
      "loss": 2.4468,
      "step": 2260
    },
    {
      "epoch": 0.3491770496846639,
      "grad_norm": 1.002502679824829,
      "learning_rate": 8.836589242680614e-05,
      "loss": 2.3132,
      "step": 2270
    },
    {
      "epoch": 0.3507152745731426,
      "grad_norm": 0.9301533699035645,
      "learning_rate": 8.831461826385685e-05,
      "loss": 2.2703,
      "step": 2280
    },
    {
      "epoch": 0.35225349946162127,
      "grad_norm": 0.7622637748718262,
      "learning_rate": 8.826334410090755e-05,
      "loss": 2.3035,
      "step": 2290
    },
    {
      "epoch": 0.3537917243501,
      "grad_norm": 1.1477632522583008,
      "learning_rate": 8.821206993795826e-05,
      "loss": 2.3787,
      "step": 2300
    },
    {
      "epoch": 0.3553299492385787,
      "grad_norm": 0.7521834969520569,
      "learning_rate": 8.816079577500898e-05,
      "loss": 2.3054,
      "step": 2310
    },
    {
      "epoch": 0.3568681741270574,
      "grad_norm": 0.8684582114219666,
      "learning_rate": 8.810952161205969e-05,
      "loss": 2.2498,
      "step": 2320
    },
    {
      "epoch": 0.35840639901553606,
      "grad_norm": 0.8882172107696533,
      "learning_rate": 8.80582474491104e-05,
      "loss": 2.2009,
      "step": 2330
    },
    {
      "epoch": 0.35994462390401477,
      "grad_norm": 0.9389574527740479,
      "learning_rate": 8.800697328616111e-05,
      "loss": 2.2551,
      "step": 2340
    },
    {
      "epoch": 0.3614828487924935,
      "grad_norm": 0.7628786563873291,
      "learning_rate": 8.795569912321182e-05,
      "loss": 2.2782,
      "step": 2350
    },
    {
      "epoch": 0.36302107368097214,
      "grad_norm": 1.4449824094772339,
      "learning_rate": 8.790442496026253e-05,
      "loss": 2.3346,
      "step": 2360
    },
    {
      "epoch": 0.36455929856945085,
      "grad_norm": 0.8784567713737488,
      "learning_rate": 8.785315079731324e-05,
      "loss": 2.3712,
      "step": 2370
    },
    {
      "epoch": 0.36609752345792956,
      "grad_norm": 0.9509233236312866,
      "learning_rate": 8.780187663436394e-05,
      "loss": 2.2112,
      "step": 2380
    },
    {
      "epoch": 0.3676357483464082,
      "grad_norm": 0.7432436347007751,
      "learning_rate": 8.775060247141465e-05,
      "loss": 2.2662,
      "step": 2390
    },
    {
      "epoch": 0.36917397323488693,
      "grad_norm": 0.7712249755859375,
      "learning_rate": 8.769932830846536e-05,
      "loss": 2.1885,
      "step": 2400
    },
    {
      "epoch": 0.37071219812336564,
      "grad_norm": 0.9576478600502014,
      "learning_rate": 8.764805414551607e-05,
      "loss": 2.267,
      "step": 2410
    },
    {
      "epoch": 0.37225042301184436,
      "grad_norm": 0.8779605627059937,
      "learning_rate": 8.75967799825668e-05,
      "loss": 2.246,
      "step": 2420
    },
    {
      "epoch": 0.373788647900323,
      "grad_norm": 0.8408971428871155,
      "learning_rate": 8.75455058196175e-05,
      "loss": 2.2216,
      "step": 2430
    },
    {
      "epoch": 0.3753268727888017,
      "grad_norm": 0.9785459637641907,
      "learning_rate": 8.749423165666821e-05,
      "loss": 2.3978,
      "step": 2440
    },
    {
      "epoch": 0.37686509767728044,
      "grad_norm": 1.1419650316238403,
      "learning_rate": 8.744295749371892e-05,
      "loss": 2.3577,
      "step": 2450
    },
    {
      "epoch": 0.3784033225657591,
      "grad_norm": 0.8390716314315796,
      "learning_rate": 8.739168333076963e-05,
      "loss": 2.2553,
      "step": 2460
    },
    {
      "epoch": 0.3799415474542378,
      "grad_norm": 0.7990594506263733,
      "learning_rate": 8.734040916782034e-05,
      "loss": 2.3093,
      "step": 2470
    },
    {
      "epoch": 0.3814797723427165,
      "grad_norm": 0.9870525002479553,
      "learning_rate": 8.728913500487105e-05,
      "loss": 2.2984,
      "step": 2480
    },
    {
      "epoch": 0.3830179972311952,
      "grad_norm": 1.096959114074707,
      "learning_rate": 8.723786084192176e-05,
      "loss": 2.2643,
      "step": 2490
    },
    {
      "epoch": 0.3845562221196739,
      "grad_norm": 0.9146120548248291,
      "learning_rate": 8.718658667897247e-05,
      "loss": 2.3411,
      "step": 2500
    },
    {
      "epoch": 0.3860944470081526,
      "grad_norm": 0.8757959008216858,
      "learning_rate": 8.713531251602318e-05,
      "loss": 2.2722,
      "step": 2510
    },
    {
      "epoch": 0.3876326718966313,
      "grad_norm": 1.1926383972167969,
      "learning_rate": 8.70840383530739e-05,
      "loss": 2.327,
      "step": 2520
    },
    {
      "epoch": 0.38917089678510997,
      "grad_norm": 0.9634771943092346,
      "learning_rate": 8.703276419012461e-05,
      "loss": 2.2203,
      "step": 2530
    },
    {
      "epoch": 0.3907091216735887,
      "grad_norm": 1.119310975074768,
      "learning_rate": 8.69814900271753e-05,
      "loss": 2.3041,
      "step": 2540
    },
    {
      "epoch": 0.3922473465620674,
      "grad_norm": 0.788459062576294,
      "learning_rate": 8.693021586422601e-05,
      "loss": 2.2488,
      "step": 2550
    },
    {
      "epoch": 0.39378557145054605,
      "grad_norm": 0.9462425708770752,
      "learning_rate": 8.687894170127672e-05,
      "loss": 2.255,
      "step": 2560
    },
    {
      "epoch": 0.39532379633902476,
      "grad_norm": 0.9861125349998474,
      "learning_rate": 8.682766753832744e-05,
      "loss": 2.2465,
      "step": 2570
    },
    {
      "epoch": 0.39686202122750347,
      "grad_norm": 1.1777220964431763,
      "learning_rate": 8.677639337537815e-05,
      "loss": 2.2967,
      "step": 2580
    },
    {
      "epoch": 0.3984002461159822,
      "grad_norm": 1.0811834335327148,
      "learning_rate": 8.672511921242886e-05,
      "loss": 2.2712,
      "step": 2590
    },
    {
      "epoch": 0.39993847100446084,
      "grad_norm": 1.0035991668701172,
      "learning_rate": 8.667384504947957e-05,
      "loss": 2.3246,
      "step": 2600
    },
    {
      "epoch": 0.40147669589293955,
      "grad_norm": 0.956218421459198,
      "learning_rate": 8.662257088653028e-05,
      "loss": 2.1974,
      "step": 2610
    },
    {
      "epoch": 0.40301492078141826,
      "grad_norm": 1.0518748760223389,
      "learning_rate": 8.6571296723581e-05,
      "loss": 2.3071,
      "step": 2620
    },
    {
      "epoch": 0.4045531456698969,
      "grad_norm": 1.1680355072021484,
      "learning_rate": 8.65200225606317e-05,
      "loss": 2.4153,
      "step": 2630
    },
    {
      "epoch": 0.40609137055837563,
      "grad_norm": 0.9885054230690002,
      "learning_rate": 8.646874839768241e-05,
      "loss": 2.3113,
      "step": 2640
    },
    {
      "epoch": 0.40762959544685434,
      "grad_norm": 0.8083099126815796,
      "learning_rate": 8.641747423473312e-05,
      "loss": 2.3058,
      "step": 2650
    },
    {
      "epoch": 0.409167820335333,
      "grad_norm": 1.118858814239502,
      "learning_rate": 8.636620007178383e-05,
      "loss": 2.3899,
      "step": 2660
    },
    {
      "epoch": 0.4107060452238117,
      "grad_norm": 0.8994693160057068,
      "learning_rate": 8.631492590883455e-05,
      "loss": 2.3393,
      "step": 2670
    },
    {
      "epoch": 0.4122442701122904,
      "grad_norm": 0.7934942245483398,
      "learning_rate": 8.626365174588526e-05,
      "loss": 2.3531,
      "step": 2680
    },
    {
      "epoch": 0.41378249500076913,
      "grad_norm": 1.1432355642318726,
      "learning_rate": 8.621237758293597e-05,
      "loss": 2.3171,
      "step": 2690
    },
    {
      "epoch": 0.4153207198892478,
      "grad_norm": 1.0568222999572754,
      "learning_rate": 8.616110341998668e-05,
      "loss": 2.2949,
      "step": 2700
    },
    {
      "epoch": 0.4168589447777265,
      "grad_norm": 1.3339487314224243,
      "learning_rate": 8.610982925703737e-05,
      "loss": 2.2467,
      "step": 2710
    },
    {
      "epoch": 0.4183971696662052,
      "grad_norm": 0.891101062297821,
      "learning_rate": 8.60585550940881e-05,
      "loss": 2.1772,
      "step": 2720
    },
    {
      "epoch": 0.41993539455468387,
      "grad_norm": 0.9895696640014648,
      "learning_rate": 8.60072809311388e-05,
      "loss": 2.2511,
      "step": 2730
    },
    {
      "epoch": 0.4214736194431626,
      "grad_norm": 1.5037952661514282,
      "learning_rate": 8.595600676818951e-05,
      "loss": 2.3094,
      "step": 2740
    },
    {
      "epoch": 0.4230118443316413,
      "grad_norm": 1.3172814846038818,
      "learning_rate": 8.590473260524022e-05,
      "loss": 2.2435,
      "step": 2750
    },
    {
      "epoch": 0.42455006922012,
      "grad_norm": 1.0354995727539062,
      "learning_rate": 8.585345844229093e-05,
      "loss": 2.2546,
      "step": 2760
    },
    {
      "epoch": 0.42608829410859866,
      "grad_norm": 0.7781267762184143,
      "learning_rate": 8.580218427934164e-05,
      "loss": 2.2895,
      "step": 2770
    },
    {
      "epoch": 0.4276265189970774,
      "grad_norm": 0.9660653471946716,
      "learning_rate": 8.575091011639236e-05,
      "loss": 2.3258,
      "step": 2780
    },
    {
      "epoch": 0.4291647438855561,
      "grad_norm": 1.0293235778808594,
      "learning_rate": 8.569963595344307e-05,
      "loss": 2.1913,
      "step": 2790
    },
    {
      "epoch": 0.43070296877403474,
      "grad_norm": 0.9636089205741882,
      "learning_rate": 8.564836179049377e-05,
      "loss": 2.2229,
      "step": 2800
    },
    {
      "epoch": 0.43224119366251346,
      "grad_norm": 1.0417877435684204,
      "learning_rate": 8.559708762754448e-05,
      "loss": 2.2899,
      "step": 2810
    },
    {
      "epoch": 0.43377941855099217,
      "grad_norm": 0.9344021081924438,
      "learning_rate": 8.55458134645952e-05,
      "loss": 2.184,
      "step": 2820
    },
    {
      "epoch": 0.4353176434394708,
      "grad_norm": 0.6903371810913086,
      "learning_rate": 8.549453930164591e-05,
      "loss": 2.3606,
      "step": 2830
    },
    {
      "epoch": 0.43685586832794954,
      "grad_norm": 0.9873772859573364,
      "learning_rate": 8.544326513869662e-05,
      "loss": 2.2671,
      "step": 2840
    },
    {
      "epoch": 0.43839409321642825,
      "grad_norm": 0.8802867531776428,
      "learning_rate": 8.539199097574733e-05,
      "loss": 2.3309,
      "step": 2850
    },
    {
      "epoch": 0.43993231810490696,
      "grad_norm": 1.0014759302139282,
      "learning_rate": 8.534071681279804e-05,
      "loss": 2.3369,
      "step": 2860
    },
    {
      "epoch": 0.4414705429933856,
      "grad_norm": 0.9903216361999512,
      "learning_rate": 8.528944264984874e-05,
      "loss": 2.2767,
      "step": 2870
    },
    {
      "epoch": 0.44300876788186433,
      "grad_norm": 0.8472523093223572,
      "learning_rate": 8.523816848689945e-05,
      "loss": 2.3227,
      "step": 2880
    },
    {
      "epoch": 0.44454699277034304,
      "grad_norm": 0.9691609740257263,
      "learning_rate": 8.518689432395016e-05,
      "loss": 2.4172,
      "step": 2890
    },
    {
      "epoch": 0.4460852176588217,
      "grad_norm": 1.1441282033920288,
      "learning_rate": 8.513562016100087e-05,
      "loss": 2.2578,
      "step": 2900
    },
    {
      "epoch": 0.4476234425473004,
      "grad_norm": 0.9551346302032471,
      "learning_rate": 8.508434599805158e-05,
      "loss": 2.2904,
      "step": 2910
    },
    {
      "epoch": 0.4491616674357791,
      "grad_norm": 0.8791012167930603,
      "learning_rate": 8.503307183510229e-05,
      "loss": 2.3203,
      "step": 2920
    },
    {
      "epoch": 0.45069989232425783,
      "grad_norm": 0.9215505719184875,
      "learning_rate": 8.498179767215301e-05,
      "loss": 2.2702,
      "step": 2930
    },
    {
      "epoch": 0.4522381172127365,
      "grad_norm": 0.9251764416694641,
      "learning_rate": 8.493052350920372e-05,
      "loss": 2.1976,
      "step": 2940
    },
    {
      "epoch": 0.4537763421012152,
      "grad_norm": 0.9312923550605774,
      "learning_rate": 8.487924934625443e-05,
      "loss": 2.2651,
      "step": 2950
    },
    {
      "epoch": 0.4553145669896939,
      "grad_norm": 1.0302073955535889,
      "learning_rate": 8.482797518330514e-05,
      "loss": 2.2914,
      "step": 2960
    },
    {
      "epoch": 0.45685279187817257,
      "grad_norm": 1.1868935823440552,
      "learning_rate": 8.477670102035585e-05,
      "loss": 2.3211,
      "step": 2970
    },
    {
      "epoch": 0.4583910167666513,
      "grad_norm": 0.8748847246170044,
      "learning_rate": 8.472542685740656e-05,
      "loss": 2.3304,
      "step": 2980
    },
    {
      "epoch": 0.45992924165513,
      "grad_norm": 0.9357801079750061,
      "learning_rate": 8.467415269445727e-05,
      "loss": 2.298,
      "step": 2990
    },
    {
      "epoch": 0.46146746654360865,
      "grad_norm": 0.940485954284668,
      "learning_rate": 8.462287853150798e-05,
      "loss": 2.3305,
      "step": 3000
    },
    {
      "epoch": 0.46300569143208736,
      "grad_norm": 0.9531272649765015,
      "learning_rate": 8.457160436855869e-05,
      "loss": 2.3077,
      "step": 3010
    },
    {
      "epoch": 0.4645439163205661,
      "grad_norm": 0.9768287539482117,
      "learning_rate": 8.45203302056094e-05,
      "loss": 2.2359,
      "step": 3020
    },
    {
      "epoch": 0.4660821412090448,
      "grad_norm": 0.9510540962219238,
      "learning_rate": 8.446905604266012e-05,
      "loss": 2.3435,
      "step": 3030
    },
    {
      "epoch": 0.46762036609752344,
      "grad_norm": 1.1645132303237915,
      "learning_rate": 8.441778187971083e-05,
      "loss": 2.2255,
      "step": 3040
    },
    {
      "epoch": 0.46915859098600216,
      "grad_norm": 0.6672899723052979,
      "learning_rate": 8.436650771676152e-05,
      "loss": 2.2322,
      "step": 3050
    },
    {
      "epoch": 0.47069681587448087,
      "grad_norm": 0.9119551181793213,
      "learning_rate": 8.431523355381223e-05,
      "loss": 2.2992,
      "step": 3060
    },
    {
      "epoch": 0.4722350407629595,
      "grad_norm": 0.9865890145301819,
      "learning_rate": 8.426395939086294e-05,
      "loss": 2.3073,
      "step": 3070
    },
    {
      "epoch": 0.47377326565143824,
      "grad_norm": 1.192719578742981,
      "learning_rate": 8.421268522791366e-05,
      "loss": 2.309,
      "step": 3080
    },
    {
      "epoch": 0.47531149053991695,
      "grad_norm": 0.8836092948913574,
      "learning_rate": 8.416141106496437e-05,
      "loss": 2.2698,
      "step": 3090
    },
    {
      "epoch": 0.4768497154283956,
      "grad_norm": 0.8378663063049316,
      "learning_rate": 8.411013690201508e-05,
      "loss": 2.3078,
      "step": 3100
    },
    {
      "epoch": 0.4783879403168743,
      "grad_norm": 1.0443509817123413,
      "learning_rate": 8.405886273906579e-05,
      "loss": 2.2434,
      "step": 3110
    },
    {
      "epoch": 0.47992616520535303,
      "grad_norm": 1.077728509902954,
      "learning_rate": 8.40075885761165e-05,
      "loss": 2.2901,
      "step": 3120
    },
    {
      "epoch": 0.48146439009383174,
      "grad_norm": 1.0350403785705566,
      "learning_rate": 8.395631441316721e-05,
      "loss": 2.2862,
      "step": 3130
    },
    {
      "epoch": 0.4830026149823104,
      "grad_norm": 1.062722086906433,
      "learning_rate": 8.390504025021792e-05,
      "loss": 2.203,
      "step": 3140
    },
    {
      "epoch": 0.4845408398707891,
      "grad_norm": 0.7822374701499939,
      "learning_rate": 8.385376608726863e-05,
      "loss": 2.2845,
      "step": 3150
    },
    {
      "epoch": 0.4860790647592678,
      "grad_norm": 1.0216268301010132,
      "learning_rate": 8.380249192431933e-05,
      "loss": 2.2627,
      "step": 3160
    },
    {
      "epoch": 0.4876172896477465,
      "grad_norm": 0.7049469351768494,
      "learning_rate": 8.375121776137004e-05,
      "loss": 2.226,
      "step": 3170
    },
    {
      "epoch": 0.4891555145362252,
      "grad_norm": 0.8990631103515625,
      "learning_rate": 8.369994359842077e-05,
      "loss": 2.2634,
      "step": 3180
    },
    {
      "epoch": 0.4906937394247039,
      "grad_norm": 1.1030187606811523,
      "learning_rate": 8.364866943547148e-05,
      "loss": 2.3319,
      "step": 3190
    },
    {
      "epoch": 0.4922319643131826,
      "grad_norm": 0.8706591129302979,
      "learning_rate": 8.359739527252218e-05,
      "loss": 2.2735,
      "step": 3200
    },
    {
      "epoch": 0.49377018920166127,
      "grad_norm": 1.1134240627288818,
      "learning_rate": 8.35461211095729e-05,
      "loss": 2.2,
      "step": 3210
    },
    {
      "epoch": 0.49530841409014,
      "grad_norm": 0.8919715881347656,
      "learning_rate": 8.349484694662359e-05,
      "loss": 2.3366,
      "step": 3220
    },
    {
      "epoch": 0.4968466389786187,
      "grad_norm": 0.8236812353134155,
      "learning_rate": 8.344357278367431e-05,
      "loss": 2.2746,
      "step": 3230
    },
    {
      "epoch": 0.49838486386709735,
      "grad_norm": 0.7762203216552734,
      "learning_rate": 8.339229862072502e-05,
      "loss": 2.2766,
      "step": 3240
    },
    {
      "epoch": 0.49992308875557606,
      "grad_norm": 0.7744325399398804,
      "learning_rate": 8.334102445777573e-05,
      "loss": 2.3017,
      "step": 3250
    },
    {
      "epoch": 0.5014613136440548,
      "grad_norm": 1.2258903980255127,
      "learning_rate": 8.328975029482644e-05,
      "loss": 2.21,
      "step": 3260
    },
    {
      "epoch": 0.5029995385325334,
      "grad_norm": 0.735567033290863,
      "learning_rate": 8.323847613187715e-05,
      "loss": 2.3122,
      "step": 3270
    },
    {
      "epoch": 0.5045377634210122,
      "grad_norm": 1.1336249113082886,
      "learning_rate": 8.318720196892786e-05,
      "loss": 2.2535,
      "step": 3280
    },
    {
      "epoch": 0.5060759883094909,
      "grad_norm": 0.8732126355171204,
      "learning_rate": 8.313592780597858e-05,
      "loss": 2.3059,
      "step": 3290
    },
    {
      "epoch": 0.5076142131979695,
      "grad_norm": 1.1084136962890625,
      "learning_rate": 8.308465364302929e-05,
      "loss": 2.2459,
      "step": 3300
    },
    {
      "epoch": 0.5091524380864483,
      "grad_norm": 0.9033851623535156,
      "learning_rate": 8.303337948007998e-05,
      "loss": 2.2607,
      "step": 3310
    },
    {
      "epoch": 0.5106906629749269,
      "grad_norm": 0.8595645427703857,
      "learning_rate": 8.29821053171307e-05,
      "loss": 2.2776,
      "step": 3320
    },
    {
      "epoch": 0.5122288878634056,
      "grad_norm": 0.8238754868507385,
      "learning_rate": 8.293083115418142e-05,
      "loss": 2.2605,
      "step": 3330
    },
    {
      "epoch": 0.5137671127518844,
      "grad_norm": 1.060767412185669,
      "learning_rate": 8.287955699123213e-05,
      "loss": 2.2651,
      "step": 3340
    },
    {
      "epoch": 0.515305337640363,
      "grad_norm": 1.2983146905899048,
      "learning_rate": 8.282828282828283e-05,
      "loss": 2.223,
      "step": 3350
    },
    {
      "epoch": 0.5168435625288417,
      "grad_norm": 1.1229500770568848,
      "learning_rate": 8.277700866533354e-05,
      "loss": 2.2844,
      "step": 3360
    },
    {
      "epoch": 0.5183817874173204,
      "grad_norm": 1.26649808883667,
      "learning_rate": 8.272573450238425e-05,
      "loss": 2.2858,
      "step": 3370
    },
    {
      "epoch": 0.5199200123057991,
      "grad_norm": 1.1915905475616455,
      "learning_rate": 8.267446033943496e-05,
      "loss": 2.3307,
      "step": 3380
    },
    {
      "epoch": 0.5214582371942778,
      "grad_norm": 0.9278625249862671,
      "learning_rate": 8.262318617648567e-05,
      "loss": 2.1453,
      "step": 3390
    },
    {
      "epoch": 0.5229964620827565,
      "grad_norm": 1.1981624364852905,
      "learning_rate": 8.257191201353638e-05,
      "loss": 2.3982,
      "step": 3400
    },
    {
      "epoch": 0.5245346869712352,
      "grad_norm": 1.0816112756729126,
      "learning_rate": 8.252063785058709e-05,
      "loss": 2.2832,
      "step": 3410
    },
    {
      "epoch": 0.5260729118597139,
      "grad_norm": 0.8670153021812439,
      "learning_rate": 8.24693636876378e-05,
      "loss": 2.3417,
      "step": 3420
    },
    {
      "epoch": 0.5276111367481926,
      "grad_norm": 1.2614654302597046,
      "learning_rate": 8.241808952468851e-05,
      "loss": 2.4044,
      "step": 3430
    },
    {
      "epoch": 0.5291493616366713,
      "grad_norm": 0.7703715562820435,
      "learning_rate": 8.236681536173923e-05,
      "loss": 2.2177,
      "step": 3440
    },
    {
      "epoch": 0.53068758652515,
      "grad_norm": 0.9281696081161499,
      "learning_rate": 8.231554119878994e-05,
      "loss": 2.2878,
      "step": 3450
    },
    {
      "epoch": 0.5322258114136287,
      "grad_norm": 0.9404263496398926,
      "learning_rate": 8.226426703584065e-05,
      "loss": 2.2975,
      "step": 3460
    },
    {
      "epoch": 0.5337640363021073,
      "grad_norm": 0.9033923149108887,
      "learning_rate": 8.221299287289136e-05,
      "loss": 2.2868,
      "step": 3470
    },
    {
      "epoch": 0.5353022611905861,
      "grad_norm": 1.2245683670043945,
      "learning_rate": 8.216171870994205e-05,
      "loss": 2.2436,
      "step": 3480
    },
    {
      "epoch": 0.5368404860790648,
      "grad_norm": 1.0220428705215454,
      "learning_rate": 8.211044454699278e-05,
      "loss": 2.2025,
      "step": 3490
    },
    {
      "epoch": 0.5383787109675434,
      "grad_norm": 1.0416522026062012,
      "learning_rate": 8.205917038404348e-05,
      "loss": 2.2463,
      "step": 3500
    },
    {
      "epoch": 0.5399169358560222,
      "grad_norm": 0.6994608044624329,
      "learning_rate": 8.20078962210942e-05,
      "loss": 2.2395,
      "step": 3510
    },
    {
      "epoch": 0.5414551607445008,
      "grad_norm": 0.8843719363212585,
      "learning_rate": 8.19566220581449e-05,
      "loss": 2.2328,
      "step": 3520
    },
    {
      "epoch": 0.5429933856329795,
      "grad_norm": 1.028660535812378,
      "learning_rate": 8.190534789519561e-05,
      "loss": 2.3549,
      "step": 3530
    },
    {
      "epoch": 0.5445316105214583,
      "grad_norm": 0.9257699847221375,
      "learning_rate": 8.185407373224633e-05,
      "loss": 2.2681,
      "step": 3540
    },
    {
      "epoch": 0.5460698354099369,
      "grad_norm": 0.8043977618217468,
      "learning_rate": 8.180279956929704e-05,
      "loss": 2.3699,
      "step": 3550
    },
    {
      "epoch": 0.5476080602984156,
      "grad_norm": 0.9151738286018372,
      "learning_rate": 8.175152540634774e-05,
      "loss": 2.2009,
      "step": 3560
    },
    {
      "epoch": 0.5491462851868943,
      "grad_norm": 1.049837350845337,
      "learning_rate": 8.170025124339845e-05,
      "loss": 2.288,
      "step": 3570
    },
    {
      "epoch": 0.550684510075373,
      "grad_norm": 1.450932264328003,
      "learning_rate": 8.164897708044916e-05,
      "loss": 2.272,
      "step": 3580
    },
    {
      "epoch": 0.5522227349638518,
      "grad_norm": 1.2276291847229004,
      "learning_rate": 8.159770291749988e-05,
      "loss": 2.2971,
      "step": 3590
    },
    {
      "epoch": 0.5537609598523304,
      "grad_norm": 0.9161737561225891,
      "learning_rate": 8.154642875455059e-05,
      "loss": 2.2038,
      "step": 3600
    },
    {
      "epoch": 0.5552991847408091,
      "grad_norm": 1.1927571296691895,
      "learning_rate": 8.14951545916013e-05,
      "loss": 2.2854,
      "step": 3610
    },
    {
      "epoch": 0.5568374096292878,
      "grad_norm": 0.6979355216026306,
      "learning_rate": 8.144388042865201e-05,
      "loss": 2.2999,
      "step": 3620
    },
    {
      "epoch": 0.5583756345177665,
      "grad_norm": 0.9130053520202637,
      "learning_rate": 8.139260626570272e-05,
      "loss": 2.3795,
      "step": 3630
    },
    {
      "epoch": 0.5599138594062452,
      "grad_norm": 1.0155171155929565,
      "learning_rate": 8.134133210275343e-05,
      "loss": 2.2,
      "step": 3640
    },
    {
      "epoch": 0.5614520842947239,
      "grad_norm": 0.9678864479064941,
      "learning_rate": 8.129005793980413e-05,
      "loss": 2.2825,
      "step": 3650
    },
    {
      "epoch": 0.5629903091832026,
      "grad_norm": 0.8350366353988647,
      "learning_rate": 8.123878377685484e-05,
      "loss": 2.2517,
      "step": 3660
    },
    {
      "epoch": 0.5645285340716812,
      "grad_norm": 0.8489223718643188,
      "learning_rate": 8.118750961390555e-05,
      "loss": 2.1966,
      "step": 3670
    },
    {
      "epoch": 0.56606675896016,
      "grad_norm": 1.1302565336227417,
      "learning_rate": 8.113623545095626e-05,
      "loss": 2.2821,
      "step": 3680
    },
    {
      "epoch": 0.5676049838486387,
      "grad_norm": 0.9800832271575928,
      "learning_rate": 8.108496128800697e-05,
      "loss": 2.2397,
      "step": 3690
    },
    {
      "epoch": 0.5691432087371173,
      "grad_norm": 0.9763533473014832,
      "learning_rate": 8.10336871250577e-05,
      "loss": 2.2371,
      "step": 3700
    },
    {
      "epoch": 0.5706814336255961,
      "grad_norm": 0.9713611006736755,
      "learning_rate": 8.09824129621084e-05,
      "loss": 2.2798,
      "step": 3710
    },
    {
      "epoch": 0.5722196585140747,
      "grad_norm": 1.1376091241836548,
      "learning_rate": 8.093113879915911e-05,
      "loss": 2.2782,
      "step": 3720
    },
    {
      "epoch": 0.5737578834025534,
      "grad_norm": 1.0999871492385864,
      "learning_rate": 8.087986463620981e-05,
      "loss": 2.1898,
      "step": 3730
    },
    {
      "epoch": 0.5752961082910322,
      "grad_norm": 1.0685678720474243,
      "learning_rate": 8.082859047326053e-05,
      "loss": 2.2232,
      "step": 3740
    },
    {
      "epoch": 0.5768343331795108,
      "grad_norm": 1.1431108713150024,
      "learning_rate": 8.077731631031124e-05,
      "loss": 2.3659,
      "step": 3750
    },
    {
      "epoch": 0.5783725580679896,
      "grad_norm": 1.0472077131271362,
      "learning_rate": 8.072604214736195e-05,
      "loss": 2.3059,
      "step": 3760
    },
    {
      "epoch": 0.5799107829564683,
      "grad_norm": 0.8702605962753296,
      "learning_rate": 8.067476798441266e-05,
      "loss": 2.3398,
      "step": 3770
    },
    {
      "epoch": 0.5814490078449469,
      "grad_norm": 1.093851923942566,
      "learning_rate": 8.062349382146337e-05,
      "loss": 2.1867,
      "step": 3780
    },
    {
      "epoch": 0.5829872327334257,
      "grad_norm": 0.724114179611206,
      "learning_rate": 8.057221965851408e-05,
      "loss": 2.3143,
      "step": 3790
    },
    {
      "epoch": 0.5845254576219043,
      "grad_norm": 0.9057135581970215,
      "learning_rate": 8.05209454955648e-05,
      "loss": 2.3214,
      "step": 3800
    },
    {
      "epoch": 0.586063682510383,
      "grad_norm": 0.6639223098754883,
      "learning_rate": 8.046967133261551e-05,
      "loss": 2.2851,
      "step": 3810
    },
    {
      "epoch": 0.5876019073988618,
      "grad_norm": 1.3725916147232056,
      "learning_rate": 8.04183971696662e-05,
      "loss": 2.2662,
      "step": 3820
    },
    {
      "epoch": 0.5891401322873404,
      "grad_norm": 0.8195849061012268,
      "learning_rate": 8.036712300671691e-05,
      "loss": 2.2739,
      "step": 3830
    },
    {
      "epoch": 0.5906783571758191,
      "grad_norm": 0.8284996747970581,
      "learning_rate": 8.031584884376762e-05,
      "loss": 2.2324,
      "step": 3840
    },
    {
      "epoch": 0.5922165820642978,
      "grad_norm": 0.9314028024673462,
      "learning_rate": 8.026457468081834e-05,
      "loss": 2.2935,
      "step": 3850
    },
    {
      "epoch": 0.5937548069527765,
      "grad_norm": 1.0108957290649414,
      "learning_rate": 8.021330051786905e-05,
      "loss": 2.3278,
      "step": 3860
    },
    {
      "epoch": 0.5952930318412551,
      "grad_norm": 0.7974213361740112,
      "learning_rate": 8.016202635491976e-05,
      "loss": 2.3402,
      "step": 3870
    },
    {
      "epoch": 0.5968312567297339,
      "grad_norm": 0.9947394728660583,
      "learning_rate": 8.011075219197047e-05,
      "loss": 2.2891,
      "step": 3880
    },
    {
      "epoch": 0.5983694816182126,
      "grad_norm": 0.8337937593460083,
      "learning_rate": 8.005947802902118e-05,
      "loss": 2.3236,
      "step": 3890
    },
    {
      "epoch": 0.5999077065066912,
      "grad_norm": 1.0671648979187012,
      "learning_rate": 8.000820386607189e-05,
      "loss": 2.2449,
      "step": 3900
    },
    {
      "epoch": 0.60144593139517,
      "grad_norm": 1.0902202129364014,
      "learning_rate": 7.99569297031226e-05,
      "loss": 2.1888,
      "step": 3910
    },
    {
      "epoch": 0.6029841562836487,
      "grad_norm": 1.0076961517333984,
      "learning_rate": 7.990565554017331e-05,
      "loss": 2.264,
      "step": 3920
    },
    {
      "epoch": 0.6045223811721273,
      "grad_norm": 0.7949910759925842,
      "learning_rate": 7.985438137722402e-05,
      "loss": 2.1501,
      "step": 3930
    },
    {
      "epoch": 0.6060606060606061,
      "grad_norm": 1.109812617301941,
      "learning_rate": 7.980310721427473e-05,
      "loss": 2.2365,
      "step": 3940
    },
    {
      "epoch": 0.6075988309490847,
      "grad_norm": 0.794342577457428,
      "learning_rate": 7.975183305132545e-05,
      "loss": 2.2831,
      "step": 3950
    },
    {
      "epoch": 0.6091370558375635,
      "grad_norm": 1.271654725074768,
      "learning_rate": 7.970055888837616e-05,
      "loss": 2.2758,
      "step": 3960
    },
    {
      "epoch": 0.6106752807260422,
      "grad_norm": 1.0191633701324463,
      "learning_rate": 7.964928472542687e-05,
      "loss": 2.3247,
      "step": 3970
    },
    {
      "epoch": 0.6122135056145208,
      "grad_norm": 0.8968227505683899,
      "learning_rate": 7.959801056247758e-05,
      "loss": 2.2861,
      "step": 3980
    },
    {
      "epoch": 0.6137517305029996,
      "grad_norm": 0.9603545665740967,
      "learning_rate": 7.954673639952827e-05,
      "loss": 2.2216,
      "step": 3990
    },
    {
      "epoch": 0.6152899553914782,
      "grad_norm": 0.7971410751342773,
      "learning_rate": 7.949546223657899e-05,
      "loss": 2.3794,
      "step": 4000
    },
    {
      "epoch": 0.6168281802799569,
      "grad_norm": 1.0451804399490356,
      "learning_rate": 7.94441880736297e-05,
      "loss": 2.2356,
      "step": 4010
    },
    {
      "epoch": 0.6183664051684357,
      "grad_norm": 1.0473926067352295,
      "learning_rate": 7.939291391068041e-05,
      "loss": 2.3069,
      "step": 4020
    },
    {
      "epoch": 0.6199046300569143,
      "grad_norm": 0.947273850440979,
      "learning_rate": 7.934163974773112e-05,
      "loss": 2.2615,
      "step": 4030
    },
    {
      "epoch": 0.621442854945393,
      "grad_norm": 0.874394953250885,
      "learning_rate": 7.929036558478183e-05,
      "loss": 2.3116,
      "step": 4040
    },
    {
      "epoch": 0.6229810798338717,
      "grad_norm": 1.0951088666915894,
      "learning_rate": 7.923909142183254e-05,
      "loss": 2.1982,
      "step": 4050
    },
    {
      "epoch": 0.6245193047223504,
      "grad_norm": 0.9210543036460876,
      "learning_rate": 7.918781725888326e-05,
      "loss": 2.2729,
      "step": 4060
    },
    {
      "epoch": 0.626057529610829,
      "grad_norm": 0.9173935651779175,
      "learning_rate": 7.913654309593397e-05,
      "loss": 2.3797,
      "step": 4070
    },
    {
      "epoch": 0.6275957544993078,
      "grad_norm": 1.1533770561218262,
      "learning_rate": 7.908526893298467e-05,
      "loss": 2.285,
      "step": 4080
    },
    {
      "epoch": 0.6291339793877865,
      "grad_norm": 1.2429746389389038,
      "learning_rate": 7.903399477003537e-05,
      "loss": 2.1504,
      "step": 4090
    },
    {
      "epoch": 0.6306722042762651,
      "grad_norm": 1.1552990674972534,
      "learning_rate": 7.89827206070861e-05,
      "loss": 2.3997,
      "step": 4100
    },
    {
      "epoch": 0.6322104291647439,
      "grad_norm": 0.942487895488739,
      "learning_rate": 7.89314464441368e-05,
      "loss": 2.2289,
      "step": 4110
    },
    {
      "epoch": 0.6337486540532226,
      "grad_norm": 1.0609931945800781,
      "learning_rate": 7.888017228118752e-05,
      "loss": 2.2794,
      "step": 4120
    },
    {
      "epoch": 0.6352868789417013,
      "grad_norm": 0.8204750418663025,
      "learning_rate": 7.882889811823822e-05,
      "loss": 2.0831,
      "step": 4130
    },
    {
      "epoch": 0.63682510383018,
      "grad_norm": 1.0106064081192017,
      "learning_rate": 7.877762395528893e-05,
      "loss": 2.2514,
      "step": 4140
    },
    {
      "epoch": 0.6383633287186586,
      "grad_norm": 1.204485297203064,
      "learning_rate": 7.872634979233964e-05,
      "loss": 2.2396,
      "step": 4150
    },
    {
      "epoch": 0.6399015536071374,
      "grad_norm": 0.9396007061004639,
      "learning_rate": 7.867507562939035e-05,
      "loss": 2.1642,
      "step": 4160
    },
    {
      "epoch": 0.6414397784956161,
      "grad_norm": 0.7816550731658936,
      "learning_rate": 7.862380146644106e-05,
      "loss": 2.2748,
      "step": 4170
    },
    {
      "epoch": 0.6429780033840947,
      "grad_norm": 1.0923268795013428,
      "learning_rate": 7.857252730349177e-05,
      "loss": 2.2577,
      "step": 4180
    },
    {
      "epoch": 0.6445162282725735,
      "grad_norm": 1.3657886981964111,
      "learning_rate": 7.852125314054248e-05,
      "loss": 2.2674,
      "step": 4190
    },
    {
      "epoch": 0.6460544531610521,
      "grad_norm": 1.1553798913955688,
      "learning_rate": 7.846997897759319e-05,
      "loss": 2.2941,
      "step": 4200
    },
    {
      "epoch": 0.6475926780495308,
      "grad_norm": 0.9940907955169678,
      "learning_rate": 7.841870481464391e-05,
      "loss": 2.2614,
      "step": 4210
    },
    {
      "epoch": 0.6491309029380096,
      "grad_norm": 0.7628971934318542,
      "learning_rate": 7.836743065169462e-05,
      "loss": 2.2925,
      "step": 4220
    },
    {
      "epoch": 0.6506691278264882,
      "grad_norm": 1.0765738487243652,
      "learning_rate": 7.831615648874533e-05,
      "loss": 2.3804,
      "step": 4230
    },
    {
      "epoch": 0.6522073527149669,
      "grad_norm": 0.942196249961853,
      "learning_rate": 7.826488232579604e-05,
      "loss": 2.3432,
      "step": 4240
    },
    {
      "epoch": 0.6537455776034456,
      "grad_norm": 1.014135479927063,
      "learning_rate": 7.821360816284675e-05,
      "loss": 2.2915,
      "step": 4250
    },
    {
      "epoch": 0.6552838024919243,
      "grad_norm": 0.9838641881942749,
      "learning_rate": 7.816233399989746e-05,
      "loss": 2.2821,
      "step": 4260
    },
    {
      "epoch": 0.656822027380403,
      "grad_norm": 0.9333420395851135,
      "learning_rate": 7.811105983694817e-05,
      "loss": 2.3422,
      "step": 4270
    },
    {
      "epoch": 0.6583602522688817,
      "grad_norm": 0.735694408416748,
      "learning_rate": 7.805978567399887e-05,
      "loss": 2.2558,
      "step": 4280
    },
    {
      "epoch": 0.6598984771573604,
      "grad_norm": 0.7827758193016052,
      "learning_rate": 7.800851151104958e-05,
      "loss": 2.3126,
      "step": 4290
    },
    {
      "epoch": 0.6614367020458392,
      "grad_norm": 1.021981120109558,
      "learning_rate": 7.795723734810029e-05,
      "loss": 2.2273,
      "step": 4300
    },
    {
      "epoch": 0.6629749269343178,
      "grad_norm": 0.8203487396240234,
      "learning_rate": 7.790596318515102e-05,
      "loss": 2.2541,
      "step": 4310
    },
    {
      "epoch": 0.6645131518227965,
      "grad_norm": 0.8656752109527588,
      "learning_rate": 7.785468902220172e-05,
      "loss": 2.2875,
      "step": 4320
    },
    {
      "epoch": 0.6660513767112752,
      "grad_norm": 1.2824372053146362,
      "learning_rate": 7.780341485925242e-05,
      "loss": 2.3113,
      "step": 4330
    },
    {
      "epoch": 0.6675896015997539,
      "grad_norm": 0.9605523943901062,
      "learning_rate": 7.775214069630313e-05,
      "loss": 2.2122,
      "step": 4340
    },
    {
      "epoch": 0.6691278264882325,
      "grad_norm": 1.0512597560882568,
      "learning_rate": 7.770086653335384e-05,
      "loss": 2.2528,
      "step": 4350
    },
    {
      "epoch": 0.6706660513767113,
      "grad_norm": 1.185811996459961,
      "learning_rate": 7.764959237040456e-05,
      "loss": 2.3408,
      "step": 4360
    },
    {
      "epoch": 0.67220427626519,
      "grad_norm": 0.9985898733139038,
      "learning_rate": 7.759831820745527e-05,
      "loss": 2.2154,
      "step": 4370
    },
    {
      "epoch": 0.6737425011536686,
      "grad_norm": 0.751891016960144,
      "learning_rate": 7.754704404450598e-05,
      "loss": 2.3509,
      "step": 4380
    },
    {
      "epoch": 0.6752807260421474,
      "grad_norm": 1.003220558166504,
      "learning_rate": 7.749576988155669e-05,
      "loss": 2.2126,
      "step": 4390
    },
    {
      "epoch": 0.676818950930626,
      "grad_norm": 1.0016852617263794,
      "learning_rate": 7.74444957186074e-05,
      "loss": 2.2416,
      "step": 4400
    },
    {
      "epoch": 0.6783571758191047,
      "grad_norm": 1.2870945930480957,
      "learning_rate": 7.73932215556581e-05,
      "loss": 2.2499,
      "step": 4410
    },
    {
      "epoch": 0.6798954007075835,
      "grad_norm": 1.0454813241958618,
      "learning_rate": 7.734194739270882e-05,
      "loss": 2.1733,
      "step": 4420
    },
    {
      "epoch": 0.6814336255960621,
      "grad_norm": 0.9748649597167969,
      "learning_rate": 7.729067322975952e-05,
      "loss": 2.2782,
      "step": 4430
    },
    {
      "epoch": 0.6829718504845408,
      "grad_norm": 0.9756820201873779,
      "learning_rate": 7.723939906681023e-05,
      "loss": 2.2745,
      "step": 4440
    },
    {
      "epoch": 0.6845100753730196,
      "grad_norm": 0.9698076844215393,
      "learning_rate": 7.718812490386094e-05,
      "loss": 2.2202,
      "step": 4450
    },
    {
      "epoch": 0.6860483002614982,
      "grad_norm": 0.9341872930526733,
      "learning_rate": 7.713685074091167e-05,
      "loss": 2.3069,
      "step": 4460
    },
    {
      "epoch": 0.687586525149977,
      "grad_norm": 1.050493597984314,
      "learning_rate": 7.708557657796237e-05,
      "loss": 2.25,
      "step": 4470
    },
    {
      "epoch": 0.6891247500384556,
      "grad_norm": 0.9203276634216309,
      "learning_rate": 7.703430241501308e-05,
      "loss": 2.3215,
      "step": 4480
    },
    {
      "epoch": 0.6906629749269343,
      "grad_norm": 1.1797164678573608,
      "learning_rate": 7.698302825206379e-05,
      "loss": 2.2553,
      "step": 4490
    },
    {
      "epoch": 0.6922011998154131,
      "grad_norm": 0.9404436349868774,
      "learning_rate": 7.693175408911449e-05,
      "loss": 2.2661,
      "step": 4500
    },
    {
      "epoch": 0.6937394247038917,
      "grad_norm": 0.9498058557510376,
      "learning_rate": 7.688047992616521e-05,
      "loss": 2.2899,
      "step": 4510
    },
    {
      "epoch": 0.6952776495923704,
      "grad_norm": 0.9099103212356567,
      "learning_rate": 7.682920576321592e-05,
      "loss": 2.2275,
      "step": 4520
    },
    {
      "epoch": 0.6968158744808491,
      "grad_norm": 1.0453510284423828,
      "learning_rate": 7.677793160026663e-05,
      "loss": 2.2335,
      "step": 4530
    },
    {
      "epoch": 0.6983540993693278,
      "grad_norm": 0.9092657566070557,
      "learning_rate": 7.672665743731734e-05,
      "loss": 2.2522,
      "step": 4540
    },
    {
      "epoch": 0.6998923242578065,
      "grad_norm": 0.9033746123313904,
      "learning_rate": 7.667538327436805e-05,
      "loss": 2.2731,
      "step": 4550
    },
    {
      "epoch": 0.7014305491462852,
      "grad_norm": 0.9081783890724182,
      "learning_rate": 7.662410911141876e-05,
      "loss": 2.1745,
      "step": 4560
    },
    {
      "epoch": 0.7029687740347639,
      "grad_norm": 1.2342073917388916,
      "learning_rate": 7.657283494846948e-05,
      "loss": 2.2233,
      "step": 4570
    },
    {
      "epoch": 0.7045069989232425,
      "grad_norm": 0.7935805320739746,
      "learning_rate": 7.652156078552019e-05,
      "loss": 2.2367,
      "step": 4580
    },
    {
      "epoch": 0.7060452238117213,
      "grad_norm": 0.950691282749176,
      "learning_rate": 7.647028662257088e-05,
      "loss": 2.1708,
      "step": 4590
    },
    {
      "epoch": 0.7075834487002,
      "grad_norm": 0.9970605969429016,
      "learning_rate": 7.641901245962159e-05,
      "loss": 2.2263,
      "step": 4600
    },
    {
      "epoch": 0.7091216735886786,
      "grad_norm": 1.1420639753341675,
      "learning_rate": 7.636773829667232e-05,
      "loss": 2.2582,
      "step": 4610
    },
    {
      "epoch": 0.7106598984771574,
      "grad_norm": 0.7424219846725464,
      "learning_rate": 7.631646413372302e-05,
      "loss": 2.2598,
      "step": 4620
    },
    {
      "epoch": 0.712198123365636,
      "grad_norm": 1.5658825635910034,
      "learning_rate": 7.626518997077373e-05,
      "loss": 2.142,
      "step": 4630
    },
    {
      "epoch": 0.7137363482541148,
      "grad_norm": 0.9133700132369995,
      "learning_rate": 7.621391580782444e-05,
      "loss": 2.2623,
      "step": 4640
    },
    {
      "epoch": 0.7152745731425935,
      "grad_norm": 1.068502426147461,
      "learning_rate": 7.616264164487515e-05,
      "loss": 2.267,
      "step": 4650
    },
    {
      "epoch": 0.7168127980310721,
      "grad_norm": 1.0619014501571655,
      "learning_rate": 7.611136748192586e-05,
      "loss": 2.204,
      "step": 4660
    },
    {
      "epoch": 0.7183510229195509,
      "grad_norm": 1.0259928703308105,
      "learning_rate": 7.606009331897657e-05,
      "loss": 2.2087,
      "step": 4670
    },
    {
      "epoch": 0.7198892478080295,
      "grad_norm": 0.9558602571487427,
      "learning_rate": 7.600881915602728e-05,
      "loss": 2.3066,
      "step": 4680
    },
    {
      "epoch": 0.7214274726965082,
      "grad_norm": 0.8789914846420288,
      "learning_rate": 7.595754499307799e-05,
      "loss": 2.2427,
      "step": 4690
    },
    {
      "epoch": 0.722965697584987,
      "grad_norm": 0.7384855151176453,
      "learning_rate": 7.59062708301287e-05,
      "loss": 2.3097,
      "step": 4700
    },
    {
      "epoch": 0.7245039224734656,
      "grad_norm": 1.436777114868164,
      "learning_rate": 7.58549966671794e-05,
      "loss": 2.336,
      "step": 4710
    },
    {
      "epoch": 0.7260421473619443,
      "grad_norm": 1.107456088066101,
      "learning_rate": 7.580372250423013e-05,
      "loss": 2.2619,
      "step": 4720
    },
    {
      "epoch": 0.727580372250423,
      "grad_norm": 1.0099685192108154,
      "learning_rate": 7.575244834128084e-05,
      "loss": 2.1765,
      "step": 4730
    },
    {
      "epoch": 0.7291185971389017,
      "grad_norm": 0.9844733476638794,
      "learning_rate": 7.570117417833155e-05,
      "loss": 2.3043,
      "step": 4740
    },
    {
      "epoch": 0.7306568220273804,
      "grad_norm": 0.8987894058227539,
      "learning_rate": 7.564990001538226e-05,
      "loss": 2.211,
      "step": 4750
    },
    {
      "epoch": 0.7321950469158591,
      "grad_norm": 0.9688206911087036,
      "learning_rate": 7.559862585243295e-05,
      "loss": 2.2408,
      "step": 4760
    },
    {
      "epoch": 0.7337332718043378,
      "grad_norm": 0.7387644052505493,
      "learning_rate": 7.554735168948367e-05,
      "loss": 2.1095,
      "step": 4770
    },
    {
      "epoch": 0.7352714966928164,
      "grad_norm": 0.7593515515327454,
      "learning_rate": 7.549607752653438e-05,
      "loss": 2.2496,
      "step": 4780
    },
    {
      "epoch": 0.7368097215812952,
      "grad_norm": 0.9634992480278015,
      "learning_rate": 7.544480336358509e-05,
      "loss": 2.2122,
      "step": 4790
    },
    {
      "epoch": 0.7383479464697739,
      "grad_norm": 1.1826181411743164,
      "learning_rate": 7.53935292006358e-05,
      "loss": 2.2495,
      "step": 4800
    },
    {
      "epoch": 0.7398861713582526,
      "grad_norm": 1.2440179586410522,
      "learning_rate": 7.534225503768651e-05,
      "loss": 2.3306,
      "step": 4810
    },
    {
      "epoch": 0.7414243962467313,
      "grad_norm": 1.012631893157959,
      "learning_rate": 7.529098087473723e-05,
      "loss": 2.1683,
      "step": 4820
    },
    {
      "epoch": 0.7429626211352099,
      "grad_norm": 1.084709882736206,
      "learning_rate": 7.523970671178794e-05,
      "loss": 2.1787,
      "step": 4830
    },
    {
      "epoch": 0.7445008460236887,
      "grad_norm": 1.0068249702453613,
      "learning_rate": 7.518843254883864e-05,
      "loss": 2.2355,
      "step": 4840
    },
    {
      "epoch": 0.7460390709121674,
      "grad_norm": 0.9928891062736511,
      "learning_rate": 7.513715838588935e-05,
      "loss": 2.1968,
      "step": 4850
    },
    {
      "epoch": 0.747577295800646,
      "grad_norm": 0.7048836350440979,
      "learning_rate": 7.508588422294006e-05,
      "loss": 2.3027,
      "step": 4860
    },
    {
      "epoch": 0.7491155206891248,
      "grad_norm": 0.9245229363441467,
      "learning_rate": 7.503461005999078e-05,
      "loss": 2.4051,
      "step": 4870
    },
    {
      "epoch": 0.7506537455776034,
      "grad_norm": 1.040338158607483,
      "learning_rate": 7.498333589704149e-05,
      "loss": 2.1853,
      "step": 4880
    },
    {
      "epoch": 0.7521919704660821,
      "grad_norm": 1.2731951475143433,
      "learning_rate": 7.49320617340922e-05,
      "loss": 2.284,
      "step": 4890
    },
    {
      "epoch": 0.7537301953545609,
      "grad_norm": 0.9600399732589722,
      "learning_rate": 7.48807875711429e-05,
      "loss": 2.3067,
      "step": 4900
    },
    {
      "epoch": 0.7552684202430395,
      "grad_norm": 0.884634256362915,
      "learning_rate": 7.482951340819362e-05,
      "loss": 2.2977,
      "step": 4910
    },
    {
      "epoch": 0.7568066451315182,
      "grad_norm": 0.8731624484062195,
      "learning_rate": 7.477823924524432e-05,
      "loss": 2.1175,
      "step": 4920
    },
    {
      "epoch": 0.758344870019997,
      "grad_norm": 0.7847326993942261,
      "learning_rate": 7.472696508229503e-05,
      "loss": 2.2962,
      "step": 4930
    },
    {
      "epoch": 0.7598830949084756,
      "grad_norm": 0.8660029172897339,
      "learning_rate": 7.467569091934574e-05,
      "loss": 2.2773,
      "step": 4940
    },
    {
      "epoch": 0.7614213197969543,
      "grad_norm": 0.7519541382789612,
      "learning_rate": 7.462441675639645e-05,
      "loss": 2.1479,
      "step": 4950
    },
    {
      "epoch": 0.762959544685433,
      "grad_norm": 1.08848237991333,
      "learning_rate": 7.457314259344716e-05,
      "loss": 2.2684,
      "step": 4960
    },
    {
      "epoch": 0.7644977695739117,
      "grad_norm": 1.2584805488586426,
      "learning_rate": 7.452186843049788e-05,
      "loss": 2.2282,
      "step": 4970
    },
    {
      "epoch": 0.7660359944623903,
      "grad_norm": 0.6932703256607056,
      "learning_rate": 7.447059426754859e-05,
      "loss": 2.2857,
      "step": 4980
    },
    {
      "epoch": 0.7675742193508691,
      "grad_norm": 0.8964722156524658,
      "learning_rate": 7.44193201045993e-05,
      "loss": 2.2534,
      "step": 4990
    },
    {
      "epoch": 0.7691124442393478,
      "grad_norm": 0.8847284913063049,
      "learning_rate": 7.436804594165001e-05,
      "loss": 2.2668,
      "step": 5000
    },
    {
      "epoch": 0.7706506691278265,
      "grad_norm": 0.8581270575523376,
      "learning_rate": 7.43167717787007e-05,
      "loss": 2.3018,
      "step": 5010
    },
    {
      "epoch": 0.7721888940163052,
      "grad_norm": 0.8274509906768799,
      "learning_rate": 7.426549761575143e-05,
      "loss": 2.2103,
      "step": 5020
    },
    {
      "epoch": 0.7737271189047839,
      "grad_norm": 0.846322774887085,
      "learning_rate": 7.421422345280214e-05,
      "loss": 2.2702,
      "step": 5030
    },
    {
      "epoch": 0.7752653437932626,
      "grad_norm": 1.1150062084197998,
      "learning_rate": 7.416294928985285e-05,
      "loss": 2.2521,
      "step": 5040
    },
    {
      "epoch": 0.7768035686817413,
      "grad_norm": 0.7525955438613892,
      "learning_rate": 7.411167512690356e-05,
      "loss": 2.2489,
      "step": 5050
    },
    {
      "epoch": 0.7783417935702199,
      "grad_norm": 0.8830128908157349,
      "learning_rate": 7.406040096395426e-05,
      "loss": 2.1661,
      "step": 5060
    },
    {
      "epoch": 0.7798800184586987,
      "grad_norm": 0.8543822765350342,
      "learning_rate": 7.400912680100497e-05,
      "loss": 2.2444,
      "step": 5070
    },
    {
      "epoch": 0.7814182433471774,
      "grad_norm": 0.8688601851463318,
      "learning_rate": 7.39578526380557e-05,
      "loss": 2.2858,
      "step": 5080
    },
    {
      "epoch": 0.782956468235656,
      "grad_norm": 1.069812297821045,
      "learning_rate": 7.39065784751064e-05,
      "loss": 2.1302,
      "step": 5090
    },
    {
      "epoch": 0.7844946931241348,
      "grad_norm": 0.8430496454238892,
      "learning_rate": 7.38553043121571e-05,
      "loss": 2.2757,
      "step": 5100
    },
    {
      "epoch": 0.7860329180126134,
      "grad_norm": 1.1058140993118286,
      "learning_rate": 7.380403014920781e-05,
      "loss": 2.2925,
      "step": 5110
    },
    {
      "epoch": 0.7875711429010921,
      "grad_norm": 1.010941505432129,
      "learning_rate": 7.375275598625852e-05,
      "loss": 2.1927,
      "step": 5120
    },
    {
      "epoch": 0.7891093677895709,
      "grad_norm": 0.9515459537506104,
      "learning_rate": 7.370148182330924e-05,
      "loss": 2.2652,
      "step": 5130
    },
    {
      "epoch": 0.7906475926780495,
      "grad_norm": 0.8635461926460266,
      "learning_rate": 7.365020766035995e-05,
      "loss": 2.2836,
      "step": 5140
    },
    {
      "epoch": 0.7921858175665282,
      "grad_norm": 0.8600487112998962,
      "learning_rate": 7.359893349741066e-05,
      "loss": 2.1996,
      "step": 5150
    },
    {
      "epoch": 0.7937240424550069,
      "grad_norm": 0.7502825856208801,
      "learning_rate": 7.354765933446137e-05,
      "loss": 2.2872,
      "step": 5160
    },
    {
      "epoch": 0.7952622673434856,
      "grad_norm": 0.9785138368606567,
      "learning_rate": 7.349638517151208e-05,
      "loss": 2.3074,
      "step": 5170
    },
    {
      "epoch": 0.7968004922319644,
      "grad_norm": 1.2859026193618774,
      "learning_rate": 7.344511100856279e-05,
      "loss": 2.2713,
      "step": 5180
    },
    {
      "epoch": 0.798338717120443,
      "grad_norm": 1.0041223764419556,
      "learning_rate": 7.33938368456135e-05,
      "loss": 2.1834,
      "step": 5190
    },
    {
      "epoch": 0.7998769420089217,
      "grad_norm": 1.150036096572876,
      "learning_rate": 7.33425626826642e-05,
      "loss": 2.1648,
      "step": 5200
    },
    {
      "epoch": 0.8014151668974004,
      "grad_norm": 1.1334002017974854,
      "learning_rate": 7.329128851971491e-05,
      "loss": 2.1831,
      "step": 5210
    },
    {
      "epoch": 0.8029533917858791,
      "grad_norm": 1.0058833360671997,
      "learning_rate": 7.324001435676562e-05,
      "loss": 2.3494,
      "step": 5220
    },
    {
      "epoch": 0.8044916166743578,
      "grad_norm": 0.9355576038360596,
      "learning_rate": 7.318874019381635e-05,
      "loss": 2.1909,
      "step": 5230
    },
    {
      "epoch": 0.8060298415628365,
      "grad_norm": 1.2194485664367676,
      "learning_rate": 7.313746603086706e-05,
      "loss": 2.2947,
      "step": 5240
    },
    {
      "epoch": 0.8075680664513152,
      "grad_norm": 1.13444185256958,
      "learning_rate": 7.308619186791776e-05,
      "loss": 2.2308,
      "step": 5250
    },
    {
      "epoch": 0.8091062913397938,
      "grad_norm": 1.0807734727859497,
      "learning_rate": 7.303491770496847e-05,
      "loss": 2.2756,
      "step": 5260
    },
    {
      "epoch": 0.8106445162282726,
      "grad_norm": 0.8338256478309631,
      "learning_rate": 7.298364354201917e-05,
      "loss": 2.2144,
      "step": 5270
    },
    {
      "epoch": 0.8121827411167513,
      "grad_norm": 0.8104408383369446,
      "learning_rate": 7.293236937906989e-05,
      "loss": 2.3796,
      "step": 5280
    },
    {
      "epoch": 0.8137209660052299,
      "grad_norm": 1.2390092611312866,
      "learning_rate": 7.28810952161206e-05,
      "loss": 2.2159,
      "step": 5290
    },
    {
      "epoch": 0.8152591908937087,
      "grad_norm": 0.8285899758338928,
      "learning_rate": 7.282982105317131e-05,
      "loss": 2.1799,
      "step": 5300
    },
    {
      "epoch": 0.8167974157821873,
      "grad_norm": 0.9037333726882935,
      "learning_rate": 7.277854689022202e-05,
      "loss": 2.3264,
      "step": 5310
    },
    {
      "epoch": 0.818335640670666,
      "grad_norm": 0.9882908463478088,
      "learning_rate": 7.272727272727273e-05,
      "loss": 2.2153,
      "step": 5320
    },
    {
      "epoch": 0.8198738655591448,
      "grad_norm": 0.9364275932312012,
      "learning_rate": 7.267599856432345e-05,
      "loss": 2.2294,
      "step": 5330
    },
    {
      "epoch": 0.8214120904476234,
      "grad_norm": 0.7310437560081482,
      "learning_rate": 7.262472440137416e-05,
      "loss": 2.326,
      "step": 5340
    },
    {
      "epoch": 0.8229503153361022,
      "grad_norm": 0.8150128126144409,
      "learning_rate": 7.257345023842486e-05,
      "loss": 2.1678,
      "step": 5350
    },
    {
      "epoch": 0.8244885402245808,
      "grad_norm": 1.2194074392318726,
      "learning_rate": 7.252217607547556e-05,
      "loss": 2.2033,
      "step": 5360
    },
    {
      "epoch": 0.8260267651130595,
      "grad_norm": 0.8046772480010986,
      "learning_rate": 7.247090191252627e-05,
      "loss": 2.2217,
      "step": 5370
    },
    {
      "epoch": 0.8275649900015383,
      "grad_norm": 0.903985321521759,
      "learning_rate": 7.2419627749577e-05,
      "loss": 2.2619,
      "step": 5380
    },
    {
      "epoch": 0.8291032148900169,
      "grad_norm": 0.7844682931900024,
      "learning_rate": 7.23683535866277e-05,
      "loss": 2.2619,
      "step": 5390
    },
    {
      "epoch": 0.8306414397784956,
      "grad_norm": 0.7035439610481262,
      "learning_rate": 7.231707942367841e-05,
      "loss": 2.263,
      "step": 5400
    },
    {
      "epoch": 0.8321796646669744,
      "grad_norm": 1.0288983583450317,
      "learning_rate": 7.226580526072912e-05,
      "loss": 2.3107,
      "step": 5410
    },
    {
      "epoch": 0.833717889555453,
      "grad_norm": 0.8949504494667053,
      "learning_rate": 7.221453109777983e-05,
      "loss": 2.2298,
      "step": 5420
    },
    {
      "epoch": 0.8352561144439317,
      "grad_norm": 1.0483810901641846,
      "learning_rate": 7.216325693483054e-05,
      "loss": 2.2856,
      "step": 5430
    },
    {
      "epoch": 0.8367943393324104,
      "grad_norm": 0.9200074672698975,
      "learning_rate": 7.211198277188125e-05,
      "loss": 2.3732,
      "step": 5440
    },
    {
      "epoch": 0.8383325642208891,
      "grad_norm": 0.7778462171554565,
      "learning_rate": 7.206070860893196e-05,
      "loss": 2.2131,
      "step": 5450
    },
    {
      "epoch": 0.8398707891093677,
      "grad_norm": 0.9634210467338562,
      "learning_rate": 7.200943444598267e-05,
      "loss": 2.227,
      "step": 5460
    },
    {
      "epoch": 0.8414090139978465,
      "grad_norm": 0.960573136806488,
      "learning_rate": 7.195816028303338e-05,
      "loss": 2.2048,
      "step": 5470
    },
    {
      "epoch": 0.8429472388863252,
      "grad_norm": 0.995637834072113,
      "learning_rate": 7.190688612008409e-05,
      "loss": 2.2041,
      "step": 5480
    },
    {
      "epoch": 0.8444854637748038,
      "grad_norm": 0.9745773673057556,
      "learning_rate": 7.185561195713481e-05,
      "loss": 2.2385,
      "step": 5490
    },
    {
      "epoch": 0.8460236886632826,
      "grad_norm": 1.005265712738037,
      "learning_rate": 7.180433779418552e-05,
      "loss": 2.2298,
      "step": 5500
    },
    {
      "epoch": 0.8475619135517612,
      "grad_norm": 1.0382027626037598,
      "learning_rate": 7.175306363123623e-05,
      "loss": 2.3451,
      "step": 5510
    },
    {
      "epoch": 0.84910013844024,
      "grad_norm": 1.2968811988830566,
      "learning_rate": 7.170178946828692e-05,
      "loss": 2.1005,
      "step": 5520
    },
    {
      "epoch": 0.8506383633287187,
      "grad_norm": 0.9986342787742615,
      "learning_rate": 7.165051530533765e-05,
      "loss": 2.2197,
      "step": 5530
    },
    {
      "epoch": 0.8521765882171973,
      "grad_norm": 0.8931381106376648,
      "learning_rate": 7.159924114238836e-05,
      "loss": 2.4121,
      "step": 5540
    },
    {
      "epoch": 0.8537148131056761,
      "grad_norm": 0.8220108151435852,
      "learning_rate": 7.154796697943906e-05,
      "loss": 2.2852,
      "step": 5550
    },
    {
      "epoch": 0.8552530379941548,
      "grad_norm": 0.7097431421279907,
      "learning_rate": 7.149669281648977e-05,
      "loss": 2.2439,
      "step": 5560
    },
    {
      "epoch": 0.8567912628826334,
      "grad_norm": 0.8796679973602295,
      "learning_rate": 7.144541865354048e-05,
      "loss": 2.2442,
      "step": 5570
    },
    {
      "epoch": 0.8583294877711122,
      "grad_norm": 1.0420113801956177,
      "learning_rate": 7.139414449059119e-05,
      "loss": 2.2996,
      "step": 5580
    },
    {
      "epoch": 0.8598677126595908,
      "grad_norm": 0.9017590284347534,
      "learning_rate": 7.134287032764191e-05,
      "loss": 2.3176,
      "step": 5590
    },
    {
      "epoch": 0.8614059375480695,
      "grad_norm": 1.0023633241653442,
      "learning_rate": 7.129159616469262e-05,
      "loss": 2.2086,
      "step": 5600
    },
    {
      "epoch": 0.8629441624365483,
      "grad_norm": 1.1581523418426514,
      "learning_rate": 7.124032200174332e-05,
      "loss": 2.2061,
      "step": 5610
    },
    {
      "epoch": 0.8644823873250269,
      "grad_norm": 0.7982566356658936,
      "learning_rate": 7.118904783879403e-05,
      "loss": 2.2175,
      "step": 5620
    },
    {
      "epoch": 0.8660206122135056,
      "grad_norm": 0.8286187648773193,
      "learning_rate": 7.113777367584474e-05,
      "loss": 2.1998,
      "step": 5630
    },
    {
      "epoch": 0.8675588371019843,
      "grad_norm": 1.0498437881469727,
      "learning_rate": 7.108649951289546e-05,
      "loss": 2.2441,
      "step": 5640
    },
    {
      "epoch": 0.869097061990463,
      "grad_norm": 0.9181168079376221,
      "learning_rate": 7.103522534994617e-05,
      "loss": 2.1805,
      "step": 5650
    },
    {
      "epoch": 0.8706352868789416,
      "grad_norm": 0.9713919758796692,
      "learning_rate": 7.098395118699688e-05,
      "loss": 2.3229,
      "step": 5660
    },
    {
      "epoch": 0.8721735117674204,
      "grad_norm": 1.0265237092971802,
      "learning_rate": 7.093267702404759e-05,
      "loss": 2.2746,
      "step": 5670
    },
    {
      "epoch": 0.8737117366558991,
      "grad_norm": 0.8737251162528992,
      "learning_rate": 7.08814028610983e-05,
      "loss": 2.203,
      "step": 5680
    },
    {
      "epoch": 0.8752499615443778,
      "grad_norm": 0.8156078457832336,
      "learning_rate": 7.0830128698149e-05,
      "loss": 2.3261,
      "step": 5690
    },
    {
      "epoch": 0.8767881864328565,
      "grad_norm": 0.7743782997131348,
      "learning_rate": 7.077885453519971e-05,
      "loss": 2.2545,
      "step": 5700
    },
    {
      "epoch": 0.8783264113213352,
      "grad_norm": 1.2242268323898315,
      "learning_rate": 7.072758037225042e-05,
      "loss": 2.2157,
      "step": 5710
    },
    {
      "epoch": 0.8798646362098139,
      "grad_norm": 1.1011817455291748,
      "learning_rate": 7.067630620930113e-05,
      "loss": 2.2289,
      "step": 5720
    },
    {
      "epoch": 0.8814028610982926,
      "grad_norm": 0.9632259011268616,
      "learning_rate": 7.062503204635184e-05,
      "loss": 2.1752,
      "step": 5730
    },
    {
      "epoch": 0.8829410859867712,
      "grad_norm": 1.1523116827011108,
      "learning_rate": 7.057375788340256e-05,
      "loss": 2.2177,
      "step": 5740
    },
    {
      "epoch": 0.88447931087525,
      "grad_norm": 1.3633785247802734,
      "learning_rate": 7.052248372045327e-05,
      "loss": 2.2051,
      "step": 5750
    },
    {
      "epoch": 0.8860175357637287,
      "grad_norm": 1.2254995107650757,
      "learning_rate": 7.047120955750398e-05,
      "loss": 2.3028,
      "step": 5760
    },
    {
      "epoch": 0.8875557606522073,
      "grad_norm": 0.8773002028465271,
      "learning_rate": 7.041993539455469e-05,
      "loss": 2.1646,
      "step": 5770
    },
    {
      "epoch": 0.8890939855406861,
      "grad_norm": 0.9160082936286926,
      "learning_rate": 7.036866123160539e-05,
      "loss": 2.152,
      "step": 5780
    },
    {
      "epoch": 0.8906322104291647,
      "grad_norm": 0.9669731259346008,
      "learning_rate": 7.031738706865611e-05,
      "loss": 2.2959,
      "step": 5790
    },
    {
      "epoch": 0.8921704353176434,
      "grad_norm": 1.0729905366897583,
      "learning_rate": 7.026611290570682e-05,
      "loss": 2.2025,
      "step": 5800
    },
    {
      "epoch": 0.8937086602061222,
      "grad_norm": 0.9462867379188538,
      "learning_rate": 7.021483874275753e-05,
      "loss": 2.2259,
      "step": 5810
    },
    {
      "epoch": 0.8952468850946008,
      "grad_norm": 1.0438424348831177,
      "learning_rate": 7.016356457980824e-05,
      "loss": 2.2253,
      "step": 5820
    },
    {
      "epoch": 0.8967851099830795,
      "grad_norm": 0.9640107750892639,
      "learning_rate": 7.011229041685895e-05,
      "loss": 2.2596,
      "step": 5830
    },
    {
      "epoch": 0.8983233348715582,
      "grad_norm": 1.0038121938705444,
      "learning_rate": 7.006101625390966e-05,
      "loss": 2.1998,
      "step": 5840
    },
    {
      "epoch": 0.8998615597600369,
      "grad_norm": 0.9922749996185303,
      "learning_rate": 7.000974209096038e-05,
      "loss": 2.2346,
      "step": 5850
    },
    {
      "epoch": 0.9013997846485157,
      "grad_norm": 1.0703380107879639,
      "learning_rate": 6.995846792801107e-05,
      "loss": 2.2383,
      "step": 5860
    },
    {
      "epoch": 0.9029380095369943,
      "grad_norm": 0.9439657330513,
      "learning_rate": 6.990719376506178e-05,
      "loss": 2.2418,
      "step": 5870
    },
    {
      "epoch": 0.904476234425473,
      "grad_norm": 1.311553716659546,
      "learning_rate": 6.985591960211249e-05,
      "loss": 2.2657,
      "step": 5880
    },
    {
      "epoch": 0.9060144593139517,
      "grad_norm": 0.8907186985015869,
      "learning_rate": 6.980464543916321e-05,
      "loss": 2.1981,
      "step": 5890
    },
    {
      "epoch": 0.9075526842024304,
      "grad_norm": 0.9506505131721497,
      "learning_rate": 6.975337127621392e-05,
      "loss": 2.1985,
      "step": 5900
    },
    {
      "epoch": 0.9090909090909091,
      "grad_norm": 0.879865288734436,
      "learning_rate": 6.970209711326463e-05,
      "loss": 2.2871,
      "step": 5910
    },
    {
      "epoch": 0.9106291339793878,
      "grad_norm": 0.9533790349960327,
      "learning_rate": 6.965082295031534e-05,
      "loss": 2.2332,
      "step": 5920
    },
    {
      "epoch": 0.9121673588678665,
      "grad_norm": 0.7192929983139038,
      "learning_rate": 6.959954878736605e-05,
      "loss": 2.2935,
      "step": 5930
    },
    {
      "epoch": 0.9137055837563451,
      "grad_norm": 0.849898636341095,
      "learning_rate": 6.954827462441676e-05,
      "loss": 2.2881,
      "step": 5940
    },
    {
      "epoch": 0.9152438086448239,
      "grad_norm": 1.002225637435913,
      "learning_rate": 6.949700046146747e-05,
      "loss": 2.1494,
      "step": 5950
    },
    {
      "epoch": 0.9167820335333026,
      "grad_norm": 1.1923339366912842,
      "learning_rate": 6.944572629851818e-05,
      "loss": 2.2591,
      "step": 5960
    },
    {
      "epoch": 0.9183202584217812,
      "grad_norm": 0.962456464767456,
      "learning_rate": 6.939445213556889e-05,
      "loss": 2.2177,
      "step": 5970
    },
    {
      "epoch": 0.91985848331026,
      "grad_norm": 1.1654248237609863,
      "learning_rate": 6.93431779726196e-05,
      "loss": 2.2075,
      "step": 5980
    },
    {
      "epoch": 0.9213967081987386,
      "grad_norm": 0.8172051906585693,
      "learning_rate": 6.92919038096703e-05,
      "loss": 2.1862,
      "step": 5990
    },
    {
      "epoch": 0.9229349330872173,
      "grad_norm": 0.9794129729270935,
      "learning_rate": 6.924062964672103e-05,
      "loss": 2.4075,
      "step": 6000
    },
    {
      "epoch": 0.9244731579756961,
      "grad_norm": 0.9437201619148254,
      "learning_rate": 6.918935548377174e-05,
      "loss": 2.1113,
      "step": 6010
    },
    {
      "epoch": 0.9260113828641747,
      "grad_norm": 1.2673958539962769,
      "learning_rate": 6.913808132082245e-05,
      "loss": 2.2537,
      "step": 6020
    },
    {
      "epoch": 0.9275496077526535,
      "grad_norm": 1.0145901441574097,
      "learning_rate": 6.908680715787314e-05,
      "loss": 2.2981,
      "step": 6030
    },
    {
      "epoch": 0.9290878326411321,
      "grad_norm": 1.0880573987960815,
      "learning_rate": 6.903553299492386e-05,
      "loss": 2.1715,
      "step": 6040
    },
    {
      "epoch": 0.9306260575296108,
      "grad_norm": 0.8708831667900085,
      "learning_rate": 6.898425883197457e-05,
      "loss": 2.245,
      "step": 6050
    },
    {
      "epoch": 0.9321642824180896,
      "grad_norm": 1.0415915250778198,
      "learning_rate": 6.893298466902528e-05,
      "loss": 2.1745,
      "step": 6060
    },
    {
      "epoch": 0.9337025073065682,
      "grad_norm": 0.8422058820724487,
      "learning_rate": 6.888171050607599e-05,
      "loss": 2.2571,
      "step": 6070
    },
    {
      "epoch": 0.9352407321950469,
      "grad_norm": 1.6928600072860718,
      "learning_rate": 6.88304363431267e-05,
      "loss": 2.3196,
      "step": 6080
    },
    {
      "epoch": 0.9367789570835257,
      "grad_norm": 0.9402989745140076,
      "learning_rate": 6.877916218017741e-05,
      "loss": 2.2494,
      "step": 6090
    },
    {
      "epoch": 0.9383171819720043,
      "grad_norm": 0.7800553441047668,
      "learning_rate": 6.872788801722813e-05,
      "loss": 2.183,
      "step": 6100
    },
    {
      "epoch": 0.939855406860483,
      "grad_norm": 1.1068652868270874,
      "learning_rate": 6.867661385427884e-05,
      "loss": 2.1707,
      "step": 6110
    },
    {
      "epoch": 0.9413936317489617,
      "grad_norm": 1.0167526006698608,
      "learning_rate": 6.862533969132954e-05,
      "loss": 2.1936,
      "step": 6120
    },
    {
      "epoch": 0.9429318566374404,
      "grad_norm": 0.8121317028999329,
      "learning_rate": 6.857406552838025e-05,
      "loss": 2.1757,
      "step": 6130
    },
    {
      "epoch": 0.944470081525919,
      "grad_norm": 0.8832755088806152,
      "learning_rate": 6.852279136543095e-05,
      "loss": 2.1901,
      "step": 6140
    },
    {
      "epoch": 0.9460083064143978,
      "grad_norm": 0.8259606957435608,
      "learning_rate": 6.847151720248168e-05,
      "loss": 2.3199,
      "step": 6150
    },
    {
      "epoch": 0.9475465313028765,
      "grad_norm": 0.7941521406173706,
      "learning_rate": 6.842024303953239e-05,
      "loss": 2.2773,
      "step": 6160
    },
    {
      "epoch": 0.9490847561913551,
      "grad_norm": 1.1399204730987549,
      "learning_rate": 6.83689688765831e-05,
      "loss": 2.1176,
      "step": 6170
    },
    {
      "epoch": 0.9506229810798339,
      "grad_norm": 0.875884473323822,
      "learning_rate": 6.83176947136338e-05,
      "loss": 2.255,
      "step": 6180
    },
    {
      "epoch": 0.9521612059683126,
      "grad_norm": 0.8200560212135315,
      "learning_rate": 6.826642055068451e-05,
      "loss": 2.2824,
      "step": 6190
    },
    {
      "epoch": 0.9536994308567912,
      "grad_norm": 0.8320664167404175,
      "learning_rate": 6.821514638773522e-05,
      "loss": 2.2587,
      "step": 6200
    },
    {
      "epoch": 0.95523765574527,
      "grad_norm": 0.8696109652519226,
      "learning_rate": 6.816387222478593e-05,
      "loss": 2.2175,
      "step": 6210
    },
    {
      "epoch": 0.9567758806337486,
      "grad_norm": 1.5644712448120117,
      "learning_rate": 6.811259806183664e-05,
      "loss": 2.1544,
      "step": 6220
    },
    {
      "epoch": 0.9583141055222274,
      "grad_norm": 0.8527478575706482,
      "learning_rate": 6.806132389888735e-05,
      "loss": 2.2551,
      "step": 6230
    },
    {
      "epoch": 0.9598523304107061,
      "grad_norm": 0.7965354323387146,
      "learning_rate": 6.801004973593806e-05,
      "loss": 2.1712,
      "step": 6240
    },
    {
      "epoch": 0.9613905552991847,
      "grad_norm": 0.9297742247581482,
      "learning_rate": 6.795877557298878e-05,
      "loss": 2.2253,
      "step": 6250
    },
    {
      "epoch": 0.9629287801876635,
      "grad_norm": 1.0412280559539795,
      "learning_rate": 6.790750141003949e-05,
      "loss": 2.2303,
      "step": 6260
    },
    {
      "epoch": 0.9644670050761421,
      "grad_norm": 0.7719501852989197,
      "learning_rate": 6.78562272470902e-05,
      "loss": 2.1924,
      "step": 6270
    },
    {
      "epoch": 0.9660052299646208,
      "grad_norm": 1.0678701400756836,
      "learning_rate": 6.780495308414091e-05,
      "loss": 2.3489,
      "step": 6280
    },
    {
      "epoch": 0.9675434548530996,
      "grad_norm": 0.9352123737335205,
      "learning_rate": 6.77536789211916e-05,
      "loss": 2.349,
      "step": 6290
    },
    {
      "epoch": 0.9690816797415782,
      "grad_norm": 1.0096973180770874,
      "learning_rate": 6.770240475824233e-05,
      "loss": 2.1835,
      "step": 6300
    },
    {
      "epoch": 0.9706199046300569,
      "grad_norm": 1.1165434122085571,
      "learning_rate": 6.765113059529304e-05,
      "loss": 2.3467,
      "step": 6310
    },
    {
      "epoch": 0.9721581295185356,
      "grad_norm": 1.1397455930709839,
      "learning_rate": 6.759985643234375e-05,
      "loss": 2.2068,
      "step": 6320
    },
    {
      "epoch": 0.9736963544070143,
      "grad_norm": 0.9018900394439697,
      "learning_rate": 6.754858226939445e-05,
      "loss": 2.2697,
      "step": 6330
    },
    {
      "epoch": 0.975234579295493,
      "grad_norm": 1.1716601848602295,
      "learning_rate": 6.749730810644516e-05,
      "loss": 2.2993,
      "step": 6340
    },
    {
      "epoch": 0.9767728041839717,
      "grad_norm": 0.9022520184516907,
      "learning_rate": 6.744603394349587e-05,
      "loss": 2.2182,
      "step": 6350
    },
    {
      "epoch": 0.9783110290724504,
      "grad_norm": 0.7480305433273315,
      "learning_rate": 6.73947597805466e-05,
      "loss": 2.2436,
      "step": 6360
    },
    {
      "epoch": 0.979849253960929,
      "grad_norm": 1.1200494766235352,
      "learning_rate": 6.734348561759729e-05,
      "loss": 2.2891,
      "step": 6370
    },
    {
      "epoch": 0.9813874788494078,
      "grad_norm": 1.060767412185669,
      "learning_rate": 6.7292211454648e-05,
      "loss": 2.2058,
      "step": 6380
    },
    {
      "epoch": 0.9829257037378865,
      "grad_norm": 0.8123694658279419,
      "learning_rate": 6.724093729169871e-05,
      "loss": 2.2358,
      "step": 6390
    },
    {
      "epoch": 0.9844639286263652,
      "grad_norm": 1.036231517791748,
      "learning_rate": 6.718966312874942e-05,
      "loss": 2.1417,
      "step": 6400
    },
    {
      "epoch": 0.9860021535148439,
      "grad_norm": 0.7160494327545166,
      "learning_rate": 6.713838896580014e-05,
      "loss": 2.316,
      "step": 6410
    },
    {
      "epoch": 0.9875403784033225,
      "grad_norm": 0.7874507904052734,
      "learning_rate": 6.708711480285085e-05,
      "loss": 2.2598,
      "step": 6420
    },
    {
      "epoch": 0.9890786032918013,
      "grad_norm": 0.9962749481201172,
      "learning_rate": 6.703584063990156e-05,
      "loss": 2.2326,
      "step": 6430
    },
    {
      "epoch": 0.99061682818028,
      "grad_norm": 0.9826802015304565,
      "learning_rate": 6.698456647695227e-05,
      "loss": 2.1553,
      "step": 6440
    },
    {
      "epoch": 0.9921550530687586,
      "grad_norm": 1.4349600076675415,
      "learning_rate": 6.693329231400298e-05,
      "loss": 2.2887,
      "step": 6450
    },
    {
      "epoch": 0.9936932779572374,
      "grad_norm": 0.6971400380134583,
      "learning_rate": 6.688201815105369e-05,
      "loss": 2.2271,
      "step": 6460
    },
    {
      "epoch": 0.995231502845716,
      "grad_norm": 0.8550315499305725,
      "learning_rate": 6.68307439881044e-05,
      "loss": 2.2542,
      "step": 6470
    },
    {
      "epoch": 0.9967697277341947,
      "grad_norm": 1.0968846082687378,
      "learning_rate": 6.67794698251551e-05,
      "loss": 2.214,
      "step": 6480
    },
    {
      "epoch": 0.9983079526226735,
      "grad_norm": 0.9878483414649963,
      "learning_rate": 6.672819566220581e-05,
      "loss": 2.3072,
      "step": 6490
    },
    {
      "epoch": 0.9998461775111521,
      "grad_norm": 0.6207689046859741,
      "learning_rate": 6.667692149925652e-05,
      "loss": 2.2189,
      "step": 6500
    },
    {
      "epoch": 1.0013844023996308,
      "grad_norm": 0.9211934804916382,
      "learning_rate": 6.662564733630725e-05,
      "loss": 2.2556,
      "step": 6510
    },
    {
      "epoch": 1.0029226272881095,
      "grad_norm": 1.1612088680267334,
      "learning_rate": 6.657437317335795e-05,
      "loss": 2.2753,
      "step": 6520
    },
    {
      "epoch": 1.0044608521765883,
      "grad_norm": 1.0176222324371338,
      "learning_rate": 6.652309901040866e-05,
      "loss": 2.1851,
      "step": 6530
    },
    {
      "epoch": 1.0059990770650669,
      "grad_norm": 0.7581601142883301,
      "learning_rate": 6.647182484745936e-05,
      "loss": 2.1792,
      "step": 6540
    },
    {
      "epoch": 1.0075373019535456,
      "grad_norm": 0.9233026504516602,
      "learning_rate": 6.642055068451007e-05,
      "loss": 2.3024,
      "step": 6550
    },
    {
      "epoch": 1.0090755268420244,
      "grad_norm": 0.8369132280349731,
      "learning_rate": 6.636927652156079e-05,
      "loss": 2.2405,
      "step": 6560
    },
    {
      "epoch": 1.010613751730503,
      "grad_norm": 1.482983112335205,
      "learning_rate": 6.63180023586115e-05,
      "loss": 2.1867,
      "step": 6570
    },
    {
      "epoch": 1.0121519766189817,
      "grad_norm": 0.6327037811279297,
      "learning_rate": 6.626672819566221e-05,
      "loss": 2.2474,
      "step": 6580
    },
    {
      "epoch": 1.0136902015074605,
      "grad_norm": 1.286847472190857,
      "learning_rate": 6.621545403271292e-05,
      "loss": 2.3656,
      "step": 6590
    },
    {
      "epoch": 1.015228426395939,
      "grad_norm": 0.8366317749023438,
      "learning_rate": 6.616417986976363e-05,
      "loss": 2.2242,
      "step": 6600
    },
    {
      "epoch": 1.0167666512844178,
      "grad_norm": 0.8523824214935303,
      "learning_rate": 6.611290570681435e-05,
      "loss": 2.2457,
      "step": 6610
    },
    {
      "epoch": 1.0183048761728966,
      "grad_norm": 0.855607807636261,
      "learning_rate": 6.606163154386506e-05,
      "loss": 2.2212,
      "step": 6620
    },
    {
      "epoch": 1.019843101061375,
      "grad_norm": 0.8414830565452576,
      "learning_rate": 6.601035738091575e-05,
      "loss": 2.2842,
      "step": 6630
    },
    {
      "epoch": 1.0213813259498539,
      "grad_norm": 1.0716480016708374,
      "learning_rate": 6.595908321796646e-05,
      "loss": 2.215,
      "step": 6640
    },
    {
      "epoch": 1.0229195508383326,
      "grad_norm": 1.0109039545059204,
      "learning_rate": 6.590780905501717e-05,
      "loss": 2.2154,
      "step": 6650
    },
    {
      "epoch": 1.0244577757268112,
      "grad_norm": 0.992416501045227,
      "learning_rate": 6.58565348920679e-05,
      "loss": 2.2485,
      "step": 6660
    },
    {
      "epoch": 1.02599600061529,
      "grad_norm": 0.9775389432907104,
      "learning_rate": 6.58052607291186e-05,
      "loss": 2.2065,
      "step": 6670
    },
    {
      "epoch": 1.0275342255037687,
      "grad_norm": 0.9921461343765259,
      "learning_rate": 6.575398656616931e-05,
      "loss": 2.1439,
      "step": 6680
    },
    {
      "epoch": 1.0290724503922473,
      "grad_norm": 0.9197810292243958,
      "learning_rate": 6.570271240322002e-05,
      "loss": 2.2738,
      "step": 6690
    },
    {
      "epoch": 1.030610675280726,
      "grad_norm": 1.1226884126663208,
      "learning_rate": 6.565143824027073e-05,
      "loss": 2.2791,
      "step": 6700
    },
    {
      "epoch": 1.0321489001692048,
      "grad_norm": 0.8367444276809692,
      "learning_rate": 6.560016407732144e-05,
      "loss": 2.2339,
      "step": 6710
    },
    {
      "epoch": 1.0336871250576833,
      "grad_norm": 0.791100025177002,
      "learning_rate": 6.554888991437215e-05,
      "loss": 2.2452,
      "step": 6720
    },
    {
      "epoch": 1.035225349946162,
      "grad_norm": 0.9463369250297546,
      "learning_rate": 6.549761575142286e-05,
      "loss": 2.2022,
      "step": 6730
    },
    {
      "epoch": 1.0367635748346409,
      "grad_norm": 1.2018417119979858,
      "learning_rate": 6.544634158847357e-05,
      "loss": 2.198,
      "step": 6740
    },
    {
      "epoch": 1.0383017997231194,
      "grad_norm": 0.800146758556366,
      "learning_rate": 6.539506742552428e-05,
      "loss": 2.1537,
      "step": 6750
    },
    {
      "epoch": 1.0398400246115982,
      "grad_norm": 0.7625159621238708,
      "learning_rate": 6.534379326257499e-05,
      "loss": 2.2295,
      "step": 6760
    },
    {
      "epoch": 1.041378249500077,
      "grad_norm": 0.8289132118225098,
      "learning_rate": 6.529251909962571e-05,
      "loss": 2.2162,
      "step": 6770
    },
    {
      "epoch": 1.0429164743885555,
      "grad_norm": 1.13119375705719,
      "learning_rate": 6.524124493667642e-05,
      "loss": 2.2063,
      "step": 6780
    },
    {
      "epoch": 1.0444546992770343,
      "grad_norm": 0.825863778591156,
      "learning_rate": 6.518997077372713e-05,
      "loss": 2.2551,
      "step": 6790
    },
    {
      "epoch": 1.045992924165513,
      "grad_norm": 0.8475519418716431,
      "learning_rate": 6.513869661077782e-05,
      "loss": 2.1569,
      "step": 6800
    },
    {
      "epoch": 1.0475311490539916,
      "grad_norm": 1.0222691297531128,
      "learning_rate": 6.508742244782855e-05,
      "loss": 2.2488,
      "step": 6810
    },
    {
      "epoch": 1.0490693739424704,
      "grad_norm": 0.7359206080436707,
      "learning_rate": 6.503614828487925e-05,
      "loss": 2.1968,
      "step": 6820
    },
    {
      "epoch": 1.0506075988309491,
      "grad_norm": 1.0759114027023315,
      "learning_rate": 6.498487412192996e-05,
      "loss": 2.1698,
      "step": 6830
    },
    {
      "epoch": 1.0521458237194279,
      "grad_norm": 1.180518388748169,
      "learning_rate": 6.493359995898067e-05,
      "loss": 2.2629,
      "step": 6840
    },
    {
      "epoch": 1.0536840486079064,
      "grad_norm": 0.7571735978126526,
      "learning_rate": 6.488232579603138e-05,
      "loss": 2.2378,
      "step": 6850
    },
    {
      "epoch": 1.0552222734963852,
      "grad_norm": 1.1258208751678467,
      "learning_rate": 6.483105163308209e-05,
      "loss": 2.3221,
      "step": 6860
    },
    {
      "epoch": 1.056760498384864,
      "grad_norm": 1.0712711811065674,
      "learning_rate": 6.477977747013281e-05,
      "loss": 2.3231,
      "step": 6870
    },
    {
      "epoch": 1.0582987232733425,
      "grad_norm": 1.0942370891571045,
      "learning_rate": 6.472850330718351e-05,
      "loss": 2.2085,
      "step": 6880
    },
    {
      "epoch": 1.0598369481618213,
      "grad_norm": 0.8343178033828735,
      "learning_rate": 6.467722914423422e-05,
      "loss": 2.2318,
      "step": 6890
    },
    {
      "epoch": 1.0613751730503,
      "grad_norm": 0.895224928855896,
      "learning_rate": 6.462595498128493e-05,
      "loss": 2.1916,
      "step": 6900
    },
    {
      "epoch": 1.0629133979387786,
      "grad_norm": 0.897879958152771,
      "learning_rate": 6.457468081833564e-05,
      "loss": 2.2584,
      "step": 6910
    },
    {
      "epoch": 1.0644516228272574,
      "grad_norm": 0.7855554223060608,
      "learning_rate": 6.452340665538636e-05,
      "loss": 2.1851,
      "step": 6920
    },
    {
      "epoch": 1.0659898477157361,
      "grad_norm": 1.049607753753662,
      "learning_rate": 6.447213249243707e-05,
      "loss": 2.1864,
      "step": 6930
    },
    {
      "epoch": 1.0675280726042147,
      "grad_norm": 0.9277946949005127,
      "learning_rate": 6.442085832948778e-05,
      "loss": 2.2771,
      "step": 6940
    },
    {
      "epoch": 1.0690662974926934,
      "grad_norm": 0.9742904305458069,
      "learning_rate": 6.436958416653849e-05,
      "loss": 2.1862,
      "step": 6950
    },
    {
      "epoch": 1.0706045223811722,
      "grad_norm": 0.873525857925415,
      "learning_rate": 6.43183100035892e-05,
      "loss": 2.2123,
      "step": 6960
    },
    {
      "epoch": 1.0721427472696508,
      "grad_norm": 1.4055308103561401,
      "learning_rate": 6.42670358406399e-05,
      "loss": 2.2605,
      "step": 6970
    },
    {
      "epoch": 1.0736809721581295,
      "grad_norm": 0.9741908311843872,
      "learning_rate": 6.421576167769061e-05,
      "loss": 2.2541,
      "step": 6980
    },
    {
      "epoch": 1.0752191970466083,
      "grad_norm": 1.2149406671524048,
      "learning_rate": 6.416448751474132e-05,
      "loss": 2.2475,
      "step": 6990
    },
    {
      "epoch": 1.0767574219350868,
      "grad_norm": 0.9169546961784363,
      "learning_rate": 6.411321335179203e-05,
      "loss": 2.2324,
      "step": 7000
    },
    {
      "epoch": 1.0782956468235656,
      "grad_norm": 1.0174561738967896,
      "learning_rate": 6.406193918884274e-05,
      "loss": 2.22,
      "step": 7010
    },
    {
      "epoch": 1.0798338717120444,
      "grad_norm": 0.935809850692749,
      "learning_rate": 6.401066502589346e-05,
      "loss": 2.1444,
      "step": 7020
    },
    {
      "epoch": 1.081372096600523,
      "grad_norm": 0.8874290585517883,
      "learning_rate": 6.395939086294417e-05,
      "loss": 2.1912,
      "step": 7030
    },
    {
      "epoch": 1.0829103214890017,
      "grad_norm": 0.9231744408607483,
      "learning_rate": 6.390811669999488e-05,
      "loss": 2.1771,
      "step": 7040
    },
    {
      "epoch": 1.0844485463774804,
      "grad_norm": 0.9294351935386658,
      "learning_rate": 6.385684253704558e-05,
      "loss": 2.1659,
      "step": 7050
    },
    {
      "epoch": 1.085986771265959,
      "grad_norm": 0.9424762725830078,
      "learning_rate": 6.380556837409629e-05,
      "loss": 2.2448,
      "step": 7060
    },
    {
      "epoch": 1.0875249961544378,
      "grad_norm": 0.9196208119392395,
      "learning_rate": 6.375429421114701e-05,
      "loss": 2.2541,
      "step": 7070
    },
    {
      "epoch": 1.0890632210429165,
      "grad_norm": 0.8472812175750732,
      "learning_rate": 6.370302004819772e-05,
      "loss": 2.2149,
      "step": 7080
    },
    {
      "epoch": 1.090601445931395,
      "grad_norm": 0.7601071000099182,
      "learning_rate": 6.365174588524843e-05,
      "loss": 2.227,
      "step": 7090
    },
    {
      "epoch": 1.0921396708198738,
      "grad_norm": 1.1337275505065918,
      "learning_rate": 6.360047172229914e-05,
      "loss": 2.309,
      "step": 7100
    },
    {
      "epoch": 1.0936778957083526,
      "grad_norm": 1.010982871055603,
      "learning_rate": 6.354919755934984e-05,
      "loss": 2.3151,
      "step": 7110
    },
    {
      "epoch": 1.0952161205968312,
      "grad_norm": 0.914633572101593,
      "learning_rate": 6.349792339640055e-05,
      "loss": 2.2061,
      "step": 7120
    },
    {
      "epoch": 1.09675434548531,
      "grad_norm": 1.1274361610412598,
      "learning_rate": 6.344664923345128e-05,
      "loss": 2.2603,
      "step": 7130
    },
    {
      "epoch": 1.0982925703737887,
      "grad_norm": 1.2371611595153809,
      "learning_rate": 6.339537507050197e-05,
      "loss": 2.2709,
      "step": 7140
    },
    {
      "epoch": 1.0998307952622675,
      "grad_norm": 1.049005150794983,
      "learning_rate": 6.334410090755268e-05,
      "loss": 2.2591,
      "step": 7150
    },
    {
      "epoch": 1.101369020150746,
      "grad_norm": 1.0838549137115479,
      "learning_rate": 6.329282674460339e-05,
      "loss": 2.2446,
      "step": 7160
    },
    {
      "epoch": 1.1029072450392248,
      "grad_norm": 1.0882234573364258,
      "learning_rate": 6.324155258165411e-05,
      "loss": 2.1473,
      "step": 7170
    },
    {
      "epoch": 1.1044454699277035,
      "grad_norm": 0.9437506198883057,
      "learning_rate": 6.319027841870482e-05,
      "loss": 2.2951,
      "step": 7180
    },
    {
      "epoch": 1.105983694816182,
      "grad_norm": 0.703149676322937,
      "learning_rate": 6.313900425575553e-05,
      "loss": 2.2377,
      "step": 7190
    },
    {
      "epoch": 1.1075219197046609,
      "grad_norm": 1.0424326658248901,
      "learning_rate": 6.308773009280624e-05,
      "loss": 2.2176,
      "step": 7200
    },
    {
      "epoch": 1.1090601445931396,
      "grad_norm": 1.1478837728500366,
      "learning_rate": 6.303645592985695e-05,
      "loss": 2.1371,
      "step": 7210
    },
    {
      "epoch": 1.1105983694816182,
      "grad_norm": 0.8235120177268982,
      "learning_rate": 6.298518176690766e-05,
      "loss": 2.1736,
      "step": 7220
    },
    {
      "epoch": 1.112136594370097,
      "grad_norm": 1.2811030149459839,
      "learning_rate": 6.293390760395837e-05,
      "loss": 2.1926,
      "step": 7230
    },
    {
      "epoch": 1.1136748192585757,
      "grad_norm": 0.6759135127067566,
      "learning_rate": 6.288263344100908e-05,
      "loss": 2.3203,
      "step": 7240
    },
    {
      "epoch": 1.1152130441470542,
      "grad_norm": 0.8137600421905518,
      "learning_rate": 6.283135927805979e-05,
      "loss": 2.164,
      "step": 7250
    },
    {
      "epoch": 1.116751269035533,
      "grad_norm": 0.9689512252807617,
      "learning_rate": 6.27800851151105e-05,
      "loss": 2.202,
      "step": 7260
    },
    {
      "epoch": 1.1182894939240118,
      "grad_norm": 0.8917565941810608,
      "learning_rate": 6.27288109521612e-05,
      "loss": 2.2503,
      "step": 7270
    },
    {
      "epoch": 1.1198277188124903,
      "grad_norm": 1.0151695013046265,
      "learning_rate": 6.267753678921193e-05,
      "loss": 2.2695,
      "step": 7280
    },
    {
      "epoch": 1.121365943700969,
      "grad_norm": 0.9036550521850586,
      "learning_rate": 6.262626262626264e-05,
      "loss": 2.1395,
      "step": 7290
    },
    {
      "epoch": 1.1229041685894479,
      "grad_norm": 1.0213050842285156,
      "learning_rate": 6.257498846331334e-05,
      "loss": 2.2717,
      "step": 7300
    },
    {
      "epoch": 1.1244423934779264,
      "grad_norm": 0.716913104057312,
      "learning_rate": 6.252371430036404e-05,
      "loss": 2.3301,
      "step": 7310
    },
    {
      "epoch": 1.1259806183664052,
      "grad_norm": 1.0038708448410034,
      "learning_rate": 6.247244013741476e-05,
      "loss": 2.2637,
      "step": 7320
    },
    {
      "epoch": 1.127518843254884,
      "grad_norm": 0.8744848370552063,
      "learning_rate": 6.242116597446547e-05,
      "loss": 2.3023,
      "step": 7330
    },
    {
      "epoch": 1.1290570681433625,
      "grad_norm": 1.0232914686203003,
      "learning_rate": 6.236989181151618e-05,
      "loss": 2.2696,
      "step": 7340
    },
    {
      "epoch": 1.1305952930318413,
      "grad_norm": 0.8365628719329834,
      "learning_rate": 6.231861764856689e-05,
      "loss": 2.2621,
      "step": 7350
    },
    {
      "epoch": 1.13213351792032,
      "grad_norm": 1.0771702527999878,
      "learning_rate": 6.22673434856176e-05,
      "loss": 2.3472,
      "step": 7360
    },
    {
      "epoch": 1.1336717428087986,
      "grad_norm": 0.7984094023704529,
      "learning_rate": 6.221606932266831e-05,
      "loss": 2.1851,
      "step": 7370
    },
    {
      "epoch": 1.1352099676972773,
      "grad_norm": 0.9673691391944885,
      "learning_rate": 6.216479515971903e-05,
      "loss": 2.1209,
      "step": 7380
    },
    {
      "epoch": 1.136748192585756,
      "grad_norm": 0.9815546274185181,
      "learning_rate": 6.211352099676974e-05,
      "loss": 2.3249,
      "step": 7390
    },
    {
      "epoch": 1.1382864174742346,
      "grad_norm": 0.7448471784591675,
      "learning_rate": 6.206224683382044e-05,
      "loss": 2.1723,
      "step": 7400
    },
    {
      "epoch": 1.1398246423627134,
      "grad_norm": 0.929509162902832,
      "learning_rate": 6.201097267087114e-05,
      "loss": 2.1861,
      "step": 7410
    },
    {
      "epoch": 1.1413628672511922,
      "grad_norm": 0.8915154337882996,
      "learning_rate": 6.195969850792185e-05,
      "loss": 2.1586,
      "step": 7420
    },
    {
      "epoch": 1.1429010921396707,
      "grad_norm": 1.0364996194839478,
      "learning_rate": 6.190842434497258e-05,
      "loss": 2.3141,
      "step": 7430
    },
    {
      "epoch": 1.1444393170281495,
      "grad_norm": 0.8384507298469543,
      "learning_rate": 6.185715018202329e-05,
      "loss": 2.217,
      "step": 7440
    },
    {
      "epoch": 1.1459775419166283,
      "grad_norm": 1.2669347524642944,
      "learning_rate": 6.1805876019074e-05,
      "loss": 2.2612,
      "step": 7450
    },
    {
      "epoch": 1.1475157668051068,
      "grad_norm": 1.117998719215393,
      "learning_rate": 6.17546018561247e-05,
      "loss": 2.2304,
      "step": 7460
    },
    {
      "epoch": 1.1490539916935856,
      "grad_norm": 0.7965844869613647,
      "learning_rate": 6.170332769317541e-05,
      "loss": 2.2767,
      "step": 7470
    },
    {
      "epoch": 1.1505922165820643,
      "grad_norm": 1.1716057062149048,
      "learning_rate": 6.165205353022612e-05,
      "loss": 2.3669,
      "step": 7480
    },
    {
      "epoch": 1.1521304414705429,
      "grad_norm": 0.9950526356697083,
      "learning_rate": 6.160077936727683e-05,
      "loss": 2.2936,
      "step": 7490
    },
    {
      "epoch": 1.1536686663590217,
      "grad_norm": 0.9135560393333435,
      "learning_rate": 6.154950520432754e-05,
      "loss": 2.1998,
      "step": 7500
    },
    {
      "epoch": 1.1552068912475004,
      "grad_norm": 0.8585549592971802,
      "learning_rate": 6.149823104137825e-05,
      "loss": 2.1146,
      "step": 7510
    },
    {
      "epoch": 1.156745116135979,
      "grad_norm": 0.7925834059715271,
      "learning_rate": 6.144695687842896e-05,
      "loss": 2.1298,
      "step": 7520
    },
    {
      "epoch": 1.1582833410244577,
      "grad_norm": 0.8777455687522888,
      "learning_rate": 6.139568271547968e-05,
      "loss": 2.2055,
      "step": 7530
    },
    {
      "epoch": 1.1598215659129365,
      "grad_norm": 1.0683585405349731,
      "learning_rate": 6.134440855253039e-05,
      "loss": 2.1206,
      "step": 7540
    },
    {
      "epoch": 1.161359790801415,
      "grad_norm": 1.1106852293014526,
      "learning_rate": 6.12931343895811e-05,
      "loss": 2.2331,
      "step": 7550
    },
    {
      "epoch": 1.1628980156898938,
      "grad_norm": 1.0741558074951172,
      "learning_rate": 6.124186022663181e-05,
      "loss": 2.1262,
      "step": 7560
    },
    {
      "epoch": 1.1644362405783726,
      "grad_norm": 1.3709325790405273,
      "learning_rate": 6.11905860636825e-05,
      "loss": 2.2587,
      "step": 7570
    },
    {
      "epoch": 1.1659744654668511,
      "grad_norm": 0.7741462588310242,
      "learning_rate": 6.113931190073323e-05,
      "loss": 2.2598,
      "step": 7580
    },
    {
      "epoch": 1.16751269035533,
      "grad_norm": 0.8759410977363586,
      "learning_rate": 6.108803773778394e-05,
      "loss": 2.3204,
      "step": 7590
    },
    {
      "epoch": 1.1690509152438087,
      "grad_norm": 1.2429133653640747,
      "learning_rate": 6.103676357483464e-05,
      "loss": 2.1956,
      "step": 7600
    },
    {
      "epoch": 1.1705891401322874,
      "grad_norm": 1.299584984779358,
      "learning_rate": 6.098548941188535e-05,
      "loss": 2.3655,
      "step": 7610
    },
    {
      "epoch": 1.172127365020766,
      "grad_norm": 0.901606559753418,
      "learning_rate": 6.093421524893607e-05,
      "loss": 2.2719,
      "step": 7620
    },
    {
      "epoch": 1.1736655899092447,
      "grad_norm": 1.0715243816375732,
      "learning_rate": 6.088294108598678e-05,
      "loss": 2.1938,
      "step": 7630
    },
    {
      "epoch": 1.1752038147977235,
      "grad_norm": 0.7615361213684082,
      "learning_rate": 6.083166692303749e-05,
      "loss": 2.205,
      "step": 7640
    },
    {
      "epoch": 1.176742039686202,
      "grad_norm": 1.1398166418075562,
      "learning_rate": 6.078039276008819e-05,
      "loss": 2.2196,
      "step": 7650
    },
    {
      "epoch": 1.1782802645746808,
      "grad_norm": 0.9141654372215271,
      "learning_rate": 6.07291185971389e-05,
      "loss": 2.3032,
      "step": 7660
    },
    {
      "epoch": 1.1798184894631596,
      "grad_norm": 1.0039955377578735,
      "learning_rate": 6.0677844434189615e-05,
      "loss": 2.2526,
      "step": 7670
    },
    {
      "epoch": 1.1813567143516381,
      "grad_norm": 0.9163035154342651,
      "learning_rate": 6.0626570271240324e-05,
      "loss": 2.253,
      "step": 7680
    },
    {
      "epoch": 1.182894939240117,
      "grad_norm": 0.6733828783035278,
      "learning_rate": 6.057529610829103e-05,
      "loss": 2.1992,
      "step": 7690
    },
    {
      "epoch": 1.1844331641285957,
      "grad_norm": 1.4401814937591553,
      "learning_rate": 6.052402194534175e-05,
      "loss": 2.2153,
      "step": 7700
    },
    {
      "epoch": 1.1859713890170742,
      "grad_norm": 0.833088219165802,
      "learning_rate": 6.047274778239246e-05,
      "loss": 2.3634,
      "step": 7710
    },
    {
      "epoch": 1.187509613905553,
      "grad_norm": 0.9359910488128662,
      "learning_rate": 6.042147361944317e-05,
      "loss": 2.2412,
      "step": 7720
    },
    {
      "epoch": 1.1890478387940318,
      "grad_norm": 0.983386754989624,
      "learning_rate": 6.037019945649388e-05,
      "loss": 2.1627,
      "step": 7730
    },
    {
      "epoch": 1.1905860636825103,
      "grad_norm": 0.8891242146492004,
      "learning_rate": 6.031892529354458e-05,
      "loss": 2.269,
      "step": 7740
    },
    {
      "epoch": 1.192124288570989,
      "grad_norm": 0.7552980780601501,
      "learning_rate": 6.0267651130595294e-05,
      "loss": 2.2415,
      "step": 7750
    },
    {
      "epoch": 1.1936625134594678,
      "grad_norm": 0.8597180247306824,
      "learning_rate": 6.0216376967646e-05,
      "loss": 2.0983,
      "step": 7760
    },
    {
      "epoch": 1.1952007383479464,
      "grad_norm": 1.0100406408309937,
      "learning_rate": 6.016510280469672e-05,
      "loss": 2.1934,
      "step": 7770
    },
    {
      "epoch": 1.1967389632364251,
      "grad_norm": 0.8115502595901489,
      "learning_rate": 6.011382864174743e-05,
      "loss": 2.2018,
      "step": 7780
    },
    {
      "epoch": 1.198277188124904,
      "grad_norm": 1.1012628078460693,
      "learning_rate": 6.006255447879814e-05,
      "loss": 2.1351,
      "step": 7790
    },
    {
      "epoch": 1.1998154130133827,
      "grad_norm": 0.8618811964988708,
      "learning_rate": 6.001128031584885e-05,
      "loss": 2.2441,
      "step": 7800
    },
    {
      "epoch": 1.2013536379018612,
      "grad_norm": 1.1551060676574707,
      "learning_rate": 5.996000615289956e-05,
      "loss": 2.2586,
      "step": 7810
    },
    {
      "epoch": 1.20289186279034,
      "grad_norm": 0.9102763533592224,
      "learning_rate": 5.9908731989950265e-05,
      "loss": 2.2586,
      "step": 7820
    },
    {
      "epoch": 1.2044300876788188,
      "grad_norm": 0.9427610635757446,
      "learning_rate": 5.9857457827000974e-05,
      "loss": 2.2464,
      "step": 7830
    },
    {
      "epoch": 1.2059683125672973,
      "grad_norm": 1.1695860624313354,
      "learning_rate": 5.980618366405168e-05,
      "loss": 2.2865,
      "step": 7840
    },
    {
      "epoch": 1.207506537455776,
      "grad_norm": 0.87127685546875,
      "learning_rate": 5.97549095011024e-05,
      "loss": 2.2142,
      "step": 7850
    },
    {
      "epoch": 1.2090447623442548,
      "grad_norm": 1.0517553091049194,
      "learning_rate": 5.970363533815311e-05,
      "loss": 2.2696,
      "step": 7860
    },
    {
      "epoch": 1.2105829872327334,
      "grad_norm": 1.1802926063537598,
      "learning_rate": 5.965236117520382e-05,
      "loss": 2.306,
      "step": 7870
    },
    {
      "epoch": 1.2121212121212122,
      "grad_norm": 1.1745392084121704,
      "learning_rate": 5.960108701225453e-05,
      "loss": 2.315,
      "step": 7880
    },
    {
      "epoch": 1.213659437009691,
      "grad_norm": 0.9966297745704651,
      "learning_rate": 5.954981284930524e-05,
      "loss": 2.2618,
      "step": 7890
    },
    {
      "epoch": 1.2151976618981695,
      "grad_norm": 1.1408624649047852,
      "learning_rate": 5.949853868635595e-05,
      "loss": 2.2,
      "step": 7900
    },
    {
      "epoch": 1.2167358867866482,
      "grad_norm": 0.9806095957756042,
      "learning_rate": 5.944726452340665e-05,
      "loss": 2.2568,
      "step": 7910
    },
    {
      "epoch": 1.218274111675127,
      "grad_norm": 1.0086625814437866,
      "learning_rate": 5.939599036045736e-05,
      "loss": 2.1933,
      "step": 7920
    },
    {
      "epoch": 1.2198123365636055,
      "grad_norm": 1.091147541999817,
      "learning_rate": 5.934471619750808e-05,
      "loss": 2.2271,
      "step": 7930
    },
    {
      "epoch": 1.2213505614520843,
      "grad_norm": 1.1544301509857178,
      "learning_rate": 5.929344203455879e-05,
      "loss": 2.1912,
      "step": 7940
    },
    {
      "epoch": 1.222888786340563,
      "grad_norm": 1.390708327293396,
      "learning_rate": 5.92421678716095e-05,
      "loss": 2.1649,
      "step": 7950
    },
    {
      "epoch": 1.2244270112290416,
      "grad_norm": 0.9827846884727478,
      "learning_rate": 5.919089370866021e-05,
      "loss": 2.1832,
      "step": 7960
    },
    {
      "epoch": 1.2259652361175204,
      "grad_norm": 0.9480863213539124,
      "learning_rate": 5.913961954571092e-05,
      "loss": 2.3117,
      "step": 7970
    },
    {
      "epoch": 1.2275034610059992,
      "grad_norm": 0.9496193528175354,
      "learning_rate": 5.908834538276164e-05,
      "loss": 2.1054,
      "step": 7980
    },
    {
      "epoch": 1.2290416858944777,
      "grad_norm": 1.035103440284729,
      "learning_rate": 5.903707121981233e-05,
      "loss": 2.1834,
      "step": 7990
    },
    {
      "epoch": 1.2305799107829565,
      "grad_norm": 0.8460426926612854,
      "learning_rate": 5.898579705686305e-05,
      "loss": 2.2326,
      "step": 8000
    },
    {
      "epoch": 1.2321181356714352,
      "grad_norm": 0.9367645382881165,
      "learning_rate": 5.893452289391376e-05,
      "loss": 2.2186,
      "step": 8010
    },
    {
      "epoch": 1.2336563605599138,
      "grad_norm": 0.8949615955352783,
      "learning_rate": 5.8883248730964467e-05,
      "loss": 2.1282,
      "step": 8020
    },
    {
      "epoch": 1.2351945854483926,
      "grad_norm": 0.784010112285614,
      "learning_rate": 5.883197456801518e-05,
      "loss": 2.2048,
      "step": 8030
    },
    {
      "epoch": 1.2367328103368713,
      "grad_norm": 0.7175517082214355,
      "learning_rate": 5.878070040506589e-05,
      "loss": 2.0957,
      "step": 8040
    },
    {
      "epoch": 1.2382710352253499,
      "grad_norm": 0.9418616890907288,
      "learning_rate": 5.87294262421166e-05,
      "loss": 2.247,
      "step": 8050
    },
    {
      "epoch": 1.2398092601138286,
      "grad_norm": 1.161415934562683,
      "learning_rate": 5.8678152079167316e-05,
      "loss": 2.2268,
      "step": 8060
    },
    {
      "epoch": 1.2413474850023074,
      "grad_norm": 0.9442469477653503,
      "learning_rate": 5.8626877916218026e-05,
      "loss": 2.2101,
      "step": 8070
    },
    {
      "epoch": 1.242885709890786,
      "grad_norm": 0.9710909128189087,
      "learning_rate": 5.857560375326873e-05,
      "loss": 2.2021,
      "step": 8080
    },
    {
      "epoch": 1.2444239347792647,
      "grad_norm": 0.8469917178153992,
      "learning_rate": 5.852432959031944e-05,
      "loss": 2.2658,
      "step": 8090
    },
    {
      "epoch": 1.2459621596677435,
      "grad_norm": 0.901660680770874,
      "learning_rate": 5.8473055427370146e-05,
      "loss": 2.1656,
      "step": 8100
    },
    {
      "epoch": 1.247500384556222,
      "grad_norm": 0.8543119430541992,
      "learning_rate": 5.842178126442086e-05,
      "loss": 2.0628,
      "step": 8110
    },
    {
      "epoch": 1.2490386094447008,
      "grad_norm": 0.9048742055892944,
      "learning_rate": 5.837050710147157e-05,
      "loss": 2.1607,
      "step": 8120
    },
    {
      "epoch": 1.2505768343331796,
      "grad_norm": 0.9281237125396729,
      "learning_rate": 5.831923293852229e-05,
      "loss": 2.2564,
      "step": 8130
    },
    {
      "epoch": 1.252115059221658,
      "grad_norm": 0.8786161541938782,
      "learning_rate": 5.8267958775572996e-05,
      "loss": 2.173,
      "step": 8140
    },
    {
      "epoch": 1.2536532841101369,
      "grad_norm": 1.010661005973816,
      "learning_rate": 5.8216684612623705e-05,
      "loss": 2.1843,
      "step": 8150
    },
    {
      "epoch": 1.2551915089986156,
      "grad_norm": 0.7818912863731384,
      "learning_rate": 5.816541044967441e-05,
      "loss": 2.3153,
      "step": 8160
    },
    {
      "epoch": 1.2567297338870942,
      "grad_norm": 0.8619550466537476,
      "learning_rate": 5.8114136286725116e-05,
      "loss": 2.2191,
      "step": 8170
    },
    {
      "epoch": 1.258267958775573,
      "grad_norm": 0.8553233742713928,
      "learning_rate": 5.806286212377583e-05,
      "loss": 2.1949,
      "step": 8180
    },
    {
      "epoch": 1.2598061836640517,
      "grad_norm": 0.8795005679130554,
      "learning_rate": 5.801158796082654e-05,
      "loss": 2.257,
      "step": 8190
    },
    {
      "epoch": 1.2613444085525303,
      "grad_norm": 0.7444161176681519,
      "learning_rate": 5.796031379787725e-05,
      "loss": 2.2508,
      "step": 8200
    },
    {
      "epoch": 1.262882633441009,
      "grad_norm": 1.0275633335113525,
      "learning_rate": 5.7909039634927966e-05,
      "loss": 2.2455,
      "step": 8210
    },
    {
      "epoch": 1.2644208583294878,
      "grad_norm": 0.9722070097923279,
      "learning_rate": 5.7857765471978675e-05,
      "loss": 2.1853,
      "step": 8220
    },
    {
      "epoch": 1.2659590832179664,
      "grad_norm": 1.0637476444244385,
      "learning_rate": 5.7806491309029385e-05,
      "loss": 2.2647,
      "step": 8230
    },
    {
      "epoch": 1.2674973081064451,
      "grad_norm": 0.9036611318588257,
      "learning_rate": 5.77552171460801e-05,
      "loss": 2.1872,
      "step": 8240
    },
    {
      "epoch": 1.2690355329949239,
      "grad_norm": 1.1068370342254639,
      "learning_rate": 5.7703942983130796e-05,
      "loss": 2.2458,
      "step": 8250
    },
    {
      "epoch": 1.2705737578834024,
      "grad_norm": 0.734359085559845,
      "learning_rate": 5.765266882018151e-05,
      "loss": 2.1944,
      "step": 8260
    },
    {
      "epoch": 1.2721119827718812,
      "grad_norm": 1.0982670783996582,
      "learning_rate": 5.760139465723222e-05,
      "loss": 2.1254,
      "step": 8270
    },
    {
      "epoch": 1.27365020766036,
      "grad_norm": 1.243453025817871,
      "learning_rate": 5.755012049428293e-05,
      "loss": 2.2089,
      "step": 8280
    },
    {
      "epoch": 1.2751884325488385,
      "grad_norm": 0.924658477306366,
      "learning_rate": 5.7498846331333646e-05,
      "loss": 2.186,
      "step": 8290
    },
    {
      "epoch": 1.2767266574373173,
      "grad_norm": 0.8850889205932617,
      "learning_rate": 5.7447572168384355e-05,
      "loss": 2.188,
      "step": 8300
    },
    {
      "epoch": 1.278264882325796,
      "grad_norm": 0.9104663729667664,
      "learning_rate": 5.739629800543507e-05,
      "loss": 2.1941,
      "step": 8310
    },
    {
      "epoch": 1.2798031072142746,
      "grad_norm": 0.9987919926643372,
      "learning_rate": 5.734502384248578e-05,
      "loss": 2.2699,
      "step": 8320
    },
    {
      "epoch": 1.2813413321027534,
      "grad_norm": 1.186160683631897,
      "learning_rate": 5.729374967953648e-05,
      "loss": 2.1978,
      "step": 8330
    },
    {
      "epoch": 1.2828795569912321,
      "grad_norm": 0.9261004328727722,
      "learning_rate": 5.724247551658719e-05,
      "loss": 2.1054,
      "step": 8340
    },
    {
      "epoch": 1.284417781879711,
      "grad_norm": 1.0666427612304688,
      "learning_rate": 5.71912013536379e-05,
      "loss": 2.1773,
      "step": 8350
    },
    {
      "epoch": 1.2859560067681894,
      "grad_norm": 0.8542518019676208,
      "learning_rate": 5.7139927190688616e-05,
      "loss": 2.1793,
      "step": 8360
    },
    {
      "epoch": 1.2874942316566682,
      "grad_norm": 0.8298385739326477,
      "learning_rate": 5.7088653027739325e-05,
      "loss": 2.2374,
      "step": 8370
    },
    {
      "epoch": 1.289032456545147,
      "grad_norm": 0.9397430419921875,
      "learning_rate": 5.7037378864790034e-05,
      "loss": 2.1758,
      "step": 8380
    },
    {
      "epoch": 1.2905706814336255,
      "grad_norm": 0.9157282114028931,
      "learning_rate": 5.698610470184075e-05,
      "loss": 2.1939,
      "step": 8390
    },
    {
      "epoch": 1.2921089063221043,
      "grad_norm": 0.8990046977996826,
      "learning_rate": 5.693483053889146e-05,
      "loss": 2.1419,
      "step": 8400
    },
    {
      "epoch": 1.293647131210583,
      "grad_norm": 0.9593772292137146,
      "learning_rate": 5.688355637594217e-05,
      "loss": 2.1844,
      "step": 8410
    },
    {
      "epoch": 1.2951853560990616,
      "grad_norm": 0.7841231226921082,
      "learning_rate": 5.683228221299287e-05,
      "loss": 2.1248,
      "step": 8420
    },
    {
      "epoch": 1.2967235809875404,
      "grad_norm": 0.9894930124282837,
      "learning_rate": 5.678100805004358e-05,
      "loss": 2.1933,
      "step": 8430
    },
    {
      "epoch": 1.2982618058760191,
      "grad_norm": 0.8496705293655396,
      "learning_rate": 5.6729733887094296e-05,
      "loss": 2.1335,
      "step": 8440
    },
    {
      "epoch": 1.299800030764498,
      "grad_norm": 0.8538897037506104,
      "learning_rate": 5.6678459724145005e-05,
      "loss": 2.2143,
      "step": 8450
    },
    {
      "epoch": 1.3013382556529764,
      "grad_norm": 1.036334753036499,
      "learning_rate": 5.6627185561195714e-05,
      "loss": 2.1617,
      "step": 8460
    },
    {
      "epoch": 1.3028764805414552,
      "grad_norm": 0.8022210001945496,
      "learning_rate": 5.657591139824643e-05,
      "loss": 2.1174,
      "step": 8470
    },
    {
      "epoch": 1.304414705429934,
      "grad_norm": 0.8585205078125,
      "learning_rate": 5.652463723529714e-05,
      "loss": 2.2307,
      "step": 8480
    },
    {
      "epoch": 1.3059529303184125,
      "grad_norm": 0.8896430730819702,
      "learning_rate": 5.647336307234785e-05,
      "loss": 2.2484,
      "step": 8490
    },
    {
      "epoch": 1.3074911552068913,
      "grad_norm": 0.7727290987968445,
      "learning_rate": 5.642208890939855e-05,
      "loss": 2.1634,
      "step": 8500
    },
    {
      "epoch": 1.30902938009537,
      "grad_norm": 1.2526957988739014,
      "learning_rate": 5.6370814746449266e-05,
      "loss": 2.2804,
      "step": 8510
    },
    {
      "epoch": 1.3105676049838486,
      "grad_norm": 1.1041432619094849,
      "learning_rate": 5.6319540583499975e-05,
      "loss": 2.179,
      "step": 8520
    },
    {
      "epoch": 1.3121058298723274,
      "grad_norm": 1.0536202192306519,
      "learning_rate": 5.6268266420550684e-05,
      "loss": 2.3389,
      "step": 8530
    },
    {
      "epoch": 1.3136440547608061,
      "grad_norm": 1.2298942804336548,
      "learning_rate": 5.62169922576014e-05,
      "loss": 2.2762,
      "step": 8540
    },
    {
      "epoch": 1.3151822796492847,
      "grad_norm": 1.0535674095153809,
      "learning_rate": 5.616571809465211e-05,
      "loss": 2.275,
      "step": 8550
    },
    {
      "epoch": 1.3167205045377635,
      "grad_norm": 0.8817421793937683,
      "learning_rate": 5.611444393170282e-05,
      "loss": 2.1836,
      "step": 8560
    },
    {
      "epoch": 1.3182587294262422,
      "grad_norm": 0.7699954509735107,
      "learning_rate": 5.6063169768753534e-05,
      "loss": 2.1715,
      "step": 8570
    },
    {
      "epoch": 1.3197969543147208,
      "grad_norm": 1.2079797983169556,
      "learning_rate": 5.601189560580424e-05,
      "loss": 2.2106,
      "step": 8580
    },
    {
      "epoch": 1.3213351792031995,
      "grad_norm": 1.4297778606414795,
      "learning_rate": 5.5960621442854946e-05,
      "loss": 2.2089,
      "step": 8590
    },
    {
      "epoch": 1.3228734040916783,
      "grad_norm": 1.1483851671218872,
      "learning_rate": 5.5909347279905655e-05,
      "loss": 2.2817,
      "step": 8600
    },
    {
      "epoch": 1.3244116289801569,
      "grad_norm": 0.8095500469207764,
      "learning_rate": 5.5858073116956364e-05,
      "loss": 2.1446,
      "step": 8610
    },
    {
      "epoch": 1.3259498538686356,
      "grad_norm": 0.8542676568031311,
      "learning_rate": 5.580679895400708e-05,
      "loss": 2.3103,
      "step": 8620
    },
    {
      "epoch": 1.3274880787571144,
      "grad_norm": 1.1099188327789307,
      "learning_rate": 5.575552479105779e-05,
      "loss": 2.1193,
      "step": 8630
    },
    {
      "epoch": 1.329026303645593,
      "grad_norm": 1.2849713563919067,
      "learning_rate": 5.57042506281085e-05,
      "loss": 2.2364,
      "step": 8640
    },
    {
      "epoch": 1.3305645285340717,
      "grad_norm": 1.2203119993209839,
      "learning_rate": 5.5652976465159214e-05,
      "loss": 2.263,
      "step": 8650
    },
    {
      "epoch": 1.3321027534225505,
      "grad_norm": 1.1784262657165527,
      "learning_rate": 5.560170230220992e-05,
      "loss": 2.1694,
      "step": 8660
    },
    {
      "epoch": 1.333640978311029,
      "grad_norm": 0.938653826713562,
      "learning_rate": 5.5550428139260625e-05,
      "loss": 2.1508,
      "step": 8670
    },
    {
      "epoch": 1.3351792031995078,
      "grad_norm": 1.045536994934082,
      "learning_rate": 5.5499153976311334e-05,
      "loss": 2.2219,
      "step": 8680
    },
    {
      "epoch": 1.3367174280879865,
      "grad_norm": 1.028208613395691,
      "learning_rate": 5.544787981336205e-05,
      "loss": 2.3113,
      "step": 8690
    },
    {
      "epoch": 1.338255652976465,
      "grad_norm": 1.0192244052886963,
      "learning_rate": 5.539660565041276e-05,
      "loss": 2.1927,
      "step": 8700
    },
    {
      "epoch": 1.3397938778649439,
      "grad_norm": 0.8479845523834229,
      "learning_rate": 5.534533148746347e-05,
      "loss": 2.1511,
      "step": 8710
    },
    {
      "epoch": 1.3413321027534226,
      "grad_norm": 1.083945870399475,
      "learning_rate": 5.5294057324514184e-05,
      "loss": 2.1577,
      "step": 8720
    },
    {
      "epoch": 1.3428703276419012,
      "grad_norm": 0.7413824200630188,
      "learning_rate": 5.524278316156489e-05,
      "loss": 2.1822,
      "step": 8730
    },
    {
      "epoch": 1.34440855253038,
      "grad_norm": 0.8767999410629272,
      "learning_rate": 5.51915089986156e-05,
      "loss": 2.1559,
      "step": 8740
    },
    {
      "epoch": 1.3459467774188587,
      "grad_norm": 0.8686264157295227,
      "learning_rate": 5.514023483566632e-05,
      "loss": 2.2497,
      "step": 8750
    },
    {
      "epoch": 1.3474850023073373,
      "grad_norm": 0.9290102124214172,
      "learning_rate": 5.5088960672717014e-05,
      "loss": 2.1552,
      "step": 8760
    },
    {
      "epoch": 1.349023227195816,
      "grad_norm": 0.7726682424545288,
      "learning_rate": 5.503768650976773e-05,
      "loss": 2.2223,
      "step": 8770
    },
    {
      "epoch": 1.3505614520842948,
      "grad_norm": 0.8884676694869995,
      "learning_rate": 5.498641234681844e-05,
      "loss": 2.2444,
      "step": 8780
    },
    {
      "epoch": 1.3520996769727733,
      "grad_norm": 0.8948368430137634,
      "learning_rate": 5.493513818386915e-05,
      "loss": 2.1624,
      "step": 8790
    },
    {
      "epoch": 1.353637901861252,
      "grad_norm": 0.7523174285888672,
      "learning_rate": 5.4883864020919864e-05,
      "loss": 2.2903,
      "step": 8800
    },
    {
      "epoch": 1.3551761267497309,
      "grad_norm": 0.9585885405540466,
      "learning_rate": 5.483258985797057e-05,
      "loss": 2.2009,
      "step": 8810
    },
    {
      "epoch": 1.3567143516382094,
      "grad_norm": 1.0354406833648682,
      "learning_rate": 5.478131569502128e-05,
      "loss": 2.1142,
      "step": 8820
    },
    {
      "epoch": 1.3582525765266882,
      "grad_norm": 0.8949919939041138,
      "learning_rate": 5.4730041532072e-05,
      "loss": 2.2005,
      "step": 8830
    },
    {
      "epoch": 1.359790801415167,
      "grad_norm": 0.8213578462600708,
      "learning_rate": 5.467876736912269e-05,
      "loss": 2.2561,
      "step": 8840
    },
    {
      "epoch": 1.3613290263036455,
      "grad_norm": 1.0482364892959595,
      "learning_rate": 5.462749320617341e-05,
      "loss": 2.1798,
      "step": 8850
    },
    {
      "epoch": 1.3628672511921243,
      "grad_norm": 1.1606894731521606,
      "learning_rate": 5.457621904322412e-05,
      "loss": 2.2282,
      "step": 8860
    },
    {
      "epoch": 1.364405476080603,
      "grad_norm": 0.9929413795471191,
      "learning_rate": 5.4524944880274834e-05,
      "loss": 2.2205,
      "step": 8870
    },
    {
      "epoch": 1.3659437009690816,
      "grad_norm": 0.7543274164199829,
      "learning_rate": 5.447367071732554e-05,
      "loss": 2.1578,
      "step": 8880
    },
    {
      "epoch": 1.3674819258575603,
      "grad_norm": 0.8750208616256714,
      "learning_rate": 5.442239655437625e-05,
      "loss": 2.1882,
      "step": 8890
    },
    {
      "epoch": 1.369020150746039,
      "grad_norm": 1.1033854484558105,
      "learning_rate": 5.437112239142697e-05,
      "loss": 2.2305,
      "step": 8900
    },
    {
      "epoch": 1.3705583756345177,
      "grad_norm": 1.0616145133972168,
      "learning_rate": 5.431984822847768e-05,
      "loss": 2.1766,
      "step": 8910
    },
    {
      "epoch": 1.3720966005229964,
      "grad_norm": 1.1228827238082886,
      "learning_rate": 5.4268574065528386e-05,
      "loss": 2.2185,
      "step": 8920
    },
    {
      "epoch": 1.3736348254114752,
      "grad_norm": 1.0682580471038818,
      "learning_rate": 5.421729990257909e-05,
      "loss": 2.2066,
      "step": 8930
    },
    {
      "epoch": 1.3751730502999537,
      "grad_norm": 0.8566948175430298,
      "learning_rate": 5.41660257396298e-05,
      "loss": 2.1196,
      "step": 8940
    },
    {
      "epoch": 1.3767112751884325,
      "grad_norm": 1.1579430103302002,
      "learning_rate": 5.4114751576680513e-05,
      "loss": 2.1636,
      "step": 8950
    },
    {
      "epoch": 1.3782495000769113,
      "grad_norm": 0.7483777403831482,
      "learning_rate": 5.406347741373122e-05,
      "loss": 2.2573,
      "step": 8960
    },
    {
      "epoch": 1.3797877249653898,
      "grad_norm": 1.0551886558532715,
      "learning_rate": 5.401220325078193e-05,
      "loss": 2.1369,
      "step": 8970
    },
    {
      "epoch": 1.3813259498538686,
      "grad_norm": 0.6933702230453491,
      "learning_rate": 5.396092908783265e-05,
      "loss": 2.1497,
      "step": 8980
    },
    {
      "epoch": 1.3828641747423474,
      "grad_norm": 0.9924807548522949,
      "learning_rate": 5.3909654924883357e-05,
      "loss": 2.1684,
      "step": 8990
    },
    {
      "epoch": 1.384402399630826,
      "grad_norm": 0.8093367218971252,
      "learning_rate": 5.3858380761934066e-05,
      "loss": 2.1735,
      "step": 9000
    },
    {
      "epoch": 1.3859406245193047,
      "grad_norm": 0.9664832949638367,
      "learning_rate": 5.380710659898477e-05,
      "loss": 2.356,
      "step": 9010
    },
    {
      "epoch": 1.3874788494077834,
      "grad_norm": 0.8395592570304871,
      "learning_rate": 5.375583243603548e-05,
      "loss": 2.1907,
      "step": 9020
    },
    {
      "epoch": 1.389017074296262,
      "grad_norm": 1.005774736404419,
      "learning_rate": 5.370455827308619e-05,
      "loss": 2.2155,
      "step": 9030
    },
    {
      "epoch": 1.3905552991847407,
      "grad_norm": 0.8530200719833374,
      "learning_rate": 5.36532841101369e-05,
      "loss": 2.1718,
      "step": 9040
    },
    {
      "epoch": 1.3920935240732195,
      "grad_norm": 1.1464399099349976,
      "learning_rate": 5.360200994718762e-05,
      "loss": 2.1639,
      "step": 9050
    },
    {
      "epoch": 1.3936317489616983,
      "grad_norm": 1.2042144536972046,
      "learning_rate": 5.355073578423833e-05,
      "loss": 2.1184,
      "step": 9060
    },
    {
      "epoch": 1.3951699738501768,
      "grad_norm": 0.9659280776977539,
      "learning_rate": 5.3499461621289036e-05,
      "loss": 2.2054,
      "step": 9070
    },
    {
      "epoch": 1.3967081987386556,
      "grad_norm": 0.93494713306427,
      "learning_rate": 5.344818745833975e-05,
      "loss": 2.1559,
      "step": 9080
    },
    {
      "epoch": 1.3982464236271344,
      "grad_norm": 1.076493263244629,
      "learning_rate": 5.339691329539046e-05,
      "loss": 2.2382,
      "step": 9090
    },
    {
      "epoch": 1.399784648515613,
      "grad_norm": 0.8613061904907227,
      "learning_rate": 5.334563913244116e-05,
      "loss": 2.2525,
      "step": 9100
    },
    {
      "epoch": 1.4013228734040917,
      "grad_norm": 1.1385234594345093,
      "learning_rate": 5.329436496949187e-05,
      "loss": 2.1897,
      "step": 9110
    },
    {
      "epoch": 1.4028610982925704,
      "grad_norm": 0.8608720898628235,
      "learning_rate": 5.324309080654258e-05,
      "loss": 2.3271,
      "step": 9120
    },
    {
      "epoch": 1.404399323181049,
      "grad_norm": 0.9511095881462097,
      "learning_rate": 5.31918166435933e-05,
      "loss": 2.2115,
      "step": 9130
    },
    {
      "epoch": 1.4059375480695278,
      "grad_norm": 0.9527361392974854,
      "learning_rate": 5.3140542480644006e-05,
      "loss": 2.197,
      "step": 9140
    },
    {
      "epoch": 1.4074757729580065,
      "grad_norm": 0.9724103212356567,
      "learning_rate": 5.3089268317694715e-05,
      "loss": 2.1967,
      "step": 9150
    },
    {
      "epoch": 1.4090139978464853,
      "grad_norm": 1.0178250074386597,
      "learning_rate": 5.303799415474543e-05,
      "loss": 2.2594,
      "step": 9160
    },
    {
      "epoch": 1.4105522227349638,
      "grad_norm": 0.9569463133811951,
      "learning_rate": 5.298671999179614e-05,
      "loss": 2.1491,
      "step": 9170
    },
    {
      "epoch": 1.4120904476234426,
      "grad_norm": 1.056908369064331,
      "learning_rate": 5.293544582884684e-05,
      "loss": 2.2316,
      "step": 9180
    },
    {
      "epoch": 1.4136286725119214,
      "grad_norm": 1.026322603225708,
      "learning_rate": 5.288417166589755e-05,
      "loss": 2.2042,
      "step": 9190
    },
    {
      "epoch": 1.4151668974004,
      "grad_norm": 0.9149841666221619,
      "learning_rate": 5.283289750294826e-05,
      "loss": 2.1508,
      "step": 9200
    },
    {
      "epoch": 1.4167051222888787,
      "grad_norm": 1.2366399765014648,
      "learning_rate": 5.278162333999898e-05,
      "loss": 2.2663,
      "step": 9210
    },
    {
      "epoch": 1.4182433471773574,
      "grad_norm": 0.9107800126075745,
      "learning_rate": 5.2730349177049686e-05,
      "loss": 2.1778,
      "step": 9220
    },
    {
      "epoch": 1.419781572065836,
      "grad_norm": 1.1476941108703613,
      "learning_rate": 5.26790750141004e-05,
      "loss": 2.2723,
      "step": 9230
    },
    {
      "epoch": 1.4213197969543148,
      "grad_norm": 1.0032418966293335,
      "learning_rate": 5.262780085115111e-05,
      "loss": 2.1633,
      "step": 9240
    },
    {
      "epoch": 1.4228580218427935,
      "grad_norm": 0.8912023305892944,
      "learning_rate": 5.257652668820182e-05,
      "loss": 2.1627,
      "step": 9250
    },
    {
      "epoch": 1.424396246731272,
      "grad_norm": 1.0589388608932495,
      "learning_rate": 5.2525252525252536e-05,
      "loss": 2.1875,
      "step": 9260
    },
    {
      "epoch": 1.4259344716197508,
      "grad_norm": 0.9956821799278259,
      "learning_rate": 5.247397836230323e-05,
      "loss": 2.1265,
      "step": 9270
    },
    {
      "epoch": 1.4274726965082296,
      "grad_norm": 1.3575109243392944,
      "learning_rate": 5.242270419935395e-05,
      "loss": 2.1792,
      "step": 9280
    },
    {
      "epoch": 1.4290109213967082,
      "grad_norm": 0.8392534852027893,
      "learning_rate": 5.2371430036404656e-05,
      "loss": 2.1776,
      "step": 9290
    },
    {
      "epoch": 1.430549146285187,
      "grad_norm": 1.0281376838684082,
      "learning_rate": 5.2320155873455365e-05,
      "loss": 2.2503,
      "step": 9300
    },
    {
      "epoch": 1.4320873711736657,
      "grad_norm": 0.7888997197151184,
      "learning_rate": 5.226888171050608e-05,
      "loss": 2.3632,
      "step": 9310
    },
    {
      "epoch": 1.4336255960621442,
      "grad_norm": 0.9235263466835022,
      "learning_rate": 5.221760754755679e-05,
      "loss": 2.3464,
      "step": 9320
    },
    {
      "epoch": 1.435163820950623,
      "grad_norm": 0.9702461957931519,
      "learning_rate": 5.21663333846075e-05,
      "loss": 2.1701,
      "step": 9330
    },
    {
      "epoch": 1.4367020458391018,
      "grad_norm": 0.7479716539382935,
      "learning_rate": 5.2115059221658215e-05,
      "loss": 2.2326,
      "step": 9340
    },
    {
      "epoch": 1.4382402707275803,
      "grad_norm": 0.8914846777915955,
      "learning_rate": 5.206378505870891e-05,
      "loss": 2.2444,
      "step": 9350
    },
    {
      "epoch": 1.439778495616059,
      "grad_norm": 0.9451538920402527,
      "learning_rate": 5.201251089575963e-05,
      "loss": 2.2225,
      "step": 9360
    },
    {
      "epoch": 1.4413167205045379,
      "grad_norm": 0.7804110050201416,
      "learning_rate": 5.1961236732810336e-05,
      "loss": 2.2521,
      "step": 9370
    },
    {
      "epoch": 1.4428549453930164,
      "grad_norm": 1.0621085166931152,
      "learning_rate": 5.1909962569861045e-05,
      "loss": 2.1632,
      "step": 9380
    },
    {
      "epoch": 1.4443931702814952,
      "grad_norm": 1.2366708517074585,
      "learning_rate": 5.185868840691176e-05,
      "loss": 2.244,
      "step": 9390
    },
    {
      "epoch": 1.445931395169974,
      "grad_norm": 1.1940531730651855,
      "learning_rate": 5.180741424396247e-05,
      "loss": 2.2854,
      "step": 9400
    },
    {
      "epoch": 1.4474696200584525,
      "grad_norm": 0.9317930340766907,
      "learning_rate": 5.1756140081013186e-05,
      "loss": 2.3098,
      "step": 9410
    },
    {
      "epoch": 1.4490078449469312,
      "grad_norm": 0.8262203931808472,
      "learning_rate": 5.1704865918063895e-05,
      "loss": 2.2204,
      "step": 9420
    },
    {
      "epoch": 1.45054606983541,
      "grad_norm": 1.0155296325683594,
      "learning_rate": 5.1653591755114604e-05,
      "loss": 2.1148,
      "step": 9430
    },
    {
      "epoch": 1.4520842947238886,
      "grad_norm": 0.8822629451751709,
      "learning_rate": 5.1602317592165306e-05,
      "loss": 2.2324,
      "step": 9440
    },
    {
      "epoch": 1.4536225196123673,
      "grad_norm": 0.9279008507728577,
      "learning_rate": 5.1551043429216015e-05,
      "loss": 2.2123,
      "step": 9450
    },
    {
      "epoch": 1.455160744500846,
      "grad_norm": 1.0746718645095825,
      "learning_rate": 5.149976926626673e-05,
      "loss": 2.1247,
      "step": 9460
    },
    {
      "epoch": 1.4566989693893246,
      "grad_norm": 1.1163986921310425,
      "learning_rate": 5.144849510331744e-05,
      "loss": 2.1643,
      "step": 9470
    },
    {
      "epoch": 1.4582371942778034,
      "grad_norm": 0.8609423637390137,
      "learning_rate": 5.139722094036815e-05,
      "loss": 2.2533,
      "step": 9480
    },
    {
      "epoch": 1.4597754191662822,
      "grad_norm": 1.4595050811767578,
      "learning_rate": 5.1345946777418865e-05,
      "loss": 2.1895,
      "step": 9490
    },
    {
      "epoch": 1.4613136440547607,
      "grad_norm": 0.8566328883171082,
      "learning_rate": 5.1294672614469574e-05,
      "loss": 2.1653,
      "step": 9500
    },
    {
      "epoch": 1.4628518689432395,
      "grad_norm": 0.9733169674873352,
      "learning_rate": 5.124339845152028e-05,
      "loss": 2.2383,
      "step": 9510
    },
    {
      "epoch": 1.4643900938317183,
      "grad_norm": 1.2314499616622925,
      "learning_rate": 5.1192124288570986e-05,
      "loss": 2.1869,
      "step": 9520
    },
    {
      "epoch": 1.4659283187201968,
      "grad_norm": 0.836913526058197,
      "learning_rate": 5.1140850125621695e-05,
      "loss": 2.3085,
      "step": 9530
    },
    {
      "epoch": 1.4674665436086756,
      "grad_norm": 0.9236031770706177,
      "learning_rate": 5.108957596267241e-05,
      "loss": 2.1608,
      "step": 9540
    },
    {
      "epoch": 1.4690047684971543,
      "grad_norm": 0.9400492906570435,
      "learning_rate": 5.103830179972312e-05,
      "loss": 2.205,
      "step": 9550
    },
    {
      "epoch": 1.4705429933856329,
      "grad_norm": 0.9410884380340576,
      "learning_rate": 5.098702763677383e-05,
      "loss": 2.1912,
      "step": 9560
    },
    {
      "epoch": 1.4720812182741116,
      "grad_norm": 1.2177613973617554,
      "learning_rate": 5.0935753473824545e-05,
      "loss": 2.2524,
      "step": 9570
    },
    {
      "epoch": 1.4736194431625904,
      "grad_norm": 0.9517473578453064,
      "learning_rate": 5.0884479310875254e-05,
      "loss": 2.1925,
      "step": 9580
    },
    {
      "epoch": 1.475157668051069,
      "grad_norm": 0.744310200214386,
      "learning_rate": 5.083320514792597e-05,
      "loss": 2.1473,
      "step": 9590
    },
    {
      "epoch": 1.4766958929395477,
      "grad_norm": 1.201551914215088,
      "learning_rate": 5.078193098497668e-05,
      "loss": 2.1739,
      "step": 9600
    },
    {
      "epoch": 1.4782341178280265,
      "grad_norm": 0.9071376919746399,
      "learning_rate": 5.073065682202738e-05,
      "loss": 2.2114,
      "step": 9610
    },
    {
      "epoch": 1.479772342716505,
      "grad_norm": 1.0544394254684448,
      "learning_rate": 5.067938265907809e-05,
      "loss": 2.219,
      "step": 9620
    },
    {
      "epoch": 1.4813105676049838,
      "grad_norm": 1.0707221031188965,
      "learning_rate": 5.06281084961288e-05,
      "loss": 2.2639,
      "step": 9630
    },
    {
      "epoch": 1.4828487924934626,
      "grad_norm": 1.083210825920105,
      "learning_rate": 5.0576834333179515e-05,
      "loss": 2.259,
      "step": 9640
    },
    {
      "epoch": 1.4843870173819411,
      "grad_norm": 0.9080063104629517,
      "learning_rate": 5.0525560170230224e-05,
      "loss": 2.1437,
      "step": 9650
    },
    {
      "epoch": 1.4859252422704199,
      "grad_norm": 1.0296835899353027,
      "learning_rate": 5.047428600728093e-05,
      "loss": 2.2346,
      "step": 9660
    },
    {
      "epoch": 1.4874634671588987,
      "grad_norm": 0.8965573906898499,
      "learning_rate": 5.042301184433165e-05,
      "loss": 2.2956,
      "step": 9670
    },
    {
      "epoch": 1.4890016920473772,
      "grad_norm": 0.934847891330719,
      "learning_rate": 5.037173768138236e-05,
      "loss": 2.2429,
      "step": 9680
    },
    {
      "epoch": 1.490539916935856,
      "grad_norm": 0.9272753596305847,
      "learning_rate": 5.032046351843306e-05,
      "loss": 2.2253,
      "step": 9690
    },
    {
      "epoch": 1.4920781418243347,
      "grad_norm": 0.7515845894813538,
      "learning_rate": 5.026918935548377e-05,
      "loss": 2.2142,
      "step": 9700
    },
    {
      "epoch": 1.4936163667128133,
      "grad_norm": 0.9632378816604614,
      "learning_rate": 5.021791519253448e-05,
      "loss": 2.1648,
      "step": 9710
    },
    {
      "epoch": 1.495154591601292,
      "grad_norm": 0.7987677454948425,
      "learning_rate": 5.0166641029585194e-05,
      "loss": 2.1931,
      "step": 9720
    },
    {
      "epoch": 1.4966928164897708,
      "grad_norm": 0.7489903569221497,
      "learning_rate": 5.0115366866635904e-05,
      "loss": 2.161,
      "step": 9730
    },
    {
      "epoch": 1.4982310413782494,
      "grad_norm": 0.9383657574653625,
      "learning_rate": 5.006409270368661e-05,
      "loss": 2.2777,
      "step": 9740
    },
    {
      "epoch": 1.4997692662667281,
      "grad_norm": 0.884377121925354,
      "learning_rate": 5.001281854073733e-05,
      "loss": 2.2128,
      "step": 9750
    },
    {
      "epoch": 1.501307491155207,
      "grad_norm": 0.915755033493042,
      "learning_rate": 4.996154437778803e-05,
      "loss": 2.2533,
      "step": 9760
    },
    {
      "epoch": 1.5028457160436854,
      "grad_norm": 1.0794305801391602,
      "learning_rate": 4.991027021483875e-05,
      "loss": 2.1681,
      "step": 9770
    },
    {
      "epoch": 1.5043839409321644,
      "grad_norm": 0.8273089528083801,
      "learning_rate": 4.9858996051889456e-05,
      "loss": 2.2667,
      "step": 9780
    },
    {
      "epoch": 1.505922165820643,
      "grad_norm": 0.9345422387123108,
      "learning_rate": 4.9807721888940165e-05,
      "loss": 2.209,
      "step": 9790
    },
    {
      "epoch": 1.5074603907091215,
      "grad_norm": 0.8396685123443604,
      "learning_rate": 4.9756447725990874e-05,
      "loss": 2.2702,
      "step": 9800
    },
    {
      "epoch": 1.5089986155976005,
      "grad_norm": 0.9333946704864502,
      "learning_rate": 4.970517356304158e-05,
      "loss": 2.3021,
      "step": 9810
    },
    {
      "epoch": 1.510536840486079,
      "grad_norm": 0.7621331810951233,
      "learning_rate": 4.96538994000923e-05,
      "loss": 2.2932,
      "step": 9820
    },
    {
      "epoch": 1.5120750653745576,
      "grad_norm": 1.0785146951675415,
      "learning_rate": 4.960262523714301e-05,
      "loss": 2.256,
      "step": 9830
    },
    {
      "epoch": 1.5136132902630366,
      "grad_norm": 0.7917519807815552,
      "learning_rate": 4.955135107419372e-05,
      "loss": 2.2682,
      "step": 9840
    },
    {
      "epoch": 1.5151515151515151,
      "grad_norm": 1.101636528968811,
      "learning_rate": 4.9500076911244426e-05,
      "loss": 2.275,
      "step": 9850
    },
    {
      "epoch": 1.5166897400399937,
      "grad_norm": 1.1007767915725708,
      "learning_rate": 4.9448802748295135e-05,
      "loss": 2.1944,
      "step": 9860
    },
    {
      "epoch": 1.5182279649284727,
      "grad_norm": 0.8376516103744507,
      "learning_rate": 4.939752858534585e-05,
      "loss": 2.2669,
      "step": 9870
    },
    {
      "epoch": 1.5197661898169512,
      "grad_norm": 0.7304989099502563,
      "learning_rate": 4.934625442239656e-05,
      "loss": 2.2431,
      "step": 9880
    },
    {
      "epoch": 1.52130441470543,
      "grad_norm": 0.8982475996017456,
      "learning_rate": 4.929498025944726e-05,
      "loss": 2.19,
      "step": 9890
    },
    {
      "epoch": 1.5228426395939088,
      "grad_norm": 1.1830205917358398,
      "learning_rate": 4.924370609649798e-05,
      "loss": 2.1506,
      "step": 9900
    },
    {
      "epoch": 1.5243808644823873,
      "grad_norm": 0.8325091004371643,
      "learning_rate": 4.919243193354869e-05,
      "loss": 2.148,
      "step": 9910
    },
    {
      "epoch": 1.525919089370866,
      "grad_norm": 1.1592494249343872,
      "learning_rate": 4.9141157770599397e-05,
      "loss": 2.1998,
      "step": 9920
    },
    {
      "epoch": 1.5274573142593448,
      "grad_norm": 0.9487836956977844,
      "learning_rate": 4.9089883607650106e-05,
      "loss": 2.3358,
      "step": 9930
    },
    {
      "epoch": 1.5289955391478234,
      "grad_norm": 1.041356086730957,
      "learning_rate": 4.9038609444700815e-05,
      "loss": 2.2212,
      "step": 9940
    },
    {
      "epoch": 1.5305337640363021,
      "grad_norm": 1.0249658823013306,
      "learning_rate": 4.898733528175153e-05,
      "loss": 2.225,
      "step": 9950
    },
    {
      "epoch": 1.532071988924781,
      "grad_norm": 0.9000499248504639,
      "learning_rate": 4.893606111880224e-05,
      "loss": 2.2555,
      "step": 9960
    },
    {
      "epoch": 1.5336102138132595,
      "grad_norm": 1.3273111581802368,
      "learning_rate": 4.888478695585295e-05,
      "loss": 2.0662,
      "step": 9970
    },
    {
      "epoch": 1.5351484387017382,
      "grad_norm": 1.1795570850372314,
      "learning_rate": 4.883351279290366e-05,
      "loss": 2.2382,
      "step": 9980
    },
    {
      "epoch": 1.536686663590217,
      "grad_norm": 0.9749130010604858,
      "learning_rate": 4.878223862995437e-05,
      "loss": 2.2158,
      "step": 9990
    },
    {
      "epoch": 1.5382248884786955,
      "grad_norm": 0.9773431420326233,
      "learning_rate": 4.873096446700508e-05,
      "loss": 2.1938,
      "step": 10000
    },
    {
      "epoch": 1.5397631133671743,
      "grad_norm": 1.0615180730819702,
      "learning_rate": 4.8679690304055785e-05,
      "loss": 2.2731,
      "step": 10010
    },
    {
      "epoch": 1.541301338255653,
      "grad_norm": 1.0366125106811523,
      "learning_rate": 4.8628416141106494e-05,
      "loss": 2.1954,
      "step": 10020
    },
    {
      "epoch": 1.5428395631441316,
      "grad_norm": 0.9686338305473328,
      "learning_rate": 4.857714197815721e-05,
      "loss": 2.2554,
      "step": 10030
    },
    {
      "epoch": 1.5443777880326104,
      "grad_norm": 0.8940095901489258,
      "learning_rate": 4.852586781520792e-05,
      "loss": 2.1988,
      "step": 10040
    },
    {
      "epoch": 1.5459160129210892,
      "grad_norm": 1.2119660377502441,
      "learning_rate": 4.8474593652258635e-05,
      "loss": 2.1766,
      "step": 10050
    },
    {
      "epoch": 1.5474542378095677,
      "grad_norm": 1.1432909965515137,
      "learning_rate": 4.842331948930934e-05,
      "loss": 2.2985,
      "step": 10060
    },
    {
      "epoch": 1.5489924626980465,
      "grad_norm": 0.8545533418655396,
      "learning_rate": 4.8372045326360046e-05,
      "loss": 2.225,
      "step": 10070
    },
    {
      "epoch": 1.5505306875865252,
      "grad_norm": 0.83059161901474,
      "learning_rate": 4.832077116341076e-05,
      "loss": 2.1339,
      "step": 10080
    },
    {
      "epoch": 1.5520689124750038,
      "grad_norm": 1.0068085193634033,
      "learning_rate": 4.826949700046147e-05,
      "loss": 2.2426,
      "step": 10090
    },
    {
      "epoch": 1.5536071373634825,
      "grad_norm": 1.2583895921707153,
      "learning_rate": 4.821822283751218e-05,
      "loss": 2.2558,
      "step": 10100
    },
    {
      "epoch": 1.5551453622519613,
      "grad_norm": 0.9470310807228088,
      "learning_rate": 4.816694867456289e-05,
      "loss": 2.2362,
      "step": 10110
    },
    {
      "epoch": 1.5566835871404399,
      "grad_norm": 0.8292353749275208,
      "learning_rate": 4.81156745116136e-05,
      "loss": 2.1934,
      "step": 10120
    },
    {
      "epoch": 1.5582218120289186,
      "grad_norm": 0.9707424640655518,
      "learning_rate": 4.8064400348664315e-05,
      "loss": 2.1517,
      "step": 10130
    },
    {
      "epoch": 1.5597600369173974,
      "grad_norm": 1.3468843698501587,
      "learning_rate": 4.801312618571502e-05,
      "loss": 2.2192,
      "step": 10140
    },
    {
      "epoch": 1.561298261805876,
      "grad_norm": 0.9781987071037292,
      "learning_rate": 4.796185202276573e-05,
      "loss": 2.2108,
      "step": 10150
    },
    {
      "epoch": 1.5628364866943547,
      "grad_norm": 1.162642002105713,
      "learning_rate": 4.791057785981644e-05,
      "loss": 2.2345,
      "step": 10160
    },
    {
      "epoch": 1.5643747115828335,
      "grad_norm": 0.8542919754981995,
      "learning_rate": 4.785930369686715e-05,
      "loss": 2.1396,
      "step": 10170
    },
    {
      "epoch": 1.565912936471312,
      "grad_norm": 1.0202232599258423,
      "learning_rate": 4.780802953391786e-05,
      "loss": 2.1706,
      "step": 10180
    },
    {
      "epoch": 1.5674511613597908,
      "grad_norm": 0.8557865023612976,
      "learning_rate": 4.775675537096857e-05,
      "loss": 2.1931,
      "step": 10190
    },
    {
      "epoch": 1.5689893862482696,
      "grad_norm": 0.9657171964645386,
      "learning_rate": 4.770548120801928e-05,
      "loss": 2.2528,
      "step": 10200
    },
    {
      "epoch": 1.570527611136748,
      "grad_norm": 1.1246685981750488,
      "learning_rate": 4.7654207045069994e-05,
      "loss": 2.2063,
      "step": 10210
    },
    {
      "epoch": 1.5720658360252269,
      "grad_norm": 0.9583677649497986,
      "learning_rate": 4.76029328821207e-05,
      "loss": 2.142,
      "step": 10220
    },
    {
      "epoch": 1.5736040609137056,
      "grad_norm": 0.9309014081954956,
      "learning_rate": 4.755165871917141e-05,
      "loss": 2.3356,
      "step": 10230
    },
    {
      "epoch": 1.5751422858021842,
      "grad_norm": 1.0112676620483398,
      "learning_rate": 4.750038455622212e-05,
      "loss": 2.126,
      "step": 10240
    },
    {
      "epoch": 1.576680510690663,
      "grad_norm": 0.9702624678611755,
      "learning_rate": 4.744911039327283e-05,
      "loss": 2.1984,
      "step": 10250
    },
    {
      "epoch": 1.5782187355791417,
      "grad_norm": 0.9623998403549194,
      "learning_rate": 4.7397836230323546e-05,
      "loss": 2.1452,
      "step": 10260
    },
    {
      "epoch": 1.5797569604676203,
      "grad_norm": 1.070246696472168,
      "learning_rate": 4.734656206737425e-05,
      "loss": 2.0668,
      "step": 10270
    },
    {
      "epoch": 1.581295185356099,
      "grad_norm": 0.977540135383606,
      "learning_rate": 4.7295287904424964e-05,
      "loss": 2.1012,
      "step": 10280
    },
    {
      "epoch": 1.5828334102445778,
      "grad_norm": 0.8494571447372437,
      "learning_rate": 4.7244013741475673e-05,
      "loss": 2.1201,
      "step": 10290
    },
    {
      "epoch": 1.5843716351330563,
      "grad_norm": 0.9772026538848877,
      "learning_rate": 4.719273957852638e-05,
      "loss": 2.2961,
      "step": 10300
    },
    {
      "epoch": 1.5859098600215351,
      "grad_norm": 0.9737097024917603,
      "learning_rate": 4.714146541557709e-05,
      "loss": 2.2618,
      "step": 10310
    },
    {
      "epoch": 1.5874480849100139,
      "grad_norm": 0.8835799098014832,
      "learning_rate": 4.70901912526278e-05,
      "loss": 2.1233,
      "step": 10320
    },
    {
      "epoch": 1.5889863097984924,
      "grad_norm": 0.8602548837661743,
      "learning_rate": 4.7038917089678517e-05,
      "loss": 2.1758,
      "step": 10330
    },
    {
      "epoch": 1.5905245346869712,
      "grad_norm": 0.9785043001174927,
      "learning_rate": 4.6987642926729226e-05,
      "loss": 2.1576,
      "step": 10340
    },
    {
      "epoch": 1.59206275957545,
      "grad_norm": 0.8643372654914856,
      "learning_rate": 4.693636876377993e-05,
      "loss": 2.2098,
      "step": 10350
    },
    {
      "epoch": 1.5936009844639285,
      "grad_norm": 1.1512248516082764,
      "learning_rate": 4.6885094600830644e-05,
      "loss": 2.2473,
      "step": 10360
    },
    {
      "epoch": 1.5951392093524073,
      "grad_norm": 0.8587090373039246,
      "learning_rate": 4.683382043788135e-05,
      "loss": 2.3158,
      "step": 10370
    },
    {
      "epoch": 1.596677434240886,
      "grad_norm": 0.790675938129425,
      "learning_rate": 4.678254627493206e-05,
      "loss": 2.1528,
      "step": 10380
    },
    {
      "epoch": 1.5982156591293646,
      "grad_norm": 1.1372195482254028,
      "learning_rate": 4.673127211198278e-05,
      "loss": 2.2232,
      "step": 10390
    },
    {
      "epoch": 1.5997538840178436,
      "grad_norm": 1.1505094766616821,
      "learning_rate": 4.667999794903348e-05,
      "loss": 2.1978,
      "step": 10400
    },
    {
      "epoch": 1.6012921089063221,
      "grad_norm": 1.0595568418502808,
      "learning_rate": 4.6628723786084196e-05,
      "loss": 2.058,
      "step": 10410
    },
    {
      "epoch": 1.6028303337948007,
      "grad_norm": 1.0009914636611938,
      "learning_rate": 4.6577449623134905e-05,
      "loss": 2.2192,
      "step": 10420
    },
    {
      "epoch": 1.6043685586832797,
      "grad_norm": 1.1201986074447632,
      "learning_rate": 4.6526175460185614e-05,
      "loss": 2.2592,
      "step": 10430
    },
    {
      "epoch": 1.6059067835717582,
      "grad_norm": 0.8145526051521301,
      "learning_rate": 4.647490129723632e-05,
      "loss": 2.1935,
      "step": 10440
    },
    {
      "epoch": 1.6074450084602367,
      "grad_norm": 0.8806764483451843,
      "learning_rate": 4.642362713428703e-05,
      "loss": 2.1631,
      "step": 10450
    },
    {
      "epoch": 1.6089832333487157,
      "grad_norm": 0.7345972061157227,
      "learning_rate": 4.637235297133775e-05,
      "loss": 2.1447,
      "step": 10460
    },
    {
      "epoch": 1.6105214582371943,
      "grad_norm": 0.7220480442047119,
      "learning_rate": 4.632107880838846e-05,
      "loss": 2.1642,
      "step": 10470
    },
    {
      "epoch": 1.6120596831256728,
      "grad_norm": 0.882445752620697,
      "learning_rate": 4.6269804645439166e-05,
      "loss": 2.2511,
      "step": 10480
    },
    {
      "epoch": 1.6135979080141518,
      "grad_norm": 1.174068570137024,
      "learning_rate": 4.6218530482489876e-05,
      "loss": 2.1725,
      "step": 10490
    },
    {
      "epoch": 1.6151361329026304,
      "grad_norm": 0.7780764698982239,
      "learning_rate": 4.6167256319540585e-05,
      "loss": 2.2046,
      "step": 10500
    },
    {
      "epoch": 1.616674357791109,
      "grad_norm": 0.9891849756240845,
      "learning_rate": 4.61159821565913e-05,
      "loss": 2.1968,
      "step": 10510
    },
    {
      "epoch": 1.618212582679588,
      "grad_norm": 0.9563999176025391,
      "learning_rate": 4.6064707993642e-05,
      "loss": 2.1884,
      "step": 10520
    },
    {
      "epoch": 1.6197508075680664,
      "grad_norm": 0.8365343809127808,
      "learning_rate": 4.601343383069271e-05,
      "loss": 2.1805,
      "step": 10530
    },
    {
      "epoch": 1.621289032456545,
      "grad_norm": 0.9702179431915283,
      "learning_rate": 4.596215966774343e-05,
      "loss": 2.2662,
      "step": 10540
    },
    {
      "epoch": 1.622827257345024,
      "grad_norm": 0.7839394807815552,
      "learning_rate": 4.591088550479414e-05,
      "loss": 2.1559,
      "step": 10550
    },
    {
      "epoch": 1.6243654822335025,
      "grad_norm": 1.0188884735107422,
      "learning_rate": 4.5859611341844846e-05,
      "loss": 2.2689,
      "step": 10560
    },
    {
      "epoch": 1.625903707121981,
      "grad_norm": 1.4538068771362305,
      "learning_rate": 4.5808337178895555e-05,
      "loss": 2.2618,
      "step": 10570
    },
    {
      "epoch": 1.62744193201046,
      "grad_norm": 0.8010264039039612,
      "learning_rate": 4.5757063015946264e-05,
      "loss": 2.2583,
      "step": 10580
    },
    {
      "epoch": 1.6289801568989386,
      "grad_norm": 0.8846955895423889,
      "learning_rate": 4.570578885299698e-05,
      "loss": 2.1665,
      "step": 10590
    },
    {
      "epoch": 1.6305183817874174,
      "grad_norm": 1.1565580368041992,
      "learning_rate": 4.565451469004769e-05,
      "loss": 2.2255,
      "step": 10600
    },
    {
      "epoch": 1.6320566066758961,
      "grad_norm": 1.1509038209915161,
      "learning_rate": 4.56032405270984e-05,
      "loss": 2.1785,
      "step": 10610
    },
    {
      "epoch": 1.6335948315643747,
      "grad_norm": 0.8825867772102356,
      "learning_rate": 4.555196636414911e-05,
      "loss": 2.2458,
      "step": 10620
    },
    {
      "epoch": 1.6351330564528535,
      "grad_norm": 1.2291847467422485,
      "learning_rate": 4.5500692201199816e-05,
      "loss": 2.2975,
      "step": 10630
    },
    {
      "epoch": 1.6366712813413322,
      "grad_norm": 0.9114438891410828,
      "learning_rate": 4.544941803825053e-05,
      "loss": 2.1596,
      "step": 10640
    },
    {
      "epoch": 1.6382095062298108,
      "grad_norm": 0.8158903121948242,
      "learning_rate": 4.5398143875301235e-05,
      "loss": 2.2841,
      "step": 10650
    },
    {
      "epoch": 1.6397477311182895,
      "grad_norm": 0.9676698446273804,
      "learning_rate": 4.534686971235195e-05,
      "loss": 2.0646,
      "step": 10660
    },
    {
      "epoch": 1.6412859560067683,
      "grad_norm": 1.1952799558639526,
      "learning_rate": 4.529559554940266e-05,
      "loss": 2.0867,
      "step": 10670
    },
    {
      "epoch": 1.6428241808952468,
      "grad_norm": 0.7560998797416687,
      "learning_rate": 4.524432138645337e-05,
      "loss": 2.1911,
      "step": 10680
    },
    {
      "epoch": 1.6443624057837256,
      "grad_norm": 1.4707295894622803,
      "learning_rate": 4.519304722350408e-05,
      "loss": 2.1446,
      "step": 10690
    },
    {
      "epoch": 1.6459006306722044,
      "grad_norm": 1.1381810903549194,
      "learning_rate": 4.514177306055479e-05,
      "loss": 2.2009,
      "step": 10700
    },
    {
      "epoch": 1.647438855560683,
      "grad_norm": 0.972576916217804,
      "learning_rate": 4.5090498897605496e-05,
      "loss": 2.2713,
      "step": 10710
    },
    {
      "epoch": 1.6489770804491617,
      "grad_norm": 0.890033483505249,
      "learning_rate": 4.503922473465621e-05,
      "loss": 2.2587,
      "step": 10720
    },
    {
      "epoch": 1.6505153053376405,
      "grad_norm": 1.0760796070098877,
      "learning_rate": 4.498795057170692e-05,
      "loss": 2.2643,
      "step": 10730
    },
    {
      "epoch": 1.652053530226119,
      "grad_norm": 0.6318127512931824,
      "learning_rate": 4.493667640875763e-05,
      "loss": 2.2037,
      "step": 10740
    },
    {
      "epoch": 1.6535917551145978,
      "grad_norm": 1.2628130912780762,
      "learning_rate": 4.488540224580834e-05,
      "loss": 2.1757,
      "step": 10750
    },
    {
      "epoch": 1.6551299800030765,
      "grad_norm": 0.9482851624488831,
      "learning_rate": 4.483412808285905e-05,
      "loss": 2.1758,
      "step": 10760
    },
    {
      "epoch": 1.656668204891555,
      "grad_norm": 1.2980858087539673,
      "learning_rate": 4.4782853919909764e-05,
      "loss": 2.1411,
      "step": 10770
    },
    {
      "epoch": 1.6582064297800339,
      "grad_norm": 0.8768374919891357,
      "learning_rate": 4.4731579756960466e-05,
      "loss": 2.2755,
      "step": 10780
    },
    {
      "epoch": 1.6597446546685126,
      "grad_norm": 0.7231269478797913,
      "learning_rate": 4.468030559401118e-05,
      "loss": 2.2072,
      "step": 10790
    },
    {
      "epoch": 1.6612828795569912,
      "grad_norm": 1.0176819562911987,
      "learning_rate": 4.462903143106189e-05,
      "loss": 2.1675,
      "step": 10800
    },
    {
      "epoch": 1.66282110444547,
      "grad_norm": 1.0765310525894165,
      "learning_rate": 4.45777572681126e-05,
      "loss": 2.2122,
      "step": 10810
    },
    {
      "epoch": 1.6643593293339487,
      "grad_norm": 1.0912998914718628,
      "learning_rate": 4.452648310516331e-05,
      "loss": 2.262,
      "step": 10820
    },
    {
      "epoch": 1.6658975542224272,
      "grad_norm": 1.0369638204574585,
      "learning_rate": 4.447520894221402e-05,
      "loss": 2.0157,
      "step": 10830
    },
    {
      "epoch": 1.667435779110906,
      "grad_norm": 1.0435835123062134,
      "learning_rate": 4.4423934779264734e-05,
      "loss": 2.1703,
      "step": 10840
    },
    {
      "epoch": 1.6689740039993848,
      "grad_norm": 1.0186666250228882,
      "learning_rate": 4.437266061631544e-05,
      "loss": 2.2517,
      "step": 10850
    },
    {
      "epoch": 1.6705122288878633,
      "grad_norm": 0.8403745293617249,
      "learning_rate": 4.4321386453366146e-05,
      "loss": 2.2482,
      "step": 10860
    },
    {
      "epoch": 1.672050453776342,
      "grad_norm": 0.9970943331718445,
      "learning_rate": 4.427011229041686e-05,
      "loss": 2.2526,
      "step": 10870
    },
    {
      "epoch": 1.6735886786648209,
      "grad_norm": 1.2017805576324463,
      "learning_rate": 4.421883812746757e-05,
      "loss": 2.294,
      "step": 10880
    },
    {
      "epoch": 1.6751269035532994,
      "grad_norm": 0.749518096446991,
      "learning_rate": 4.416756396451828e-05,
      "loss": 2.2141,
      "step": 10890
    },
    {
      "epoch": 1.6766651284417782,
      "grad_norm": 1.0483466386795044,
      "learning_rate": 4.4116289801568996e-05,
      "loss": 2.0869,
      "step": 10900
    },
    {
      "epoch": 1.678203353330257,
      "grad_norm": 0.9943712949752808,
      "learning_rate": 4.40650156386197e-05,
      "loss": 2.1316,
      "step": 10910
    },
    {
      "epoch": 1.6797415782187355,
      "grad_norm": 1.0447251796722412,
      "learning_rate": 4.4013741475670414e-05,
      "loss": 2.1797,
      "step": 10920
    },
    {
      "epoch": 1.6812798031072143,
      "grad_norm": 0.8944276571273804,
      "learning_rate": 4.396246731272112e-05,
      "loss": 2.2421,
      "step": 10930
    },
    {
      "epoch": 1.682818027995693,
      "grad_norm": 1.0049134492874146,
      "learning_rate": 4.391119314977183e-05,
      "loss": 2.2307,
      "step": 10940
    },
    {
      "epoch": 1.6843562528841716,
      "grad_norm": 1.3204418420791626,
      "learning_rate": 4.385991898682254e-05,
      "loss": 2.1722,
      "step": 10950
    },
    {
      "epoch": 1.6858944777726503,
      "grad_norm": 1.2456625699996948,
      "learning_rate": 4.380864482387325e-05,
      "loss": 2.2089,
      "step": 10960
    },
    {
      "epoch": 1.687432702661129,
      "grad_norm": 1.0450665950775146,
      "learning_rate": 4.3757370660923966e-05,
      "loss": 2.2064,
      "step": 10970
    },
    {
      "epoch": 1.6889709275496076,
      "grad_norm": 0.9484050273895264,
      "learning_rate": 4.3706096497974675e-05,
      "loss": 2.2173,
      "step": 10980
    },
    {
      "epoch": 1.6905091524380864,
      "grad_norm": 1.3225303888320923,
      "learning_rate": 4.365482233502538e-05,
      "loss": 2.2389,
      "step": 10990
    },
    {
      "epoch": 1.6920473773265652,
      "grad_norm": 0.9655736684799194,
      "learning_rate": 4.360354817207609e-05,
      "loss": 2.2809,
      "step": 11000
    },
    {
      "epoch": 1.6935856022150437,
      "grad_norm": 0.8912659287452698,
      "learning_rate": 4.35522740091268e-05,
      "loss": 2.189,
      "step": 11010
    },
    {
      "epoch": 1.6951238271035225,
      "grad_norm": 0.9791345596313477,
      "learning_rate": 4.350099984617751e-05,
      "loss": 2.1846,
      "step": 11020
    },
    {
      "epoch": 1.6966620519920013,
      "grad_norm": 1.039873719215393,
      "learning_rate": 4.344972568322822e-05,
      "loss": 2.2071,
      "step": 11030
    },
    {
      "epoch": 1.6982002768804798,
      "grad_norm": 1.078251600265503,
      "learning_rate": 4.339845152027893e-05,
      "loss": 2.1167,
      "step": 11040
    },
    {
      "epoch": 1.6997385017689586,
      "grad_norm": 0.9968696236610413,
      "learning_rate": 4.3347177357329645e-05,
      "loss": 2.1979,
      "step": 11050
    },
    {
      "epoch": 1.7012767266574373,
      "grad_norm": 0.9127858877182007,
      "learning_rate": 4.3295903194380355e-05,
      "loss": 2.1422,
      "step": 11060
    },
    {
      "epoch": 1.702814951545916,
      "grad_norm": 0.8496168255805969,
      "learning_rate": 4.3244629031431064e-05,
      "loss": 2.1963,
      "step": 11070
    },
    {
      "epoch": 1.7043531764343947,
      "grad_norm": 0.960370659828186,
      "learning_rate": 4.319335486848177e-05,
      "loss": 2.2926,
      "step": 11080
    },
    {
      "epoch": 1.7058914013228734,
      "grad_norm": 0.9071131348609924,
      "learning_rate": 4.314208070553248e-05,
      "loss": 2.1837,
      "step": 11090
    },
    {
      "epoch": 1.707429626211352,
      "grad_norm": 1.470245361328125,
      "learning_rate": 4.30908065425832e-05,
      "loss": 2.2054,
      "step": 11100
    },
    {
      "epoch": 1.708967851099831,
      "grad_norm": 0.9130299687385559,
      "learning_rate": 4.303953237963391e-05,
      "loss": 2.1434,
      "step": 11110
    },
    {
      "epoch": 1.7105060759883095,
      "grad_norm": 0.7954946756362915,
      "learning_rate": 4.2988258216684616e-05,
      "loss": 2.1809,
      "step": 11120
    },
    {
      "epoch": 1.712044300876788,
      "grad_norm": 1.4721170663833618,
      "learning_rate": 4.2936984053735325e-05,
      "loss": 2.2177,
      "step": 11130
    },
    {
      "epoch": 1.713582525765267,
      "grad_norm": 0.8779252767562866,
      "learning_rate": 4.2885709890786034e-05,
      "loss": 2.2438,
      "step": 11140
    },
    {
      "epoch": 1.7151207506537456,
      "grad_norm": 0.9179439544677734,
      "learning_rate": 4.283443572783675e-05,
      "loss": 2.2067,
      "step": 11150
    },
    {
      "epoch": 1.7166589755422241,
      "grad_norm": 1.093463659286499,
      "learning_rate": 4.278316156488745e-05,
      "loss": 2.1716,
      "step": 11160
    },
    {
      "epoch": 1.7181972004307031,
      "grad_norm": 0.7588905692100525,
      "learning_rate": 4.273188740193816e-05,
      "loss": 2.2498,
      "step": 11170
    },
    {
      "epoch": 1.7197354253191817,
      "grad_norm": 1.1686569452285767,
      "learning_rate": 4.268061323898888e-05,
      "loss": 2.1039,
      "step": 11180
    },
    {
      "epoch": 1.7212736502076602,
      "grad_norm": 1.0222331285476685,
      "learning_rate": 4.2629339076039586e-05,
      "loss": 2.1263,
      "step": 11190
    },
    {
      "epoch": 1.7228118750961392,
      "grad_norm": 0.9252604246139526,
      "learning_rate": 4.2578064913090295e-05,
      "loss": 2.1224,
      "step": 11200
    },
    {
      "epoch": 1.7243500999846177,
      "grad_norm": 1.2173511981964111,
      "learning_rate": 4.2526790750141004e-05,
      "loss": 2.2529,
      "step": 11210
    },
    {
      "epoch": 1.7258883248730963,
      "grad_norm": 0.8736873865127563,
      "learning_rate": 4.2475516587191713e-05,
      "loss": 2.2276,
      "step": 11220
    },
    {
      "epoch": 1.7274265497615753,
      "grad_norm": 0.8621634244918823,
      "learning_rate": 4.242424242424243e-05,
      "loss": 2.2045,
      "step": 11230
    },
    {
      "epoch": 1.7289647746500538,
      "grad_norm": 0.755499541759491,
      "learning_rate": 4.237296826129314e-05,
      "loss": 2.3216,
      "step": 11240
    },
    {
      "epoch": 1.7305029995385324,
      "grad_norm": 1.0847158432006836,
      "learning_rate": 4.232169409834385e-05,
      "loss": 2.2822,
      "step": 11250
    },
    {
      "epoch": 1.7320412244270114,
      "grad_norm": 0.9860473871231079,
      "learning_rate": 4.227041993539456e-05,
      "loss": 2.1595,
      "step": 11260
    },
    {
      "epoch": 1.73357944931549,
      "grad_norm": 0.8025517463684082,
      "learning_rate": 4.2219145772445266e-05,
      "loss": 2.0826,
      "step": 11270
    },
    {
      "epoch": 1.7351176742039687,
      "grad_norm": 1.0406826734542847,
      "learning_rate": 4.216787160949598e-05,
      "loss": 2.2338,
      "step": 11280
    },
    {
      "epoch": 1.7366558990924474,
      "grad_norm": 0.9331663846969604,
      "learning_rate": 4.2116597446546684e-05,
      "loss": 2.308,
      "step": 11290
    },
    {
      "epoch": 1.738194123980926,
      "grad_norm": 1.2308180332183838,
      "learning_rate": 4.20653232835974e-05,
      "loss": 2.1819,
      "step": 11300
    },
    {
      "epoch": 1.7397323488694048,
      "grad_norm": 1.057453989982605,
      "learning_rate": 4.201404912064811e-05,
      "loss": 2.0932,
      "step": 11310
    },
    {
      "epoch": 1.7412705737578835,
      "grad_norm": 1.2821558713912964,
      "learning_rate": 4.196277495769882e-05,
      "loss": 2.1695,
      "step": 11320
    },
    {
      "epoch": 1.742808798646362,
      "grad_norm": 0.9327696561813354,
      "learning_rate": 4.191150079474953e-05,
      "loss": 2.1551,
      "step": 11330
    },
    {
      "epoch": 1.7443470235348408,
      "grad_norm": 0.934537947177887,
      "learning_rate": 4.1860226631800236e-05,
      "loss": 2.1082,
      "step": 11340
    },
    {
      "epoch": 1.7458852484233196,
      "grad_norm": 1.0339984893798828,
      "learning_rate": 4.1808952468850945e-05,
      "loss": 2.1912,
      "step": 11350
    },
    {
      "epoch": 1.7474234733117981,
      "grad_norm": 0.8954998850822449,
      "learning_rate": 4.175767830590166e-05,
      "loss": 2.2194,
      "step": 11360
    },
    {
      "epoch": 1.748961698200277,
      "grad_norm": 1.1211893558502197,
      "learning_rate": 4.170640414295236e-05,
      "loss": 2.1561,
      "step": 11370
    },
    {
      "epoch": 1.7504999230887557,
      "grad_norm": 0.8481971025466919,
      "learning_rate": 4.165512998000308e-05,
      "loss": 2.1627,
      "step": 11380
    },
    {
      "epoch": 1.7520381479772342,
      "grad_norm": 0.8754997253417969,
      "learning_rate": 4.160385581705379e-05,
      "loss": 2.2305,
      "step": 11390
    },
    {
      "epoch": 1.753576372865713,
      "grad_norm": 0.9497880935668945,
      "learning_rate": 4.15525816541045e-05,
      "loss": 2.2911,
      "step": 11400
    },
    {
      "epoch": 1.7551145977541918,
      "grad_norm": 0.8692591786384583,
      "learning_rate": 4.150130749115521e-05,
      "loss": 2.2266,
      "step": 11410
    },
    {
      "epoch": 1.7566528226426703,
      "grad_norm": 0.8316658139228821,
      "learning_rate": 4.1450033328205916e-05,
      "loss": 2.1551,
      "step": 11420
    },
    {
      "epoch": 1.758191047531149,
      "grad_norm": 1.1599041223526,
      "learning_rate": 4.139875916525663e-05,
      "loss": 2.2686,
      "step": 11430
    },
    {
      "epoch": 1.7597292724196278,
      "grad_norm": 0.9681688547134399,
      "learning_rate": 4.134748500230734e-05,
      "loss": 2.259,
      "step": 11440
    },
    {
      "epoch": 1.7612674973081064,
      "grad_norm": 1.0962682962417603,
      "learning_rate": 4.129621083935805e-05,
      "loss": 2.2228,
      "step": 11450
    },
    {
      "epoch": 1.7628057221965852,
      "grad_norm": 0.787952721118927,
      "learning_rate": 4.124493667640876e-05,
      "loss": 2.1662,
      "step": 11460
    },
    {
      "epoch": 1.764343947085064,
      "grad_norm": 0.8073084354400635,
      "learning_rate": 4.119366251345947e-05,
      "loss": 2.2166,
      "step": 11470
    },
    {
      "epoch": 1.7658821719735425,
      "grad_norm": 0.9905676245689392,
      "learning_rate": 4.1142388350510184e-05,
      "loss": 2.2096,
      "step": 11480
    },
    {
      "epoch": 1.7674203968620212,
      "grad_norm": 0.9665678143501282,
      "learning_rate": 4.109111418756089e-05,
      "loss": 2.2199,
      "step": 11490
    },
    {
      "epoch": 1.7689586217505,
      "grad_norm": 0.8657187819480896,
      "learning_rate": 4.1039840024611595e-05,
      "loss": 2.2615,
      "step": 11500
    },
    {
      "epoch": 1.7704968466389785,
      "grad_norm": 1.067205786705017,
      "learning_rate": 4.098856586166231e-05,
      "loss": 2.1214,
      "step": 11510
    },
    {
      "epoch": 1.7720350715274573,
      "grad_norm": 0.98930424451828,
      "learning_rate": 4.093729169871302e-05,
      "loss": 2.2392,
      "step": 11520
    },
    {
      "epoch": 1.773573296415936,
      "grad_norm": 0.6271334886550903,
      "learning_rate": 4.088601753576373e-05,
      "loss": 2.1239,
      "step": 11530
    },
    {
      "epoch": 1.7751115213044146,
      "grad_norm": 0.8576090335845947,
      "learning_rate": 4.0834743372814445e-05,
      "loss": 2.201,
      "step": 11540
    },
    {
      "epoch": 1.7766497461928934,
      "grad_norm": 0.9568202495574951,
      "learning_rate": 4.078346920986515e-05,
      "loss": 2.2369,
      "step": 11550
    },
    {
      "epoch": 1.7781879710813722,
      "grad_norm": 1.1567373275756836,
      "learning_rate": 4.073219504691586e-05,
      "loss": 2.2476,
      "step": 11560
    },
    {
      "epoch": 1.7797261959698507,
      "grad_norm": 0.8948328495025635,
      "learning_rate": 4.068092088396657e-05,
      "loss": 2.2584,
      "step": 11570
    },
    {
      "epoch": 1.7812644208583295,
      "grad_norm": 0.9811996817588806,
      "learning_rate": 4.062964672101728e-05,
      "loss": 2.1642,
      "step": 11580
    },
    {
      "epoch": 1.7828026457468082,
      "grad_norm": 1.0205155611038208,
      "learning_rate": 4.057837255806799e-05,
      "loss": 2.224,
      "step": 11590
    },
    {
      "epoch": 1.7843408706352868,
      "grad_norm": 1.1869170665740967,
      "learning_rate": 4.05270983951187e-05,
      "loss": 2.1566,
      "step": 11600
    },
    {
      "epoch": 1.7858790955237656,
      "grad_norm": 0.9409869313240051,
      "learning_rate": 4.0475824232169415e-05,
      "loss": 2.0061,
      "step": 11610
    },
    {
      "epoch": 1.7874173204122443,
      "grad_norm": 1.1590638160705566,
      "learning_rate": 4.0424550069220124e-05,
      "loss": 2.1811,
      "step": 11620
    },
    {
      "epoch": 1.7889555453007229,
      "grad_norm": 0.9096376895904541,
      "learning_rate": 4.037327590627083e-05,
      "loss": 2.1797,
      "step": 11630
    },
    {
      "epoch": 1.7904937701892016,
      "grad_norm": 1.1117632389068604,
      "learning_rate": 4.032200174332154e-05,
      "loss": 2.1263,
      "step": 11640
    },
    {
      "epoch": 1.7920319950776804,
      "grad_norm": 1.0942503213882446,
      "learning_rate": 4.027072758037225e-05,
      "loss": 2.321,
      "step": 11650
    },
    {
      "epoch": 1.793570219966159,
      "grad_norm": 0.8847519755363464,
      "learning_rate": 4.021945341742297e-05,
      "loss": 2.1842,
      "step": 11660
    },
    {
      "epoch": 1.7951084448546377,
      "grad_norm": 0.7275874614715576,
      "learning_rate": 4.016817925447367e-05,
      "loss": 2.1721,
      "step": 11670
    },
    {
      "epoch": 1.7966466697431165,
      "grad_norm": 1.0335015058517456,
      "learning_rate": 4.011690509152438e-05,
      "loss": 2.233,
      "step": 11680
    },
    {
      "epoch": 1.798184894631595,
      "grad_norm": 1.3035504817962646,
      "learning_rate": 4.0065630928575095e-05,
      "loss": 2.3117,
      "step": 11690
    },
    {
      "epoch": 1.7997231195200738,
      "grad_norm": 1.105059266090393,
      "learning_rate": 4.0014356765625804e-05,
      "loss": 2.1169,
      "step": 11700
    },
    {
      "epoch": 1.8012613444085526,
      "grad_norm": 1.1255799531936646,
      "learning_rate": 3.996308260267651e-05,
      "loss": 2.112,
      "step": 11710
    },
    {
      "epoch": 1.8027995692970311,
      "grad_norm": 0.7489379644393921,
      "learning_rate": 3.991180843972722e-05,
      "loss": 2.2808,
      "step": 11720
    },
    {
      "epoch": 1.8043377941855099,
      "grad_norm": 0.9108218550682068,
      "learning_rate": 3.986053427677793e-05,
      "loss": 2.2704,
      "step": 11730
    },
    {
      "epoch": 1.8058760190739886,
      "grad_norm": 0.8234254717826843,
      "learning_rate": 3.980926011382865e-05,
      "loss": 2.2196,
      "step": 11740
    },
    {
      "epoch": 1.8074142439624672,
      "grad_norm": 0.918180525302887,
      "learning_rate": 3.9757985950879356e-05,
      "loss": 2.2098,
      "step": 11750
    },
    {
      "epoch": 1.808952468850946,
      "grad_norm": 1.1317055225372314,
      "learning_rate": 3.9706711787930065e-05,
      "loss": 2.1867,
      "step": 11760
    },
    {
      "epoch": 1.8104906937394247,
      "grad_norm": 1.132336974143982,
      "learning_rate": 3.9655437624980774e-05,
      "loss": 2.227,
      "step": 11770
    },
    {
      "epoch": 1.8120289186279033,
      "grad_norm": 1.0990068912506104,
      "learning_rate": 3.9604163462031483e-05,
      "loss": 2.2206,
      "step": 11780
    },
    {
      "epoch": 1.813567143516382,
      "grad_norm": 0.9561378359794617,
      "learning_rate": 3.95528892990822e-05,
      "loss": 2.2675,
      "step": 11790
    },
    {
      "epoch": 1.8151053684048608,
      "grad_norm": 1.077895164489746,
      "learning_rate": 3.95016151361329e-05,
      "loss": 2.215,
      "step": 11800
    },
    {
      "epoch": 1.8166435932933394,
      "grad_norm": 0.9710025191307068,
      "learning_rate": 3.945034097318361e-05,
      "loss": 2.2388,
      "step": 11810
    },
    {
      "epoch": 1.8181818181818183,
      "grad_norm": 0.9615585207939148,
      "learning_rate": 3.9399066810234327e-05,
      "loss": 2.2071,
      "step": 11820
    },
    {
      "epoch": 1.8197200430702969,
      "grad_norm": 0.966063380241394,
      "learning_rate": 3.9347792647285036e-05,
      "loss": 2.2372,
      "step": 11830
    },
    {
      "epoch": 1.8212582679587754,
      "grad_norm": 0.9995972514152527,
      "learning_rate": 3.9296518484335745e-05,
      "loss": 2.196,
      "step": 11840
    },
    {
      "epoch": 1.8227964928472544,
      "grad_norm": 1.0137927532196045,
      "learning_rate": 3.9245244321386454e-05,
      "loss": 2.2505,
      "step": 11850
    },
    {
      "epoch": 1.824334717735733,
      "grad_norm": 1.084058403968811,
      "learning_rate": 3.919397015843716e-05,
      "loss": 2.1734,
      "step": 11860
    },
    {
      "epoch": 1.8258729426242115,
      "grad_norm": 0.8679330348968506,
      "learning_rate": 3.914269599548788e-05,
      "loss": 2.236,
      "step": 11870
    },
    {
      "epoch": 1.8274111675126905,
      "grad_norm": 1.2254834175109863,
      "learning_rate": 3.909142183253859e-05,
      "loss": 2.2312,
      "step": 11880
    },
    {
      "epoch": 1.828949392401169,
      "grad_norm": 1.0347263813018799,
      "learning_rate": 3.90401476695893e-05,
      "loss": 2.2027,
      "step": 11890
    },
    {
      "epoch": 1.8304876172896476,
      "grad_norm": 1.2514864206314087,
      "learning_rate": 3.8988873506640006e-05,
      "loss": 2.2088,
      "step": 11900
    },
    {
      "epoch": 1.8320258421781266,
      "grad_norm": 0.8166452050209045,
      "learning_rate": 3.8937599343690715e-05,
      "loss": 2.2222,
      "step": 11910
    },
    {
      "epoch": 1.8335640670666051,
      "grad_norm": 0.9240435361862183,
      "learning_rate": 3.888632518074143e-05,
      "loss": 2.1461,
      "step": 11920
    },
    {
      "epoch": 1.8351022919550837,
      "grad_norm": 0.7560679316520691,
      "learning_rate": 3.883505101779213e-05,
      "loss": 2.2314,
      "step": 11930
    },
    {
      "epoch": 1.8366405168435627,
      "grad_norm": 0.7935932874679565,
      "learning_rate": 3.878377685484285e-05,
      "loss": 2.0923,
      "step": 11940
    },
    {
      "epoch": 1.8381787417320412,
      "grad_norm": 1.0431039333343506,
      "learning_rate": 3.873250269189356e-05,
      "loss": 2.1944,
      "step": 11950
    },
    {
      "epoch": 1.8397169666205198,
      "grad_norm": 0.8961933851242065,
      "learning_rate": 3.868122852894427e-05,
      "loss": 2.1128,
      "step": 11960
    },
    {
      "epoch": 1.8412551915089987,
      "grad_norm": 0.9080154895782471,
      "learning_rate": 3.8629954365994976e-05,
      "loss": 2.2121,
      "step": 11970
    },
    {
      "epoch": 1.8427934163974773,
      "grad_norm": 0.8134128451347351,
      "learning_rate": 3.8578680203045685e-05,
      "loss": 2.1482,
      "step": 11980
    },
    {
      "epoch": 1.844331641285956,
      "grad_norm": 0.9392910003662109,
      "learning_rate": 3.8527406040096395e-05,
      "loss": 2.1619,
      "step": 11990
    },
    {
      "epoch": 1.8458698661744348,
      "grad_norm": 0.6601165533065796,
      "learning_rate": 3.847613187714711e-05,
      "loss": 2.2085,
      "step": 12000
    },
    {
      "epoch": 1.8474080910629134,
      "grad_norm": 1.1473982334136963,
      "learning_rate": 3.842485771419781e-05,
      "loss": 2.142,
      "step": 12010
    },
    {
      "epoch": 1.8489463159513921,
      "grad_norm": 0.9024242758750916,
      "learning_rate": 3.837358355124853e-05,
      "loss": 2.2116,
      "step": 12020
    },
    {
      "epoch": 1.850484540839871,
      "grad_norm": 0.8493668437004089,
      "learning_rate": 3.832230938829924e-05,
      "loss": 2.1879,
      "step": 12030
    },
    {
      "epoch": 1.8520227657283495,
      "grad_norm": 0.8794130682945251,
      "learning_rate": 3.827103522534995e-05,
      "loss": 2.2352,
      "step": 12040
    },
    {
      "epoch": 1.8535609906168282,
      "grad_norm": 1.4623746871948242,
      "learning_rate": 3.821976106240066e-05,
      "loss": 2.262,
      "step": 12050
    },
    {
      "epoch": 1.855099215505307,
      "grad_norm": 0.8756735920906067,
      "learning_rate": 3.8168486899451365e-05,
      "loss": 2.1629,
      "step": 12060
    },
    {
      "epoch": 1.8566374403937855,
      "grad_norm": 0.9588733315467834,
      "learning_rate": 3.811721273650208e-05,
      "loss": 2.2614,
      "step": 12070
    },
    {
      "epoch": 1.8581756652822643,
      "grad_norm": 1.2146624326705933,
      "learning_rate": 3.806593857355279e-05,
      "loss": 2.2276,
      "step": 12080
    },
    {
      "epoch": 1.859713890170743,
      "grad_norm": 0.7245152592658997,
      "learning_rate": 3.80146644106035e-05,
      "loss": 2.207,
      "step": 12090
    },
    {
      "epoch": 1.8612521150592216,
      "grad_norm": 1.1283038854599,
      "learning_rate": 3.796339024765421e-05,
      "loss": 2.1988,
      "step": 12100
    },
    {
      "epoch": 1.8627903399477004,
      "grad_norm": 1.4833447933197021,
      "learning_rate": 3.791211608470492e-05,
      "loss": 2.2543,
      "step": 12110
    },
    {
      "epoch": 1.8643285648361791,
      "grad_norm": 0.9001907110214233,
      "learning_rate": 3.786084192175563e-05,
      "loss": 2.1609,
      "step": 12120
    },
    {
      "epoch": 1.8658667897246577,
      "grad_norm": 0.9545237421989441,
      "learning_rate": 3.780956775880634e-05,
      "loss": 2.1768,
      "step": 12130
    },
    {
      "epoch": 1.8674050146131365,
      "grad_norm": 0.7043129801750183,
      "learning_rate": 3.7758293595857044e-05,
      "loss": 2.1146,
      "step": 12140
    },
    {
      "epoch": 1.8689432395016152,
      "grad_norm": 0.9409192800521851,
      "learning_rate": 3.770701943290776e-05,
      "loss": 2.1997,
      "step": 12150
    },
    {
      "epoch": 1.8704814643900938,
      "grad_norm": 0.9809172749519348,
      "learning_rate": 3.765574526995847e-05,
      "loss": 2.2626,
      "step": 12160
    },
    {
      "epoch": 1.8720196892785725,
      "grad_norm": 0.8453825116157532,
      "learning_rate": 3.760447110700918e-05,
      "loss": 2.2982,
      "step": 12170
    },
    {
      "epoch": 1.8735579141670513,
      "grad_norm": 1.0741262435913086,
      "learning_rate": 3.755319694405989e-05,
      "loss": 2.253,
      "step": 12180
    },
    {
      "epoch": 1.8750961390555299,
      "grad_norm": 0.9957574605941772,
      "learning_rate": 3.75019227811106e-05,
      "loss": 2.2271,
      "step": 12190
    },
    {
      "epoch": 1.8766343639440086,
      "grad_norm": 1.0990862846374512,
      "learning_rate": 3.745064861816131e-05,
      "loss": 2.1351,
      "step": 12200
    },
    {
      "epoch": 1.8781725888324874,
      "grad_norm": 1.1993870735168457,
      "learning_rate": 3.739937445521202e-05,
      "loss": 2.2303,
      "step": 12210
    },
    {
      "epoch": 1.879710813720966,
      "grad_norm": 1.0165332555770874,
      "learning_rate": 3.734810029226273e-05,
      "loss": 2.358,
      "step": 12220
    },
    {
      "epoch": 1.8812490386094447,
      "grad_norm": 1.0935810804367065,
      "learning_rate": 3.729682612931344e-05,
      "loss": 2.1755,
      "step": 12230
    },
    {
      "epoch": 1.8827872634979235,
      "grad_norm": 1.2086107730865479,
      "learning_rate": 3.724555196636415e-05,
      "loss": 2.2149,
      "step": 12240
    },
    {
      "epoch": 1.884325488386402,
      "grad_norm": 0.881757378578186,
      "learning_rate": 3.7194277803414865e-05,
      "loss": 2.0977,
      "step": 12250
    },
    {
      "epoch": 1.8858637132748808,
      "grad_norm": 1.2750900983810425,
      "learning_rate": 3.7143003640465574e-05,
      "loss": 2.1302,
      "step": 12260
    },
    {
      "epoch": 1.8874019381633595,
      "grad_norm": 0.9933390021324158,
      "learning_rate": 3.7091729477516276e-05,
      "loss": 2.2314,
      "step": 12270
    },
    {
      "epoch": 1.888940163051838,
      "grad_norm": 1.292952060699463,
      "learning_rate": 3.704045531456699e-05,
      "loss": 2.2008,
      "step": 12280
    },
    {
      "epoch": 1.8904783879403169,
      "grad_norm": 1.2158364057540894,
      "learning_rate": 3.69891811516177e-05,
      "loss": 2.1201,
      "step": 12290
    },
    {
      "epoch": 1.8920166128287956,
      "grad_norm": 0.9663323760032654,
      "learning_rate": 3.693790698866842e-05,
      "loss": 2.0788,
      "step": 12300
    },
    {
      "epoch": 1.8935548377172742,
      "grad_norm": 1.0403294563293457,
      "learning_rate": 3.688663282571912e-05,
      "loss": 2.2393,
      "step": 12310
    },
    {
      "epoch": 1.895093062605753,
      "grad_norm": 1.0525355339050293,
      "learning_rate": 3.683535866276983e-05,
      "loss": 2.3942,
      "step": 12320
    },
    {
      "epoch": 1.8966312874942317,
      "grad_norm": 0.8939074873924255,
      "learning_rate": 3.6784084499820544e-05,
      "loss": 2.1773,
      "step": 12330
    },
    {
      "epoch": 1.8981695123827103,
      "grad_norm": 1.1281812191009521,
      "learning_rate": 3.673281033687125e-05,
      "loss": 2.2248,
      "step": 12340
    },
    {
      "epoch": 1.899707737271189,
      "grad_norm": 0.9633532166481018,
      "learning_rate": 3.668153617392196e-05,
      "loss": 2.211,
      "step": 12350
    },
    {
      "epoch": 1.9012459621596678,
      "grad_norm": 0.9838277101516724,
      "learning_rate": 3.663026201097267e-05,
      "loss": 2.1125,
      "step": 12360
    },
    {
      "epoch": 1.9027841870481463,
      "grad_norm": 1.0912752151489258,
      "learning_rate": 3.657898784802338e-05,
      "loss": 2.231,
      "step": 12370
    },
    {
      "epoch": 1.904322411936625,
      "grad_norm": 1.1526790857315063,
      "learning_rate": 3.6527713685074096e-05,
      "loss": 2.1521,
      "step": 12380
    },
    {
      "epoch": 1.9058606368251039,
      "grad_norm": 1.0216995477676392,
      "learning_rate": 3.6476439522124806e-05,
      "loss": 2.2505,
      "step": 12390
    },
    {
      "epoch": 1.9073988617135824,
      "grad_norm": 1.0945402383804321,
      "learning_rate": 3.6425165359175515e-05,
      "loss": 2.0647,
      "step": 12400
    },
    {
      "epoch": 1.9089370866020612,
      "grad_norm": 1.024196743965149,
      "learning_rate": 3.6373891196226224e-05,
      "loss": 2.2594,
      "step": 12410
    },
    {
      "epoch": 1.91047531149054,
      "grad_norm": 0.779141366481781,
      "learning_rate": 3.632261703327693e-05,
      "loss": 2.2722,
      "step": 12420
    },
    {
      "epoch": 1.9120135363790185,
      "grad_norm": 1.1125221252441406,
      "learning_rate": 3.627134287032765e-05,
      "loss": 2.137,
      "step": 12430
    },
    {
      "epoch": 1.9135517612674973,
      "grad_norm": 0.9177180528640747,
      "learning_rate": 3.622006870737835e-05,
      "loss": 2.0921,
      "step": 12440
    },
    {
      "epoch": 1.915089986155976,
      "grad_norm": 0.9004350304603577,
      "learning_rate": 3.616879454442906e-05,
      "loss": 2.3057,
      "step": 12450
    },
    {
      "epoch": 1.9166282110444546,
      "grad_norm": 1.271777629852295,
      "learning_rate": 3.6117520381479776e-05,
      "loss": 2.2723,
      "step": 12460
    },
    {
      "epoch": 1.9181664359329333,
      "grad_norm": 1.0165590047836304,
      "learning_rate": 3.6066246218530485e-05,
      "loss": 2.1382,
      "step": 12470
    },
    {
      "epoch": 1.9197046608214121,
      "grad_norm": 0.8962761759757996,
      "learning_rate": 3.6014972055581194e-05,
      "loss": 2.1564,
      "step": 12480
    },
    {
      "epoch": 1.9212428857098907,
      "grad_norm": 0.7960043549537659,
      "learning_rate": 3.59636978926319e-05,
      "loss": 2.1683,
      "step": 12490
    },
    {
      "epoch": 1.9227811105983696,
      "grad_norm": 0.9558842778205872,
      "learning_rate": 3.591242372968261e-05,
      "loss": 2.2093,
      "step": 12500
    },
    {
      "epoch": 1.9243193354868482,
      "grad_norm": 1.2171382904052734,
      "learning_rate": 3.586114956673333e-05,
      "loss": 2.2532,
      "step": 12510
    },
    {
      "epoch": 1.9258575603753267,
      "grad_norm": 1.1459585428237915,
      "learning_rate": 3.580987540378403e-05,
      "loss": 2.2156,
      "step": 12520
    },
    {
      "epoch": 1.9273957852638057,
      "grad_norm": 0.8599263429641724,
      "learning_rate": 3.5758601240834746e-05,
      "loss": 2.2009,
      "step": 12530
    },
    {
      "epoch": 1.9289340101522843,
      "grad_norm": 0.9515960812568665,
      "learning_rate": 3.5707327077885455e-05,
      "loss": 2.2006,
      "step": 12540
    },
    {
      "epoch": 1.9304722350407628,
      "grad_norm": 0.9968891143798828,
      "learning_rate": 3.5656052914936164e-05,
      "loss": 2.1913,
      "step": 12550
    },
    {
      "epoch": 1.9320104599292418,
      "grad_norm": 0.8953114748001099,
      "learning_rate": 3.560477875198688e-05,
      "loss": 2.2124,
      "step": 12560
    },
    {
      "epoch": 1.9335486848177204,
      "grad_norm": 1.0475413799285889,
      "learning_rate": 3.555350458903758e-05,
      "loss": 2.2526,
      "step": 12570
    },
    {
      "epoch": 1.935086909706199,
      "grad_norm": 1.1775448322296143,
      "learning_rate": 3.55022304260883e-05,
      "loss": 2.2146,
      "step": 12580
    },
    {
      "epoch": 1.9366251345946779,
      "grad_norm": 0.9113805294036865,
      "learning_rate": 3.545095626313901e-05,
      "loss": 2.2615,
      "step": 12590
    },
    {
      "epoch": 1.9381633594831564,
      "grad_norm": 0.8998093605041504,
      "learning_rate": 3.539968210018972e-05,
      "loss": 2.1651,
      "step": 12600
    },
    {
      "epoch": 1.939701584371635,
      "grad_norm": 0.914761483669281,
      "learning_rate": 3.5348407937240426e-05,
      "loss": 2.1382,
      "step": 12610
    },
    {
      "epoch": 1.941239809260114,
      "grad_norm": 1.05409574508667,
      "learning_rate": 3.5297133774291135e-05,
      "loss": 2.1244,
      "step": 12620
    },
    {
      "epoch": 1.9427780341485925,
      "grad_norm": 0.9231938123703003,
      "learning_rate": 3.5245859611341844e-05,
      "loss": 2.1941,
      "step": 12630
    },
    {
      "epoch": 1.944316259037071,
      "grad_norm": 0.9583561420440674,
      "learning_rate": 3.519458544839256e-05,
      "loss": 2.2428,
      "step": 12640
    },
    {
      "epoch": 1.94585448392555,
      "grad_norm": 0.7844263911247253,
      "learning_rate": 3.514331128544326e-05,
      "loss": 2.1758,
      "step": 12650
    },
    {
      "epoch": 1.9473927088140286,
      "grad_norm": 1.1220797300338745,
      "learning_rate": 3.509203712249398e-05,
      "loss": 2.2531,
      "step": 12660
    },
    {
      "epoch": 1.9489309337025074,
      "grad_norm": 0.8294132351875305,
      "learning_rate": 3.504076295954469e-05,
      "loss": 2.2227,
      "step": 12670
    },
    {
      "epoch": 1.9504691585909861,
      "grad_norm": 1.1784422397613525,
      "learning_rate": 3.4989488796595396e-05,
      "loss": 2.2289,
      "step": 12680
    },
    {
      "epoch": 1.9520073834794647,
      "grad_norm": 1.0723763704299927,
      "learning_rate": 3.4938214633646105e-05,
      "loss": 2.2392,
      "step": 12690
    },
    {
      "epoch": 1.9535456083679434,
      "grad_norm": 0.9018438458442688,
      "learning_rate": 3.4886940470696814e-05,
      "loss": 2.2126,
      "step": 12700
    },
    {
      "epoch": 1.9550838332564222,
      "grad_norm": 0.7937344312667847,
      "learning_rate": 3.483566630774753e-05,
      "loss": 2.006,
      "step": 12710
    },
    {
      "epoch": 1.9566220581449008,
      "grad_norm": 0.9771037697792053,
      "learning_rate": 3.478439214479824e-05,
      "loss": 2.2606,
      "step": 12720
    },
    {
      "epoch": 1.9581602830333795,
      "grad_norm": 0.9327136874198914,
      "learning_rate": 3.473311798184895e-05,
      "loss": 2.1881,
      "step": 12730
    },
    {
      "epoch": 1.9596985079218583,
      "grad_norm": 1.0237489938735962,
      "learning_rate": 3.468184381889966e-05,
      "loss": 2.254,
      "step": 12740
    },
    {
      "epoch": 1.9612367328103368,
      "grad_norm": 0.9393296837806702,
      "learning_rate": 3.4630569655950367e-05,
      "loss": 2.2486,
      "step": 12750
    },
    {
      "epoch": 1.9627749576988156,
      "grad_norm": 0.9011686444282532,
      "learning_rate": 3.457929549300108e-05,
      "loss": 2.1852,
      "step": 12760
    },
    {
      "epoch": 1.9643131825872944,
      "grad_norm": 1.048905611038208,
      "learning_rate": 3.452802133005179e-05,
      "loss": 2.2024,
      "step": 12770
    },
    {
      "epoch": 1.965851407475773,
      "grad_norm": 1.1650800704956055,
      "learning_rate": 3.4476747167102494e-05,
      "loss": 2.1886,
      "step": 12780
    },
    {
      "epoch": 1.9673896323642517,
      "grad_norm": 0.9255760908126831,
      "learning_rate": 3.442547300415321e-05,
      "loss": 2.143,
      "step": 12790
    },
    {
      "epoch": 1.9689278572527305,
      "grad_norm": 0.7802961468696594,
      "learning_rate": 3.437419884120392e-05,
      "loss": 2.061,
      "step": 12800
    },
    {
      "epoch": 1.970466082141209,
      "grad_norm": 1.118289828300476,
      "learning_rate": 3.432292467825463e-05,
      "loss": 2.1322,
      "step": 12810
    },
    {
      "epoch": 1.9720043070296878,
      "grad_norm": 0.8631650805473328,
      "learning_rate": 3.427165051530534e-05,
      "loss": 2.2146,
      "step": 12820
    },
    {
      "epoch": 1.9735425319181665,
      "grad_norm": 1.2895448207855225,
      "learning_rate": 3.4220376352356046e-05,
      "loss": 2.1807,
      "step": 12830
    },
    {
      "epoch": 1.975080756806645,
      "grad_norm": 1.0292304754257202,
      "learning_rate": 3.416910218940676e-05,
      "loss": 2.2023,
      "step": 12840
    },
    {
      "epoch": 1.9766189816951238,
      "grad_norm": 1.48505437374115,
      "learning_rate": 3.411782802645747e-05,
      "loss": 2.13,
      "step": 12850
    },
    {
      "epoch": 1.9781572065836026,
      "grad_norm": 0.8313589096069336,
      "learning_rate": 3.406655386350818e-05,
      "loss": 2.2931,
      "step": 12860
    },
    {
      "epoch": 1.9796954314720812,
      "grad_norm": 0.8047856092453003,
      "learning_rate": 3.401527970055889e-05,
      "loss": 2.1792,
      "step": 12870
    },
    {
      "epoch": 1.98123365636056,
      "grad_norm": 0.655029833316803,
      "learning_rate": 3.39640055376096e-05,
      "loss": 2.2108,
      "step": 12880
    },
    {
      "epoch": 1.9827718812490387,
      "grad_norm": 0.6835587620735168,
      "learning_rate": 3.3912731374660314e-05,
      "loss": 2.1264,
      "step": 12890
    },
    {
      "epoch": 1.9843101061375172,
      "grad_norm": 0.968829870223999,
      "learning_rate": 3.386145721171102e-05,
      "loss": 2.2783,
      "step": 12900
    },
    {
      "epoch": 1.985848331025996,
      "grad_norm": 0.8899255990982056,
      "learning_rate": 3.3810183048761726e-05,
      "loss": 2.0791,
      "step": 12910
    },
    {
      "epoch": 1.9873865559144748,
      "grad_norm": 0.9630827307701111,
      "learning_rate": 3.375890888581244e-05,
      "loss": 2.0954,
      "step": 12920
    },
    {
      "epoch": 1.9889247808029533,
      "grad_norm": 0.9868404269218445,
      "learning_rate": 3.370763472286315e-05,
      "loss": 2.2282,
      "step": 12930
    },
    {
      "epoch": 1.990463005691432,
      "grad_norm": 0.9945070743560791,
      "learning_rate": 3.3656360559913866e-05,
      "loss": 2.1563,
      "step": 12940
    },
    {
      "epoch": 1.9920012305799109,
      "grad_norm": 0.7788315415382385,
      "learning_rate": 3.360508639696457e-05,
      "loss": 2.1938,
      "step": 12950
    },
    {
      "epoch": 1.9935394554683894,
      "grad_norm": 1.012092113494873,
      "learning_rate": 3.355381223401528e-05,
      "loss": 2.2358,
      "step": 12960
    },
    {
      "epoch": 1.9950776803568682,
      "grad_norm": 1.2656893730163574,
      "learning_rate": 3.3502538071065994e-05,
      "loss": 2.2841,
      "step": 12970
    },
    {
      "epoch": 1.996615905245347,
      "grad_norm": 0.86236572265625,
      "learning_rate": 3.34512639081167e-05,
      "loss": 2.2564,
      "step": 12980
    },
    {
      "epoch": 1.9981541301338255,
      "grad_norm": 0.7351665496826172,
      "learning_rate": 3.339998974516741e-05,
      "loss": 2.2659,
      "step": 12990
    },
    {
      "epoch": 1.9996923550223042,
      "grad_norm": 0.9107856750488281,
      "learning_rate": 3.334871558221812e-05,
      "loss": 2.2082,
      "step": 13000
    },
    {
      "epoch": 2.001230579910783,
      "grad_norm": 1.2875291109085083,
      "learning_rate": 3.329744141926883e-05,
      "loss": 2.1198,
      "step": 13010
    },
    {
      "epoch": 2.0027688047992616,
      "grad_norm": 0.9068182110786438,
      "learning_rate": 3.3246167256319546e-05,
      "loss": 2.1855,
      "step": 13020
    },
    {
      "epoch": 2.0043070296877405,
      "grad_norm": 1.0676398277282715,
      "learning_rate": 3.319489309337025e-05,
      "loss": 2.1693,
      "step": 13030
    },
    {
      "epoch": 2.005845254576219,
      "grad_norm": 1.2075263261795044,
      "learning_rate": 3.3143618930420964e-05,
      "loss": 2.1729,
      "step": 13040
    },
    {
      "epoch": 2.0073834794646976,
      "grad_norm": 1.089200496673584,
      "learning_rate": 3.309234476747167e-05,
      "loss": 2.2484,
      "step": 13050
    },
    {
      "epoch": 2.0089217043531766,
      "grad_norm": 0.8539716005325317,
      "learning_rate": 3.304107060452238e-05,
      "loss": 2.1705,
      "step": 13060
    },
    {
      "epoch": 2.010459929241655,
      "grad_norm": 0.7813805937767029,
      "learning_rate": 3.29897964415731e-05,
      "loss": 2.1517,
      "step": 13070
    },
    {
      "epoch": 2.0119981541301337,
      "grad_norm": 1.0325918197631836,
      "learning_rate": 3.29385222786238e-05,
      "loss": 2.3338,
      "step": 13080
    },
    {
      "epoch": 2.0135363790186127,
      "grad_norm": 0.8101334571838379,
      "learning_rate": 3.288724811567451e-05,
      "loss": 2.1195,
      "step": 13090
    },
    {
      "epoch": 2.0150746039070913,
      "grad_norm": 1.0040078163146973,
      "learning_rate": 3.2835973952725225e-05,
      "loss": 2.2089,
      "step": 13100
    },
    {
      "epoch": 2.01661282879557,
      "grad_norm": 0.9944762587547302,
      "learning_rate": 3.2784699789775934e-05,
      "loss": 2.1508,
      "step": 13110
    },
    {
      "epoch": 2.018151053684049,
      "grad_norm": 1.1004676818847656,
      "learning_rate": 3.2733425626826643e-05,
      "loss": 2.2169,
      "step": 13120
    },
    {
      "epoch": 2.0196892785725273,
      "grad_norm": 0.9193671941757202,
      "learning_rate": 3.268215146387735e-05,
      "loss": 2.2506,
      "step": 13130
    },
    {
      "epoch": 2.021227503461006,
      "grad_norm": 1.117799997329712,
      "learning_rate": 3.263087730092806e-05,
      "loss": 2.2977,
      "step": 13140
    },
    {
      "epoch": 2.022765728349485,
      "grad_norm": 1.0666049718856812,
      "learning_rate": 3.257960313797878e-05,
      "loss": 2.2743,
      "step": 13150
    },
    {
      "epoch": 2.0243039532379634,
      "grad_norm": 1.0543019771575928,
      "learning_rate": 3.252832897502948e-05,
      "loss": 2.1207,
      "step": 13160
    },
    {
      "epoch": 2.025842178126442,
      "grad_norm": 0.9294948577880859,
      "learning_rate": 3.2477054812080196e-05,
      "loss": 2.1915,
      "step": 13170
    },
    {
      "epoch": 2.027380403014921,
      "grad_norm": 0.9527046084403992,
      "learning_rate": 3.2425780649130905e-05,
      "loss": 2.1527,
      "step": 13180
    },
    {
      "epoch": 2.0289186279033995,
      "grad_norm": 0.934114396572113,
      "learning_rate": 3.2374506486181614e-05,
      "loss": 2.2764,
      "step": 13190
    },
    {
      "epoch": 2.030456852791878,
      "grad_norm": 0.9789080619812012,
      "learning_rate": 3.232323232323233e-05,
      "loss": 2.243,
      "step": 13200
    },
    {
      "epoch": 2.031995077680357,
      "grad_norm": 0.9403070211410522,
      "learning_rate": 3.227195816028303e-05,
      "loss": 2.2514,
      "step": 13210
    },
    {
      "epoch": 2.0335333025688356,
      "grad_norm": 0.8311731815338135,
      "learning_rate": 3.222068399733375e-05,
      "loss": 2.203,
      "step": 13220
    },
    {
      "epoch": 2.035071527457314,
      "grad_norm": 0.9709030985832214,
      "learning_rate": 3.216940983438446e-05,
      "loss": 2.1886,
      "step": 13230
    },
    {
      "epoch": 2.036609752345793,
      "grad_norm": 1.1210739612579346,
      "learning_rate": 3.2118135671435166e-05,
      "loss": 2.2698,
      "step": 13240
    },
    {
      "epoch": 2.0381479772342717,
      "grad_norm": 1.1029436588287354,
      "learning_rate": 3.2066861508485875e-05,
      "loss": 2.1984,
      "step": 13250
    },
    {
      "epoch": 2.03968620212275,
      "grad_norm": 0.9385703206062317,
      "learning_rate": 3.2015587345536584e-05,
      "loss": 2.1587,
      "step": 13260
    },
    {
      "epoch": 2.041224427011229,
      "grad_norm": 0.7838046550750732,
      "learning_rate": 3.196431318258729e-05,
      "loss": 2.2243,
      "step": 13270
    },
    {
      "epoch": 2.0427626518997077,
      "grad_norm": 1.1748545169830322,
      "learning_rate": 3.191303901963801e-05,
      "loss": 2.076,
      "step": 13280
    },
    {
      "epoch": 2.0443008767881863,
      "grad_norm": 1.178568959236145,
      "learning_rate": 3.186176485668871e-05,
      "loss": 2.2193,
      "step": 13290
    },
    {
      "epoch": 2.0458391016766653,
      "grad_norm": 0.9837998151779175,
      "learning_rate": 3.181049069373943e-05,
      "loss": 2.176,
      "step": 13300
    },
    {
      "epoch": 2.047377326565144,
      "grad_norm": 0.9710680842399597,
      "learning_rate": 3.1759216530790136e-05,
      "loss": 2.3092,
      "step": 13310
    },
    {
      "epoch": 2.0489155514536224,
      "grad_norm": 1.0044004917144775,
      "learning_rate": 3.1707942367840846e-05,
      "loss": 2.2374,
      "step": 13320
    },
    {
      "epoch": 2.0504537763421014,
      "grad_norm": 1.0680088996887207,
      "learning_rate": 3.1656668204891555e-05,
      "loss": 2.1624,
      "step": 13330
    },
    {
      "epoch": 2.05199200123058,
      "grad_norm": 1.0259971618652344,
      "learning_rate": 3.1605394041942264e-05,
      "loss": 2.2107,
      "step": 13340
    },
    {
      "epoch": 2.0535302261190584,
      "grad_norm": 1.089430809020996,
      "learning_rate": 3.155411987899298e-05,
      "loss": 2.1297,
      "step": 13350
    },
    {
      "epoch": 2.0550684510075374,
      "grad_norm": 1.1467385292053223,
      "learning_rate": 3.150284571604369e-05,
      "loss": 2.1229,
      "step": 13360
    },
    {
      "epoch": 2.056606675896016,
      "grad_norm": 1.0275994539260864,
      "learning_rate": 3.14515715530944e-05,
      "loss": 2.198,
      "step": 13370
    },
    {
      "epoch": 2.0581449007844945,
      "grad_norm": 1.194806694984436,
      "learning_rate": 3.140029739014511e-05,
      "loss": 2.1212,
      "step": 13380
    },
    {
      "epoch": 2.0596831256729735,
      "grad_norm": 0.7426186203956604,
      "learning_rate": 3.1349023227195816e-05,
      "loss": 2.1747,
      "step": 13390
    },
    {
      "epoch": 2.061221350561452,
      "grad_norm": 0.946909487247467,
      "learning_rate": 3.129774906424653e-05,
      "loss": 2.2959,
      "step": 13400
    },
    {
      "epoch": 2.0627595754499306,
      "grad_norm": 0.9835613369941711,
      "learning_rate": 3.124647490129724e-05,
      "loss": 2.1935,
      "step": 13410
    },
    {
      "epoch": 2.0642978003384096,
      "grad_norm": 1.1787033081054688,
      "learning_rate": 3.119520073834794e-05,
      "loss": 2.2795,
      "step": 13420
    },
    {
      "epoch": 2.065836025226888,
      "grad_norm": 1.1964752674102783,
      "learning_rate": 3.114392657539866e-05,
      "loss": 2.2181,
      "step": 13430
    },
    {
      "epoch": 2.0673742501153667,
      "grad_norm": 1.2113621234893799,
      "learning_rate": 3.109265241244937e-05,
      "loss": 2.2384,
      "step": 13440
    },
    {
      "epoch": 2.0689124750038457,
      "grad_norm": 1.0353820323944092,
      "learning_rate": 3.104137824950008e-05,
      "loss": 2.2548,
      "step": 13450
    },
    {
      "epoch": 2.070450699892324,
      "grad_norm": 1.158869743347168,
      "learning_rate": 3.0990104086550786e-05,
      "loss": 2.1445,
      "step": 13460
    },
    {
      "epoch": 2.0719889247808028,
      "grad_norm": 0.9294489622116089,
      "learning_rate": 3.0938829923601495e-05,
      "loss": 2.1191,
      "step": 13470
    },
    {
      "epoch": 2.0735271496692818,
      "grad_norm": 0.8135626912117004,
      "learning_rate": 3.088755576065221e-05,
      "loss": 2.1101,
      "step": 13480
    },
    {
      "epoch": 2.0750653745577603,
      "grad_norm": 1.0623928308486938,
      "learning_rate": 3.083628159770292e-05,
      "loss": 2.1807,
      "step": 13490
    },
    {
      "epoch": 2.076603599446239,
      "grad_norm": 0.9988210797309875,
      "learning_rate": 3.078500743475363e-05,
      "loss": 2.1747,
      "step": 13500
    },
    {
      "epoch": 2.078141824334718,
      "grad_norm": 1.0527896881103516,
      "learning_rate": 3.073373327180434e-05,
      "loss": 2.3349,
      "step": 13510
    },
    {
      "epoch": 2.0796800492231964,
      "grad_norm": 0.9817227721214294,
      "learning_rate": 3.068245910885505e-05,
      "loss": 2.2558,
      "step": 13520
    },
    {
      "epoch": 2.081218274111675,
      "grad_norm": 1.2952531576156616,
      "learning_rate": 3.0631184945905763e-05,
      "loss": 2.0404,
      "step": 13530
    },
    {
      "epoch": 2.082756499000154,
      "grad_norm": 0.8613524436950684,
      "learning_rate": 3.057991078295647e-05,
      "loss": 2.1747,
      "step": 13540
    },
    {
      "epoch": 2.0842947238886325,
      "grad_norm": 0.9672804474830627,
      "learning_rate": 3.052863662000718e-05,
      "loss": 2.0924,
      "step": 13550
    },
    {
      "epoch": 2.085832948777111,
      "grad_norm": 0.9156234860420227,
      "learning_rate": 3.047736245705789e-05,
      "loss": 2.1502,
      "step": 13560
    },
    {
      "epoch": 2.08737117366559,
      "grad_norm": 0.875990092754364,
      "learning_rate": 3.04260882941086e-05,
      "loss": 2.108,
      "step": 13570
    },
    {
      "epoch": 2.0889093985540685,
      "grad_norm": 1.096423864364624,
      "learning_rate": 3.0374814131159312e-05,
      "loss": 2.2075,
      "step": 13580
    },
    {
      "epoch": 2.090447623442547,
      "grad_norm": 1.1850136518478394,
      "learning_rate": 3.0323539968210018e-05,
      "loss": 2.3213,
      "step": 13590
    },
    {
      "epoch": 2.091985848331026,
      "grad_norm": 1.0467263460159302,
      "learning_rate": 3.027226580526073e-05,
      "loss": 2.1746,
      "step": 13600
    },
    {
      "epoch": 2.0935240732195046,
      "grad_norm": 1.1558860540390015,
      "learning_rate": 3.0220991642311443e-05,
      "loss": 2.1255,
      "step": 13610
    },
    {
      "epoch": 2.095062298107983,
      "grad_norm": 1.1741331815719604,
      "learning_rate": 3.0169717479362152e-05,
      "loss": 2.2853,
      "step": 13620
    },
    {
      "epoch": 2.096600522996462,
      "grad_norm": 1.029974102973938,
      "learning_rate": 3.0118443316412858e-05,
      "loss": 2.1294,
      "step": 13630
    },
    {
      "epoch": 2.0981387478849407,
      "grad_norm": 0.8843310475349426,
      "learning_rate": 3.006716915346357e-05,
      "loss": 2.253,
      "step": 13640
    },
    {
      "epoch": 2.0996769727734197,
      "grad_norm": 1.2545733451843262,
      "learning_rate": 3.0015894990514283e-05,
      "loss": 2.1716,
      "step": 13650
    },
    {
      "epoch": 2.1012151976618982,
      "grad_norm": 0.9113606810569763,
      "learning_rate": 2.9964620827564992e-05,
      "loss": 2.145,
      "step": 13660
    },
    {
      "epoch": 2.102753422550377,
      "grad_norm": 0.8905961513519287,
      "learning_rate": 2.9913346664615697e-05,
      "loss": 2.2284,
      "step": 13670
    },
    {
      "epoch": 2.1042916474388558,
      "grad_norm": 0.9434868693351746,
      "learning_rate": 2.986207250166641e-05,
      "loss": 2.2161,
      "step": 13680
    },
    {
      "epoch": 2.1058298723273343,
      "grad_norm": 1.02804696559906,
      "learning_rate": 2.9810798338717122e-05,
      "loss": 2.1511,
      "step": 13690
    },
    {
      "epoch": 2.107368097215813,
      "grad_norm": 0.9433062672615051,
      "learning_rate": 2.9759524175767835e-05,
      "loss": 2.1458,
      "step": 13700
    },
    {
      "epoch": 2.108906322104292,
      "grad_norm": 1.017617106437683,
      "learning_rate": 2.9708250012818544e-05,
      "loss": 2.2011,
      "step": 13710
    },
    {
      "epoch": 2.1104445469927704,
      "grad_norm": 0.9758481383323669,
      "learning_rate": 2.965697584986925e-05,
      "loss": 2.1329,
      "step": 13720
    },
    {
      "epoch": 2.111982771881249,
      "grad_norm": 0.9438018202781677,
      "learning_rate": 2.9605701686919962e-05,
      "loss": 2.2032,
      "step": 13730
    },
    {
      "epoch": 2.113520996769728,
      "grad_norm": 1.2553602457046509,
      "learning_rate": 2.9554427523970675e-05,
      "loss": 2.1332,
      "step": 13740
    },
    {
      "epoch": 2.1150592216582065,
      "grad_norm": 0.7829607129096985,
      "learning_rate": 2.9503153361021384e-05,
      "loss": 2.1495,
      "step": 13750
    },
    {
      "epoch": 2.116597446546685,
      "grad_norm": 0.9680314064025879,
      "learning_rate": 2.945187919807209e-05,
      "loss": 2.1881,
      "step": 13760
    },
    {
      "epoch": 2.118135671435164,
      "grad_norm": 0.8956647515296936,
      "learning_rate": 2.9400605035122802e-05,
      "loss": 2.1612,
      "step": 13770
    },
    {
      "epoch": 2.1196738963236426,
      "grad_norm": 1.1463645696640015,
      "learning_rate": 2.9349330872173514e-05,
      "loss": 2.1844,
      "step": 13780
    },
    {
      "epoch": 2.121212121212121,
      "grad_norm": 1.081416368484497,
      "learning_rate": 2.9298056709224227e-05,
      "loss": 2.162,
      "step": 13790
    },
    {
      "epoch": 2.1227503461006,
      "grad_norm": 0.8087849617004395,
      "learning_rate": 2.9246782546274933e-05,
      "loss": 2.2178,
      "step": 13800
    },
    {
      "epoch": 2.1242885709890786,
      "grad_norm": 1.0204700231552124,
      "learning_rate": 2.919550838332564e-05,
      "loss": 2.1758,
      "step": 13810
    },
    {
      "epoch": 2.125826795877557,
      "grad_norm": 0.9389615654945374,
      "learning_rate": 2.9144234220376354e-05,
      "loss": 2.1729,
      "step": 13820
    },
    {
      "epoch": 2.127365020766036,
      "grad_norm": 1.0939966440200806,
      "learning_rate": 2.9092960057427067e-05,
      "loss": 2.2658,
      "step": 13830
    },
    {
      "epoch": 2.1289032456545147,
      "grad_norm": 1.0272613763809204,
      "learning_rate": 2.9041685894477772e-05,
      "loss": 2.2086,
      "step": 13840
    },
    {
      "epoch": 2.1304414705429933,
      "grad_norm": 1.1807358264923096,
      "learning_rate": 2.899041173152848e-05,
      "loss": 2.1798,
      "step": 13850
    },
    {
      "epoch": 2.1319796954314723,
      "grad_norm": 0.887175440788269,
      "learning_rate": 2.8939137568579194e-05,
      "loss": 2.2332,
      "step": 13860
    },
    {
      "epoch": 2.133517920319951,
      "grad_norm": 1.106878638267517,
      "learning_rate": 2.8887863405629906e-05,
      "loss": 2.237,
      "step": 13870
    },
    {
      "epoch": 2.1350561452084293,
      "grad_norm": 0.8799247145652771,
      "learning_rate": 2.883658924268062e-05,
      "loss": 2.221,
      "step": 13880
    },
    {
      "epoch": 2.1365943700969083,
      "grad_norm": 0.7790150046348572,
      "learning_rate": 2.8785315079731325e-05,
      "loss": 2.2134,
      "step": 13890
    },
    {
      "epoch": 2.138132594985387,
      "grad_norm": 1.041158676147461,
      "learning_rate": 2.8734040916782034e-05,
      "loss": 2.2274,
      "step": 13900
    },
    {
      "epoch": 2.1396708198738654,
      "grad_norm": 1.4090553522109985,
      "learning_rate": 2.8682766753832746e-05,
      "loss": 2.1871,
      "step": 13910
    },
    {
      "epoch": 2.1412090447623444,
      "grad_norm": 0.9608466029167175,
      "learning_rate": 2.863149259088346e-05,
      "loss": 2.1381,
      "step": 13920
    },
    {
      "epoch": 2.142747269650823,
      "grad_norm": 1.027198314666748,
      "learning_rate": 2.8580218427934164e-05,
      "loss": 2.162,
      "step": 13930
    },
    {
      "epoch": 2.1442854945393015,
      "grad_norm": 1.1920849084854126,
      "learning_rate": 2.8528944264984873e-05,
      "loss": 2.2877,
      "step": 13940
    },
    {
      "epoch": 2.1458237194277805,
      "grad_norm": 1.0824458599090576,
      "learning_rate": 2.8477670102035586e-05,
      "loss": 2.2714,
      "step": 13950
    },
    {
      "epoch": 2.147361944316259,
      "grad_norm": 1.4422882795333862,
      "learning_rate": 2.84263959390863e-05,
      "loss": 2.1618,
      "step": 13960
    },
    {
      "epoch": 2.1489001692047376,
      "grad_norm": 0.8501550555229187,
      "learning_rate": 2.8375121776137004e-05,
      "loss": 2.1395,
      "step": 13970
    },
    {
      "epoch": 2.1504383940932166,
      "grad_norm": 0.8276148438453674,
      "learning_rate": 2.8323847613187716e-05,
      "loss": 2.2522,
      "step": 13980
    },
    {
      "epoch": 2.151976618981695,
      "grad_norm": 1.1200199127197266,
      "learning_rate": 2.8272573450238426e-05,
      "loss": 2.237,
      "step": 13990
    },
    {
      "epoch": 2.1535148438701737,
      "grad_norm": 1.2769662141799927,
      "learning_rate": 2.8221299287289138e-05,
      "loss": 2.1996,
      "step": 14000
    }
  ],
  "logging_steps": 10,
  "max_steps": 19503,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 2.954058083912909e+16,
  "train_batch_size": 8,
  "trial_name": null,
  "trial_params": null
}
